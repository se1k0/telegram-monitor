#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æ•°æ®åº“æ“ä½œå¤„ç†æ¨¡å—
ä»…æ”¯æŒSupabaseæ•°æ®åº“
"""

import asyncio
import traceback
import time
import functools
import json
import os
import re
import inspect
from typing import Tuple, Optional, List, Dict, Any, Callable
from datetime import datetime, timezone, timedelta
from contextlib import contextmanager

# å¯¼å…¥æ•°æ®åº“å·¥å‚ï¼ˆå·²ç»ç§»é™¤SQLAlchemyä¼šè¯ï¼‰
from src.database.db_factory import get_db_adapter
# å¯¼å…¥å¿…è¦çš„æ•°æ®æ¨¡å‹
from src.database.models import PromotionInfo

# æ·»åŠ æ—¥å¿—æ”¯æŒ
try:
    from src.utils.logger import get_logger
    logger = get_logger(__name__)
except ImportError:
    import logging
    logger = logging.getLogger(__name__)

# å¯¼å…¥ä»£å¸åˆ†æå™¨
try:
    from src.analysis.token_analyzer import get_analyzer
    token_analyzer = get_analyzer()
    HAS_ANALYZER = True
except ImportError:
    logger.warning("æ— æ³•å¯¼å…¥ä»£å¸åˆ†æå™¨ï¼Œå°†ä½¿ç”¨åŸºæœ¬åˆ†æ")
    token_analyzer = None
    HAS_ANALYZER = False

# æ‰¹å¤„ç†æ¶ˆæ¯é˜Ÿåˆ—
message_batch = []
token_batch = []

# ä»é…ç½®æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡ä¸­è·å–æ‰¹å¤„ç†è®¾ç½®
try:
    from config.settings import BATCH_SIZE, BATCH_INTERVAL
    MAX_BATCH_SIZE = BATCH_SIZE if hasattr(BATCH_SIZE, '__int__') else 50
    BATCH_TIMEOUT = BATCH_INTERVAL if hasattr(BATCH_INTERVAL, '__int__') else 10
except (ImportError, AttributeError):
    # é»˜è®¤å€¼
    MAX_BATCH_SIZE = 50
    BATCH_TIMEOUT = 10  # ç§’

# é‡è¯•è®¾ç½®
OPERATION_RETRIES = 5  # é‡è¯•æ¬¡æ•°
OPERATION_RETRY_DELAY = 1.0  # é‡è¯•é—´éš”(ç§’)

# æ·»åŠ æ•°æ®åº“æ€§èƒ½ç›‘æ§ç›¸å…³çš„å˜é‡
db_performance_stats = {
    'operation_counts': {},
    'operation_times': {},
    'lock_errors': 0,
    'total_retries': 0
}

# æ·»åŠ å¸¸é‡å®šä¹‰ - æ”¯æŒçš„åŒºå—é“¾åŠå…¶ä»£ç 
CHAINS = {
    'SOL': ['solana', 'sol', 'ç´¢æ‹‰çº³', 'ç´¢å…°çº³', 'raydium', 'orca', 'jupiter'],
    'ETH': ['ethereum', 'eth', 'ä»¥å¤ªåŠ', 'ä»¥å¤ª', 'uniswap', 'sushiswap'],
    'BSC': ['binance', 'bsc', 'bnb', 'å¸å®‰', 'pancakeswap', 'poocoin'],
    'ARB': ['arbitrum', 'arb', 'é˜¿æ¯”ç‰¹é¾™', 'ì•„ë¹„íŠ¸ëŸ¼'],
    'BASE': ['base', 'basechain', 'coinbase', 'è´æ–¯é“¾', 'ë² ì´ìŠ¤'],
    'AVAX': ['avalanche', 'avax', 'é›ªå´©é“¾', 'ì•„ë°œë€ì²´'],
    'MATIC': ['polygon', 'matic', 'æ³¢åˆ©å†ˆ', 'í´ë¦¬ê³¤'],
    'OP': ['optimism', 'op', 'ä¹è§‚é“¾', 'ì˜µí‹°ë¯¸ì¦˜']
}

# æ·»åŠ EVMé“¾åˆ—è¡¨å¸¸é‡
EVM_CHAINS = ['ETH', 'BSC', 'ARB', 'BASE', 'MATIC', 'AVAX', 'OP']
NON_EVM_CHAINS = ['SOL']

# é“¾çš„åŒºå—æµè§ˆå™¨
CHAIN_EXPLORERS = {
    'SOL': ['solscan.io', 'explorer.solana.com'],
    'ETH': ['etherscan.io'],
    'BSC': ['bscscan.com'],
    'ARB': ['arbiscan.io'],
    'BASE': ['basescan.org'],
    'AVAX': ['snowtrace.io'],
    'MATIC': ['polygonscan.com'],
    'OP': ['optimistic.etherscan.io']
}

# DEXå¹³å°URLåŒ¹é…
DEX_PATTERNS = {
    'SOL': [r'raydium\.io', r'orca\.so', r'jup\.ag'],
    'ETH': [r'uniswap\.org', r'app\.uniswap\.org', r'sushi\.com'],
    'BSC': [r'pancakeswap\.finance', r'poocoin\.app']
}

# æ¨ç‰¹è´¦å·ä¸é“¾çš„æ˜ å°„
TWITTER_CHAIN_MAP = {
    'cz_binance': 'BSC',
    'binance': 'BSC',
    'bnbchain': 'BSC',
    'ethereum': 'ETH',
    'vitalikbuterin': 'ETH',
    'solana': 'SOL',
    'arbitrum': 'ARB',
    'optimism': 'OP',
    'avalancheavax': 'AVAX',
    'polygonlabs': 'MATIC',
    'base': 'BASE'
}

# URLæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
URL_PATTERNS = [
    r'https?://\S+',  # æ ‡å‡†HTTP/HTTPS URL
    r'www\.\S+',      # ä»¥wwwå¼€å¤´çš„URL
    r't\.me/\S+',     # Telegramé“¾æ¥
    r'twitter\.com/\S+',  # Twitteré“¾æ¥
    r'x\.com/\S+'     # X.comé“¾æ¥
]

# åˆçº¦åœ°å€åŒ¹é…æ¨¡å¼
CONTRACT_PATTERNS = [
    # å¸¦æ ‡è®°çš„åˆçº¦åœ°å€
    r'(?:ğŸ“|åˆçº¦[ï¼š:]|[Cc]ontract[ï¼š:])[ ]*([0-9a-fA-FxX]{8,})',
    r'åˆçº¦åœ°å€[ï¼š:][ ]*([0-9a-fA-FxX]{8,})',
    r'åœ°å€[ï¼š:][ ]*([0-9a-fA-FxX]{8,})',
    # æ ‡å‡†ä»¥å¤ªåŠåœ°å€æ ¼å¼
    r'\b(0x[0-9a-fA-F]{40})\b',
    # å…¶ä»–å¯èƒ½çš„åˆçº¦åœ°å€æ ¼å¼
    r'\b([a-zA-Z0-9]{32,50})\b'
]

# è¾…åŠ©å‡½æ•°ï¼šæŸ¥æ‰¾æ–‡æœ¬ä¸­çš„æ‰€æœ‰URL
def find_urls_in_text(text: str) -> List[str]:
    """
    ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰URL
    
    Args:
        text: è¦å¤„ç†çš„æ–‡æœ¬
        
    Returns:
        List[str]: æå–å‡ºçš„URLåˆ—è¡¨
    """
    if not text:
        return []
    
    # åˆå¹¶æ‰€æœ‰URLæ¨¡å¼
    combined_pattern = '|'.join(URL_PATTERNS)
    
    # æå–æ‰€æœ‰URL
    urls = re.findall(combined_pattern, text)
    
    # æ¸…ç†URL
    clean_urls = []
    for url in urls:
        # å¤„ç†URLæœ«å°¾å¯èƒ½çš„æ ‡ç‚¹ç¬¦å·
        markers = [' ', '\n', '\t', ',', ')', ']', '}', '"', "'", 'ã€‚', 'ï¼Œ', 'ï¼š', 'ï¼›']
        end_idx = len(url)
        for marker in markers:
            marker_idx = url.find(marker)
            if marker_idx > 0 and marker_idx < end_idx:
                end_idx = marker_idx
        
        clean_url = url[:end_idx].strip()
        if clean_url:
            clean_urls.append(clean_url)
    
    return clean_urls

# è¾…åŠ©å‡½æ•°ï¼šæ ¹æ®URLåˆ¤æ–­é“¾
def get_chain_from_url(url: str) -> Optional[str]:
    """
    ä»URLä¸­åˆ¤æ–­åŒºå—é“¾ç±»å‹
    
    Args:
        url: URLå­—ç¬¦ä¸²
        
    Returns:
        Optional[str]: é“¾åç§°æˆ–None
    """
    url_lower = url.lower()
    
    # DexScreener URLæ ¼å¼
    dexscreener_match = re.search(r'(?:https?://)?(?:www\.)?dexscreener\.com/([a-zA-Z0-9]+)(?:/[^/\s]+)?', url_lower)
    if dexscreener_match:
        chain_str = dexscreener_match.group(1).upper()
        dexscreener_map = {
            'SOLANA': 'SOL',
            'ETHEREUM': 'ETH',
            'BSC': 'BSC',
            'ARBITRUM': 'ARB',
            'BASE': 'BASE',
            'AVALANCHE': 'AVAX',
            'POLYGON': 'MATIC',
            'OPTIMISM': 'OP'
        }
        if chain_str in dexscreener_map:
            return dexscreener_map[chain_str]
    
    # GMGN.ai URLæ ¼å¼
    gmgn_match = re.search(r'gmgn\.ai(?:/[^/]+)?/([^/]+)/token/', url_lower)
    if gmgn_match:
        chain_id = gmgn_match.group(1).upper()
        if chain_id in ['BSC', 'ETH', 'ARBITRUM', 'BASE', 'POLYGON', 'OPTIMISM']:
            if chain_id == 'ARBITRUM':
                return 'ARB'
            elif chain_id == 'POLYGON':
                return 'MATIC'
            elif chain_id == 'OPTIMISM':
                return 'OP'
            return chain_id
    
    # æ£€æŸ¥åŒºå—æµè§ˆå™¨åŸŸå
    for chain, explorers in CHAIN_EXPLORERS.items():
        for explorer in explorers:
            if explorer in url_lower:
                return chain
    
    # æ£€æŸ¥DEXåŸŸå
    for chain, patterns in DEX_PATTERNS.items():
        for pattern in patterns:
            if re.search(pattern, url_lower):
                return chain
    
    # æ£€æŸ¥å¸¸è§å…³é”®è¯
    for chain, keywords in CHAINS.items():
        for keyword in keywords:
            if keyword.lower() in url_lower:
                return chain
    
    # æ¨ç‰¹è´¦å·åˆ¤æ–­
    twitter_match = re.search(r'(?:twitter\.com|x\.com)/([^/\s]+)', url_lower)
    if twitter_match:
        twitter_user = twitter_match.group(1).lower()
        if twitter_user in TWITTER_CHAIN_MAP:
            return TWITTER_CHAIN_MAP[twitter_user]
    
    return None

# è¾…åŠ©å‡½æ•°ï¼šåˆ¤æ–­åœ°å€æ ¼å¼å¯¹åº”çš„é“¾
def get_chain_from_address(address: str) -> Optional[str]:
    """
    æ ¹æ®åˆçº¦åœ°å€æ ¼å¼åˆ¤æ–­å¯èƒ½çš„é“¾
    
    Args:
        address: åˆçº¦åœ°å€
        
    Returns:
        Optional[str]: é“¾åç§°ã€'EVM'è¡¨ç¤ºä»¥å¤ªåŠé£æ ¼åœ°å€(éœ€è¦è¿›ä¸€æ­¥ç¡®å®šå…·ä½“é“¾)ï¼Œæˆ–None
    """
    if not address:
        return None
        
    # EVMæ ¼å¼åœ°å€(ä»¥å¤ªåŠã€BSCç­‰) - 0xå¼€å¤´çš„42ä½16è¿›åˆ¶æ•°
    if re.match(r'^0x[a-fA-F0-9]{40}$', address):
        return 'EVM'
    
    # Solanaæ ¼å¼åœ°å€ - Base58ç¼–ç ï¼Œä¸ä»¥0xå¼€å¤´ï¼Œé€šå¸¸32-44ä½
    if re.match(r'^[1-9A-HJ-NP-Za-km-z]{32,44}$', address) and not address.startswith('0x'):
        return 'SOL'
    
    # æ£€æŸ¥æ˜¯å¦å¯èƒ½æ˜¯ä¸å®Œæ•´çš„EVMåœ°å€
    if address.startswith('0x') and len(address) >= 10:
        logger.warning(f"å‘ç°å¯èƒ½ä¸å®Œæ•´çš„EVMæ ¼å¼åœ°å€: {address}")
        return 'EVM_PARTIAL'
    
    # é’ˆå¯¹ç‰¹æ®Šæ ¼å¼çš„åœ°å€ï¼Œå¯ä»¥æ‰©å±•æ›´å¤šåˆ¤æ–­
    # ä¾‹å¦‚: Arweaveã€NEARã€Cosmosç­‰
    
    return None

@contextmanager
def session_scope():
    """æä¾›äº‹åŠ¡èŒƒå›´çš„ä¼šè¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    
    æ³¨æ„ï¼šæ­¤å‡½æ•°ä»…ä½œä¸ºå…¼å®¹å±‚ã€‚åœ¨Supabaseä¸­æ²¡æœ‰äº‹åŠ¡çš„æ¦‚å¿µï¼Œ
    è€Œæ˜¯ç›´æ¥ä½¿ç”¨adapterè¿›è¡Œæ“ä½œã€‚
    """
    # å¯¼å…¥å…¨å±€é…ç½®
    import config.settings as config
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨Supabase
    if not config.DATABASE_URI.startswith('supabase://'):
        logger.error("æœªä½¿ç”¨Supabaseæ•°æ®åº“ï¼Œè¯·æ£€æŸ¥é…ç½®")
        logger.error(f"å½“å‰DATABASE_URI: {config.DATABASE_URI}")
        logger.error("DATABASE_URIåº”ä»¥'supabase://'å¼€å¤´")
        raise ValueError("å¿…é¡»ä½¿ç”¨Supabaseæ•°æ®åº“")
        
    try:
        # ä½¿ç”¨Supabaseé€‚é…å™¨ï¼Œä¸å†åˆ›å»ºSQLAlchemyä¼šè¯
        from src.database.db_factory import get_db_adapter
        adapter = get_db_adapter()
        logger.debug("ä½¿ç”¨Supabaseé€‚é…å™¨åˆ›å»ºä¼šè¯")
        
        # è¿”å›é€‚é…å™¨å®ä¾‹è€Œä¸æ˜¯ä¼šè¯å¯¹è±¡
        yield adapter
        
    except Exception as e:
        # å‘ç”Ÿé”™è¯¯æ—¶è®°å½•ä½†ä¸å†è¿›è¡Œå›æ»šï¼ˆSupabaseæ²¡æœ‰ä¼šè¯æ¦‚å¿µï¼‰
        logger.error(f"Supabaseæ“ä½œå‡ºé”™: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        # é‡æ–°æŠ›å‡ºå¼‚å¸¸
        raise e

async def process_batches():
    """å®šæœŸå¤„ç†æ‰¹å¤„ç†é˜Ÿåˆ—çš„æ¶ˆæ¯å’Œä»£å¸"""
    global message_batch, token_batch
    
    while True:
        try:
            if message_batch:
                local_batch = message_batch.copy()
                message_batch = []
                
                try:
                    # ä½¿ç”¨Supabaseé€‚é…å™¨å¤„ç†æ‰¹é‡æ¶ˆæ¯
                    from src.database.db_factory import get_db_adapter
                    db_adapter = get_db_adapter()
                    
                    for msg_data in local_batch:
                        try:
                            # æ„å»ºæ¶ˆæ¯æ•°æ®
                            message_data = {
                                'chain': msg_data.get('chain'),
                                'message_id': msg_data.get('message_id'),
                                'date': msg_data.get('date'),
                                'text': msg_data.get('text'),
                                'media_path': msg_data.get('media_path'),
                                'channel_id': msg_data.get('channel_id')
                            }
                            
                            # ä½¿ç”¨Supabaseé€‚é…å™¨ä¿å­˜æ¶ˆæ¯
                            await db_adapter.save_message(message_data)
                        except Exception as e:
                            logger.error(f"å¤„ç†æ¶ˆæ¯æ‰¹æ¬¡æ—¶å‡ºé”™: {str(e)}")
                            logger.debug(traceback.format_exc())
                            continue
                    
                    logger.info(f"æ‰¹é‡å¤„ç†äº† {len(local_batch)} æ¡æ¶ˆæ¯")
                except Exception as e:
                    logger.error(f"æ¶ˆæ¯æ‰¹å¤„ç†å¤±è´¥: {str(e)}")
                    import traceback
                    logger.error(traceback.format_exc())
                    # åªè®°å½•æ—¥å¿—ï¼Œä¸æŠ›å‡ºå¼‚å¸¸ï¼Œä¿è¯ä¸»æµç¨‹ç»§ç»­
                    continue
                
            if token_batch:
                local_batch = token_batch.copy()
                token_batch = []
                
                try:
                    # ä½¿ç”¨Supabaseé€‚é…å™¨å¤„ç†æ‰¹é‡ä»£å¸ä¿¡æ¯
                    from src.database.db_factory import get_db_adapter
                    db_adapter = get_db_adapter()
                    
                    for token_data in local_batch:
                        try:
                            # ç®€å•éªŒè¯ä»£å¸æ•°æ®
                            if not all(key in token_data for key in ['chain', 'token_symbol', 'contract']):
                                logger.warning(f"æ— æ•ˆçš„ä»£å¸æ•°æ®: ç¼ºå°‘å¿…è¦å­—æ®µï¼Œæ•°æ®: {token_data}")
                                continue
                                
                            # ä½¿ç”¨Supabaseé€‚é…å™¨ä¿å­˜ä»£å¸ä¿¡æ¯
                            await db_adapter.save_token(token_data)
                        except Exception as e:
                            logger.error(f"å¤„ç†ä»£å¸æ‰¹æ¬¡æ—¶å‡ºé”™: {str(e)}")
                            logger.debug(traceback.format_exc())
                            continue
                    
                    logger.info(f"æ‰¹é‡å¤„ç†äº† {len(local_batch)} æ¡ä»£å¸ä¿¡æ¯")
                except Exception as e:
                    logger.error(f"ä»£å¸æ‰¹å¤„ç†å¤±è´¥: {str(e)}")
                    import traceback
                    logger.error(traceback.format_exc())
                    # åªè®°å½•æ—¥å¿—ï¼Œä¸æŠ›å‡ºå¼‚å¸¸ï¼Œä¿è¯ä¸»æµç¨‹ç»§ç»­
                    continue
                
            await asyncio.sleep(BATCH_TIMEOUT)
        except Exception as e:
            logger.error(f"æ‰¹å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            logger.debug(traceback.format_exc())
            await asyncio.sleep(5)  # å‡ºé”™åç­‰å¾…çŸ­æš‚æ—¶é—´å†ç»§ç»­

def monitor_db_operation(operation_name):
    """è£…é¥°å™¨å‡½æ•°ï¼šç›‘æ§æ•°æ®åº“æ“ä½œæ€§èƒ½
    
    Args:
        operation_name: æ“ä½œåç§°ï¼Œç”¨äºç»Ÿè®¡
        
    Returns:
        è£…é¥°å™¨å‡½æ•°
    """
    def decorator(func):
        @functools.wraps(func)
        def sync_wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                return func(*args, **kwargs)
            finally:
                execution_time = time.time() - start_time
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                if operation_name not in db_performance_stats['operation_counts']:
                    db_performance_stats['operation_counts'][operation_name] = 0
                    db_performance_stats['operation_times'][operation_name] = 0
                
                db_performance_stats['operation_counts'][operation_name] += 1
                db_performance_stats['operation_times'][operation_name] += execution_time
        
        @functools.wraps(func)
        async def async_wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                return await func(*args, **kwargs)
            finally:
                execution_time = time.time() - start_time
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                if operation_name not in db_performance_stats['operation_counts']:
                    db_performance_stats['operation_counts'][operation_name] = 0
                    db_performance_stats['operation_times'][operation_name] = 0
                
                db_performance_stats['operation_counts'][operation_name] += 1
                db_performance_stats['operation_times'][operation_name] += execution_time
        
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper
    
    return decorator

@monitor_db_operation('save_messages_batch')
async def save_messages_batch(messages: List[Dict]):
    """æ‰¹é‡ä¿å­˜æ¶ˆæ¯åˆ°æ•°æ®åº“
    
    Args:
        messages: æ¶ˆæ¯å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«æ¶ˆæ¯çš„æ‰€æœ‰å¿…è¦å­—æ®µ
    
    Returns:
        int: æˆåŠŸä¿å­˜çš„æ¶ˆæ¯æ•°é‡
    """
    if not messages:
        return 0
        
    try:
        # ä½¿ç”¨æ•°æ®åº“é€‚é…å™¨
        from src.database.db_factory import get_db_adapter
        db_adapter = get_db_adapter()
        # æ–°å¢ï¼šæ‰¹é‡æŸ¥é‡ï¼Œè¿‡æ»¤å·²å­˜åœ¨çš„æ¶ˆæ¯
        filtered_msgs = []
        for msg in messages:
            if not all(key in msg for key in ['chain', 'message_id', 'date']):
                logger.warning(f"æ¶ˆæ¯ç¼ºå°‘å¿…è¦å­—æ®µ: {msg}")
                continue
            exists = await db_adapter.check_message_exists(msg['chain'], msg['message_id'])
            if exists:
                logger.info(f"æ¶ˆæ¯ {msg['chain']}-{msg['message_id']} å·²å­˜åœ¨ï¼Œæ‰¹é‡ä¿å­˜æ—¶è·³è¿‡")
                continue
            filtered_msgs.append(msg)
        successful = 0
        for msg in filtered_msgs:
            # åˆå§‹æ£€æŸ¥ï¼Œç¡®ä¿å¿…é¡»çš„å­—æ®µå­˜åœ¨
            if not all(key in msg for key in ['chain', 'message_id', 'date']):
                logger.warning(f"æ¶ˆæ¯ç¼ºå°‘å¿…è¦å­—æ®µ: {msg}")
                continue
                
            # å‡†å¤‡æ¶ˆæ¯æ•°æ®
            message_data = {
                'chain': msg['chain'],
                'message_id': msg['message_id'],
                'date': msg['date'].isoformat() if isinstance(msg['date'], datetime) else msg['date'],
                'text': msg.get('text'),
                'media_path': msg.get('media_path'),
                'channel_id': msg.get('channel_id')
            }
            
            # ä¿å­˜æ¶ˆæ¯
            result = await db_adapter.save_message(message_data)
            if result:
                successful += 1
                
        # è¿”å›æˆåŠŸæ·»åŠ çš„æ•°é‡
        return successful
    except Exception as e:
        logger.error(f"æ‰¹é‡ä¿å­˜æ¶ˆæ¯å¤±è´¥: {str(e)}")
        import traceback
        logger.debug(traceback.format_exc())
        return 0

async def save_messages_individually(messages: List[Dict]):
    """å½“æ‰¹é‡ä¿å­˜å¤±è´¥æ—¶ï¼Œå°è¯•é€ä¸ªä¿å­˜æ¶ˆæ¯
    
    Args:
        messages: æ¶ˆæ¯æ•°æ®åˆ—è¡¨
    """
    successful = 0
    from src.database.db_factory import get_db_adapter
    db_adapter = get_db_adapter()
    for msg_data in messages:
        try:
            exists = await db_adapter.check_message_exists(msg_data['chain'], msg_data['message_id'])
            if exists:
                logger.info(f"æ¶ˆæ¯ {msg_data['chain']}-{msg_data['message_id']} å·²å­˜åœ¨ï¼Œé€ä¸ªä¿å­˜æ—¶è·³è¿‡")
                continue
            # ä½¿ç”¨å¼‚æ­¥æ–¹å¼ä¿å­˜
            if await save_telegram_message(
                chain=msg_data['chain'],
                message_id=msg_data['message_id'],
                date=msg_data['date'],
                text=msg_data['text'],
                media_path=msg_data.get('media_path'),
                channel_id=msg_data.get('channel_id')
            ):
                successful += 1
        except Exception as individual_error:
            logger.error(f"å•ç‹¬ä¿å­˜æ¶ˆæ¯ {msg_data['message_id']} æ—¶å‡ºé”™: {individual_error}")
            import traceback
            logger.debug(traceback.format_exc())
    logger.info(f"é€ä¸ªä¿å­˜: æˆåŠŸ {successful}/{len(messages)} æ¡æ¶ˆæ¯")

async def save_telegram_message(
    chain: str,
    message_id: int,
    date: datetime,
    text: str,
    media_path: Optional[str] = None,
    channel_id: Optional[int] = None
) -> bool:
    """ä¿å­˜Telegramæ¶ˆæ¯åˆ°æ•°æ®åº“
    
    Args:
        chain: åŒºå—é“¾åç§°
        message_id: æ¶ˆæ¯ID
        date: æ¶ˆæ¯æ—¥æœŸ
        text: æ¶ˆæ¯æ–‡æœ¬
        media_path: åª’ä½“æ–‡ä»¶è·¯å¾„
        channel_id: é¢‘é“æˆ–ç¾¤ç»„ID
        
    Returns:
        bool: æ“ä½œæ˜¯å¦æˆåŠŸ
    """
    global message_batch
    if MAX_BATCH_SIZE > 0:
        from src.database.db_factory import get_db_adapter
        db_adapter = get_db_adapter()
        exists = await db_adapter.check_message_exists(chain, message_id)
        if exists:
            logger.info(f"æ¶ˆæ¯ {chain}-{message_id} å·²å­˜åœ¨ï¼Œè·³è¿‡å…¥é˜Ÿå’Œä¿å­˜")
            return False
        message_batch.append({
            'chain': chain,
            'message_id': message_id,
            'date': date,
            'text': text,
            'media_path': media_path,
            'channel_id': channel_id
        })
        if len(message_batch) >= MAX_BATCH_SIZE:
            asyncio.create_task(process_message_batch())
        return True
    try:
        from src.database.db_factory import get_db_adapter
        db_adapter = get_db_adapter()
        exists = await db_adapter.check_message_exists(chain, message_id)
        if exists:
            logger.info(f"æ¶ˆæ¯ {chain}-{message_id} å·²å­˜åœ¨ï¼Œè·³è¿‡ä¿å­˜")
            return False
        message_data = {
            'chain': chain,
            'message_id': message_id,
            'date': date.isoformat() if isinstance(date, datetime) else date,
            'text': text,
            'media_path': media_path,
            'channel_id': channel_id
        }
        result = await db_adapter.save_message(message_data)
        return result
    except Exception as e:
        logger.error(f"ä¿å­˜æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}")
        import traceback
        logger.debug(traceback.format_exc())
        return False

async def process_message_batch():
    """å¤„ç†æ¶ˆæ¯æ‰¹å¤„ç†é˜Ÿåˆ—"""
    global message_batch
    
    if not message_batch:
        return
        
    # å¤åˆ¶å½“å‰é˜Ÿåˆ—ï¼Œå¹¶æ¸…ç©ºå…¨å±€é˜Ÿåˆ—
    current_batch = message_batch.copy()
    message_batch = []
    
    logger.info(f"å¤„ç†æ¶ˆæ¯æ‰¹å¤„ç†é˜Ÿåˆ—ï¼Œå…± {len(current_batch)} æ¡æ¶ˆæ¯")
    
    try:
        saved_count = await save_messages_batch(current_batch)
        logger.info(f"æˆåŠŸæ‰¹é‡ä¿å­˜ {saved_count}/{len(current_batch)} æ¡æ¶ˆæ¯")
        
        # å¦‚æœæ‰¹é‡ä¿å­˜å¤±è´¥ï¼Œå°è¯•é€ä¸ªä¿å­˜
        if saved_count < len(current_batch):
            logger.warning("æ‰¹é‡ä¿å­˜éƒ¨åˆ†å¤±è´¥ï¼Œå°è¯•é€ä¸ªä¿å­˜å‰©ä½™æ¶ˆæ¯")
            await save_messages_individually(current_batch)
    except Exception as e:
        logger.error(f"å¤„ç†æ¶ˆæ¯æ‰¹å¤„ç†æ—¶å‡ºé”™: {str(e)}")
        # å‡ºé”™æ—¶å°è¯•é€ä¸ªä¿å­˜
        try:
            await save_messages_individually(current_batch)
        except Exception as e2:
            logger.error(f"é€ä¸ªä¿å­˜æ¶ˆæ¯æ—¶ä¹Ÿå‡ºé”™: {str(e2)}")
            import traceback
            logger.debug(traceback.format_exc())

def save_tokens_batch(tokens: List[Dict]):
    """æ‰¹é‡ä¿å­˜ä»£å¸ä¿¡æ¯åˆ°æ•°æ®åº“
    
    è­¦å‘Šï¼šæ­¤å‡½æ•°å·²è¢«åºŸå¼ƒï¼Œä¸åº”å†ä½¿ç”¨ã€‚
    æ­¤å‡½æ•°åŸæœ¬ç”¨äºæ‰¹é‡å¤„ç†ä»£å¸ä¿¡æ¯ï¼Œä½†ç°åœ¨å·²ç”±process_batcheså‡½æ•°æ›¿ä»£ã€‚
    åœ¨ä»£ç ä¸­ç›´æ¥ä½¿ç”¨process_batcheså¤„ç†token_batchå˜é‡ï¼Œè€Œä¸è¦è°ƒç”¨æ­¤å‡½æ•°ã€‚
    
    ä»…ä¸ºäº†ä¿æŒå‘åå…¼å®¹æ€§è€Œä¿ç•™ã€‚å°†åœ¨ä¸‹ä¸€æ¬¡ä¸»è¦ç‰ˆæœ¬æ›´æ–°ä¸­å½»åº•ç§»é™¤ã€‚
    
    Args:
        tokens: ä»£å¸ä¿¡æ¯åˆ—è¡¨
        
    Returns:
        æ›´æ–°çš„ä»£å¸æ•°é‡
    """
    logger.warning("è°ƒç”¨äº†åºŸå¼ƒçš„save_tokens_batchå‡½æ•°ï¼Œè¯·æ”¹ç”¨process_batcheså¤„ç†token_batch")
    
    if not tokens:
        return 0
    
    # ä½¿ç”¨æ•°æ®åº“é€‚é…å™¨
    from src.database.db_factory import get_db_adapter
    db_adapter = get_db_adapter()
    
    # ä½¿ç”¨é‡è¯•æœºåˆ¶
    for attempt in range(OPERATION_RETRIES):
        try:
            # å¤„ç†æ¯ä¸ªä»£å¸ä¿¡æ¯
            updated_count = 0
            for token_data in tokens:
                token_symbol = token_data.get('token_symbol')
                chain = token_data.get('chain')
                contract = token_data.get('contract')
                
                if not token_symbol or not chain:
                    logger.warning(f"è·³è¿‡æ— æ•ˆçš„ä»£å¸æ•°æ®: ç¼ºå°‘token_symbolæˆ–chain")
                    continue
                
                # æ ‡å‡†åŒ–é£é™©ç­‰çº§å€¼
                if 'risk_level' in token_data:
                    risk_level = token_data['risk_level']
                    # ç¡®ä¿é£é™©ç­‰çº§æ˜¯æœ‰æ•ˆå€¼
                    if risk_level not in ['low', 'medium', 'high', 'medium-high', 'low-medium', 'unknown']:
                        # å¤„ç†ä¸­æ–‡é£é™©ç­‰çº§ï¼Œç»Ÿä¸€è½¬ä¸ºè‹±æ–‡
                        if risk_level == 'ä½':
                            token_data['risk_level'] = 'low'
                        elif risk_level == 'ä¸­':
                            token_data['risk_level'] = 'medium'
                        elif risk_level == 'é«˜':
                            token_data['risk_level'] = 'high'
                        elif not risk_level:
                            token_data['risk_level'] = 'unknown'
                
                # å¤„ç†æ—¥æœŸæ—¶é—´ç±»å‹
                for key, value in token_data.items():
                    if isinstance(value, datetime):
                        token_data[key] = value.isoformat()
                
                # ä½¿ç”¨äº‹ä»¶å¾ªç¯æ‰§è¡Œå¼‚æ­¥ä¿å­˜æ–¹æ³•
                loop = asyncio.new_event_loop()
                try:
                    result = loop.run_until_complete(db_adapter.save_token(token_data))
                    if result:
                        updated_count += 1
                        
                    # å¦‚æœæœ‰contractå­—æ®µï¼Œä¿å­˜token markä¿¡æ¯
                    if contract:
                        token_exists = loop.run_until_complete(db_adapter.get_token_by_contract(chain, contract))
                        if token_exists:
                            # ä¿å­˜ä»£å¸æ ‡è®°
                            loop.run_until_complete(db_adapter.save_token_mark(token_data))
                finally:
                    loop.close()
            
            logger.debug(f"æ›´æ–°/æ·»åŠ äº† {updated_count} æ¡ä»£å¸ä¿¡æ¯")
            return updated_count
            
        except Exception as e:
            logger.error(f"æ‰¹é‡ä¿å­˜ä»£å¸ä¿¡æ¯æ—¶å‡ºé”™(å°è¯• {attempt+1}/{OPERATION_RETRIES}): {str(e)}")
            import traceback
            logger.debug(traceback.format_exc())
            
            # å¦‚æœè¿˜æœ‰é‡è¯•æ¬¡æ•°ï¼Œç­‰å¾…åé‡è¯•
            if attempt < OPERATION_RETRIES - 1:
                time.sleep(OPERATION_RETRY_DELAY * (attempt + 1))  # æŒ‡æ•°é€€é¿
            else:
                logger.error(f"æ‰¹é‡ä¿å­˜ä»£å¸ä¿¡æ¯å¤±è´¥ï¼Œè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                return 0

def save_token_info(token_data: Dict[str, Any]) -> bool:
    """ä¿å­˜ä»£å¸ä¿¡æ¯
    
    Args:
        token_data: ä»£å¸æ•°æ®å­—å…¸
        
    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    # å¢å¼ºæ•°æ®éªŒè¯
    if not isinstance(token_data, dict):
        logger.error(f"token_dataå¿…é¡»æ˜¯å­—å…¸ï¼Œä½†æ”¶åˆ°äº†: {type(token_data)}")
        return False
        
    # éªŒè¯å¿…éœ€å­—æ®µ
    required_fields = ['chain', 'token_symbol']
    for field in required_fields:
        if field not in token_data or not token_data[field]:
            logger.error(f"ä¿å­˜ä»£å¸ä¿¡æ¯å¤±è´¥: ç¼ºå°‘å¿…éœ€å­—æ®µ '{field}'")
            return False
            
    # ç‰¹åˆ«éªŒè¯contractå­—æ®µ - è¿™æ˜¯ä¸€ä¸ªå…³é”®å­—æ®µ
    if 'contract' not in token_data or not token_data['contract']:
        logger.error(f"ä¿å­˜ä»£å¸ä¿¡æ¯å¤±è´¥: ç¼ºå°‘å¿…éœ€å­—æ®µ 'contract'ï¼ˆä¸èƒ½ä¸ºnullï¼‰")
        return False
        
    # å¸¸è§„éªŒè¯
    valid, message = validate_token_data(token_data)
    if not valid:
        logger.warning(f"ä»£å¸æ•°æ®éªŒè¯å¤±è´¥: {message}")
        return False
        
    # ä½¿ç”¨æ•°æ®åº“é€‚é…å™¨
    from src.database.db_factory import get_db_adapter
    db_adapter = get_db_adapter()
    
    try:
        # å¤„ç†æ—¥æœŸæ—¶é—´ç±»å‹
        for key, value in token_data.items():
            if isinstance(value, datetime):
                token_data[key] = value.isoformat()
        
        # ä½¿ç”¨äº‹ä»¶å¾ªç¯æ‰§è¡Œå¼‚æ­¥ä¿å­˜æ–¹æ³•
        loop = asyncio.new_event_loop()
        try:
            result = loop.run_until_complete(db_adapter.save_token(token_data))
            return result
        finally:
            loop.close()
    except Exception as e:
        logger.error(f"ä¿å­˜ä»£å¸ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        logger.debug(f"é—®é¢˜æ•°æ®: {token_data}")
        import traceback
        logger.debug(traceback.format_exc())
        return False

def extract_promotion_info(message_text: str, date: datetime, chain: str = None, message_id: int = None, channel_id: int = None) -> List[PromotionInfo]:
    """
    æ‰¹é‡æå–æ‰€æœ‰åˆçº¦åœ°å€ï¼Œå¹¶å¯¹æ¯ä¸ªåˆçº¦åœ°å€ç‹¬ç«‹èµ°ä¸€éåŸæœ‰ä¸¥å¯†çš„é“¾æ¨æ–­ã€ç¬¦å·ã€é£é™©ã€å¸‚å€¼ç­‰ä¸»æµç¨‹ï¼Œè¿”å›PromotionInfoå¯¹è±¡åˆ—è¡¨ã€‚
    ä¿è¯æ¯ä¸ªPromotionInfoçš„chainå­—æ®µéƒ½ä¸ºå…·ä½“é“¾ï¼ˆETH/BSC/ARBç­‰ï¼‰ï¼Œç»ä¸ä¼šä¸ºEVMã€‚
    æ—¥å¿—è¾“å‡ºå…¨éƒ¨è¿ç§»åˆ°ä¸»æµç¨‹ï¼Œåº•å±‚å‡½æ•°åªä¿ç•™å¼‚å¸¸å’Œè°ƒè¯•æ—¥å¿—ã€‚
    æ—¥å¿—è¾“å‡ºä¸¥æ ¼åŒºåˆ†è°ƒç”¨ç›®çš„ï¼šæ”¶é›†æ–°åœ°å€æ—¶è¾“å‡º"å‘ç°æ–°åœ°å€"æ—¥å¿—ï¼Œé“¾æ¨æ–­æ—¶åªè¾“å‡ºé“¾æ¨æ–­ç›¸å…³æ—¥å¿—ã€‚
    """
    results = []
    try:
        # 1. æ¸…ç†æ¶ˆæ¯æ–‡æœ¬
        cleaned_text = re.sub(r'\s+', ' ', message_text)
        cleaned_text = re.sub(r'[\u200b\u200c\u200d\ufeff]', '', cleaned_text)  # ç§»é™¤é›¶å®½å­—ç¬¦
        # 2. æ‰¹é‡æå–æ‰€æœ‰åˆçº¦åœ°å€ï¼ˆURL+æ­£åˆ™ï¼‰ï¼Œå¹¶è®°å½•æ¥æºå’Œé“¾ä¿¡æ¯ï¼Œä¾¿äºåç»­å”¯ä¸€æ—¥å¿—è¾“å‡º
        urls = find_urls_in_text(cleaned_text)
        address_info_map = dict()  # åˆçº¦åœ°å€ -> {'source': 'url'/'regex', 'url': url, 'chain': chain}
        # 2.1 ä»URLä¸­æå–åˆçº¦åœ°å€ï¼Œå¹¶åœ¨ä¸»æµç¨‹è¾“å‡ºæ—¥å¿—ï¼ˆç›®çš„ï¼šæ”¶é›†æ–°åœ°å€ï¼‰
        for url in urls:
            contract, detected_chain = extract_contract_from_url(url)
            if contract:
                # åªä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„æ¥æº
                if contract not in address_info_map:
                    address_info_map[contract] = {'source': 'url', 'url': url, 'chain': detected_chain}
                    # === åªåœ¨"æ”¶é›†æ–°åœ°å€"é˜¶æ®µè¾“å‡ºæ—¥å¿— ===
                    if detected_chain:
                        logger.info(f"ä»URL '{url}' æ£€æµ‹åˆ°é“¾: {detected_chain}")
                        if detected_chain == 'SOL':
                            logger.info(f"ä»URLç›´æ¥æå–åˆ°Solanaæ ¼å¼åœ°å€: {contract}")
                        elif detected_chain in EVM_CHAINS:
                            logger.info(f"ä»URLç›´æ¥æå–åˆ°EVMæ ¼å¼åœ°å€: {contract}, é“¾: {detected_chain}")
        # 2.2 ä»æ–‡æœ¬ä¸­æ­£åˆ™æå–åˆçº¦åœ°å€
        for pattern in CONTRACT_PATTERNS:
            for match in re.finditer(pattern, cleaned_text):
                potential_address = match.group(1) if '(' in pattern else match.group(0)
                if potential_address and potential_address not in address_info_map:
                    address_info_map[potential_address] = {'source': 'regex', 'url': None, 'chain': None}
        # 3. å¯¹æ¯ä¸ªå”¯ä¸€åˆçº¦åœ°å€ï¼Œç‹¬ç«‹èµ°ä¸€éåŸæœ‰ä¸»æµç¨‹
        for contract_address, info_dict in address_info_map.items():
            # === ä»¥ä¸‹ä¸ºåŸæœ‰extract_promotion_infoä¸»æµç¨‹ï¼Œé’ˆå¯¹å½“å‰contract_address ===
            # 3.1 ç‹¬ç«‹é“¾æ¨æ–­
            local_chain = chain
            # ä¼˜å…ˆä»URLåˆ¤æ–­é“¾ä¿¡æ¯
            chain_from_url = None
            for url in urls:
                c, detected_chain = extract_contract_from_url(url)
                if c == contract_address and detected_chain:
                    # === åªåœ¨é“¾æ¨æ–­å‘ç”Ÿå˜åŒ–æˆ–è¡¥å……ä¿¡æ¯æ—¶è¾“å‡ºé“¾æ¨æ–­ç›¸å…³æ—¥å¿— ===
                    if (not info_dict['chain']) or (info_dict['chain'] != detected_chain):
                        logger.info(f"é“¾æ¨æ–­ï¼šåˆçº¦åœ°å€{contract_address}åœ¨URL '{url}' æ£€æµ‹åˆ°é“¾ä¿¡æ¯: {detected_chain}")
                        if detected_chain == 'SOL':
                            logger.info(f"é“¾æ¨æ–­ï¼šåˆçº¦åœ°å€{contract_address}ä¸ºSolanaæ ¼å¼")
                        elif detected_chain in EVM_CHAINS:
                            logger.info(f"é“¾æ¨æ–­ï¼šåˆçº¦åœ°å€{contract_address}ä¸ºEVMæ ¼å¼ï¼Œé“¾: {detected_chain}")
                    chain_from_url = detected_chain
                    break
            if chain_from_url:
                local_chain = chain_from_url
            # å¦‚æœæœªæä¾›é“¾ä¿¡æ¯æˆ–ä¸ºUNKNOWNï¼Œå°è¯•ä»æ¶ˆæ¯ä¸­æå–
            if not local_chain or local_chain == "UNKNOWN":
                # å…ˆæ£€æŸ¥æ¶ˆæ¯ä¸­çš„å¸‚å€¼å•ä½æ¥åˆ¤æ–­é“¾
                if re.search(r'(\bmc\b|\bmarket\s*cap\b|å¸‚å€¼)[ï¼š:]*\s*[`\'"]*\d+(?:\.\d+)?\s*(?:bnb|BNB)', cleaned_text, re.IGNORECASE):
                    logger.info("ä»å¸‚å€¼å•ä½(BNB)åˆ¤æ–­ä¸ºBSCé“¾")
                    local_chain = 'BSC'
                elif re.search(r'(\bmc\b|\bmarket\s*cap\b|å¸‚å€¼)[ï¼š:]*\s*[`\'"]*\d+(?:\.\d+)?\s*(?:eth|ETH)', cleaned_text, re.IGNORECASE):
                    logger.info("ä»å¸‚å€¼å•ä½(ETH)åˆ¤æ–­ä¸ºETHé“¾")
                    local_chain = 'ETH'
                elif re.search(r'(\bmc\b|\bmarket\s*cap\b|å¸‚å€¼)[ï¼š:]*\s*[`\'"]*\d+(?:\.\d+)?\s*(?:sol|SOL)', cleaned_text, re.IGNORECASE):
                    logger.info("ä»å¸‚å€¼å•ä½(SOL)åˆ¤æ–­ä¸ºSOLé“¾")
                    local_chain = 'SOL'
                else:
                    chain_from_message = extract_chain_from_message(message_text)
                    if chain_from_message:
                        logger.info(f"ä»æ¶ˆæ¯ä¸­æå–åˆ°é“¾ä¿¡æ¯: {chain_from_message}")
                        local_chain = chain_from_message
            # 3.2 æå–ä»£å¸ç¬¦å·ï¼ˆå¤šæ ¼å¼ï¼‰
            token_symbol = None
            symbol_match = re.search(r'\$([A-Za-z0-9_]{1,20})\b', cleaned_text)
            if symbol_match:
                token_symbol = symbol_match.group(1).upper()
                logger.info(f"ä»æ¶ˆæ¯ä¸­æå–åˆ°ä»£å¸ç¬¦å·: {token_symbol}")
            else:
                # å°è¯•ä»ç¬¬ä¸€è¡Œæˆ–å…¶ä»–æ ¼å¼ä¸­æå–ä»£å¸ç¬¦å·
                first_line = cleaned_text.split('\n')[0] if '\n' in cleaned_text else cleaned_text
                symbol_patterns = [
                    r'"([A-Za-z0-9_]{1,10})"',
                    r'\'([A-Za-z0-9_]{1,10})\'',
                    r'\*\*([A-Za-z0-9_]{1,10})\*\*',
                    r'`([A-Za-z0-9_]{1,10})`'
                ]
                for pattern in symbol_patterns:
                    match = re.search(pattern, first_line)
                    if match:
                        potential_symbol = match.group(1).upper()
                        common_words = ['NEW', 'TOKEN', 'CONTRACT', 'ADDRESS', 'LINK', 'ALPHA']
                        if potential_symbol not in common_words and len(potential_symbol) >= 2:
                            token_symbol = potential_symbol
                            logger.info(f"ä»æ¶ˆæ¯æ ¼å¼ä¸­æå–åˆ°ä»£å¸ç¬¦å·: {token_symbol}")
                            break
            # 3.3 åˆçº¦æ ¼å¼åˆ¤æ–­å’Œé“¾ä¿®æ­£
            address_chain = get_chain_from_address(contract_address)
            if address_chain == 'EVM':
                if local_chain not in EVM_CHAINS:
                    for evm_chain in EVM_CHAINS:
                        for keyword in CHAINS.get(evm_chain, []):
                            if keyword.lower() in cleaned_text.lower():
                                logger.warning(f"æœ€ç»ˆæ£€æŸ¥: åˆçº¦åœ°å€{contract_address}æ˜¯EVMæ ¼å¼ï¼Œä½†é“¾ä¸º{local_chain}ï¼Œä¸Šä¸‹æ–‡æš—ç¤ºåº”ä¸º{evm_chain}é“¾")
                                local_chain = evm_chain
                                break
                        if local_chain in EVM_CHAINS:
                            break
                    if local_chain not in EVM_CHAINS:
                        logger.warning(f"æœ€ç»ˆæ£€æŸ¥: åˆçº¦åœ°å€{contract_address}æ˜¯EVMæ ¼å¼ï¼Œä½†é“¾ä¸º{local_chain}ï¼Œè¿™ä¸åŒ¹é…ã€‚ä¿®æ­£ä¸ºBSC")
                        local_chain = 'BSC'
            elif address_chain == 'SOL':
                if local_chain != 'SOL':
                    logger.warning(f"æœ€ç»ˆæ£€æŸ¥: åˆçº¦åœ°å€{contract_address}æ˜¯SOLæ ¼å¼ï¼Œä½†é“¾ä¸º{local_chain}ï¼Œè¿™ä¸åŒ¹é…ã€‚ä¿®æ­£ä¸ºSOL")
                    local_chain = 'SOL'
            elif not local_chain or local_chain == 'UNKNOWN':
                if address_chain == 'EVM':
                    for evm_chain in EVM_CHAINS:
                        for keyword in CHAINS.get(evm_chain, []):
                            if keyword.lower() in cleaned_text.lower():
                                logger.info(f"é€šè¿‡å…³é”®è¯å°†EVMåœ°å€å½’ç±»ä¸º{evm_chain}é“¾")
                                local_chain = evm_chain
                                break
                        if local_chain and local_chain != 'UNKNOWN':
                            break
                    if not local_chain or local_chain == 'UNKNOWN':
                        local_chain = 'BSC'
                        logger.info("æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„EVMé“¾å…³é”®è¯ï¼Œé»˜è®¤è®¾ç½®ä¸ºBSC")
                elif address_chain == 'SOL':
                    local_chain = 'SOL'
            # 3.4 PromotionInfoå¯¹è±¡æ„å»º
            info = PromotionInfo(
                token_symbol=token_symbol,
                contract_address=contract_address,
                chain=local_chain,
                promotion_count=1,
                first_trending_time=date
            )
            info.message_id = message_id
            info.channel_id = channel_id
            # 3.5 é£é™©è¯„çº§æå–
            risk_level = None
            risk_patterns = [
                r'[Rr]isk[ï¼š:]\s*([A-Za-z]+)',
                r'é£é™©[ï¼š:]\s*([A-Za-zé«˜ä¸­ä½]+)',
                r'[Ss]afe[ï¼š:]\s*([A-Za-z]+)',
                r'å®‰å…¨[ï¼š:]\s*([A-Za-zæ˜¯å¦]+)'
            ]
            for pattern in risk_patterns:
                match = re.search(pattern, cleaned_text)
                if match:
                    risk_text = match.group(1).strip().lower()
                    if risk_text in ['high', 'é«˜', 'high risk']:
                        risk_level = 'high'
                    elif risk_text in ['medium', 'mid', 'moderate', 'ä¸­']:
                        risk_level = 'medium'
                    elif risk_text in ['low', 'ä½', 'safe', 'yes', 'æ˜¯']:
                        risk_level = 'low'
                    break
            info.risk_level = risk_level
            # 3.6 å¸‚å€¼æå–ä¸é“¾ä¿®æ­£
            from src.utils.utils import parse_market_cap
            # ç»Ÿä¸€å¸‚å€¼å­—æ®µæå–é€»è¾‘ï¼Œä¸åŒºåˆ†USDå¸‚å€¼ä¸å¸‚å€¼ï¼Œæ‰€æœ‰å¸‚å€¼ç›¸å…³å­—æ®µä¸€è§†åŒä»
            # åŒ¹é…"å¸‚å€¼"ã€"Market Cap"ã€"Mc"ç­‰å¸¸è§å†™æ³•
            market_cap_text = re.search(r'([Mm]arket\s*[Cc]ap|å¸‚å€¼|[Mm][Cc])[ï¼š:]*\s*[`\'\"]?([^,\n]+)', cleaned_text)
            if market_cap_text:
                mc_value = market_cap_text.group(2).strip()
                # === åªä¿ç•™ç¬¬ä¸€ä¸ª"æ•°å­—+å•ä½"ç‰‡æ®µï¼Œä¸¢å¼ƒåç»­æ‰€æœ‰éå¸‚å€¼å†…å®¹ ===
                # å…è®¸æ ¼å¼å¦‚ 3.35Mã€3369587ã€100ä¸‡ã€2.1B ç­‰ï¼Œé˜²æ­¢æ··æ‚å†…å®¹å¯¼è‡´è§£æå¤±è´¥
                mc_match = re.match(r'([0-9.,]+\s*[a-zA-Zä¸‡äº¿KMB]*)', mc_value)
                if mc_match:
                    mc_clean = mc_match.group(1).replace(' ', '')
                else:
                    # å…œåº•ï¼šåªå–ç¬¬ä¸€ä¸ªå•è¯ï¼Œæœ€å¤§ç¨‹åº¦ä¿è¯å¥å£®æ€§
                    mc_clean = mc_value.split()[0]
                try:
                    # åªå°†çº¯å‡€çš„å¸‚å€¼å­—ç¬¦ä¸²ä¼ å…¥è§£æå‡½æ•°ï¼Œæå¤§é™ä½å¼‚å¸¸æ¦‚ç‡
                    parsed_mc = parse_market_cap(mc_clean)
                    if parsed_mc:
                        info.market_cap = str(parsed_mc)
                        mc_lower = mc_clean.lower()
                        # é“¾æ¨æ–­é€»è¾‘ä¿ç•™
                        if 'bnb' in mc_lower and (not info.chain or info.chain == 'UNKNOWN'):
                            info.chain = 'BSC'
                        elif 'eth' in mc_lower and (not info.chain or info.chain == 'UNKNOWN'):
                            info.chain = 'ETH'
                        elif 'sol' in mc_lower and (not info.chain or info.chain == 'UNKNOWN'):
                            info.chain = 'SOL'
                except Exception as e:
                    # è‹¥è§£æå¤±è´¥ï¼Œè¯¦ç»†è®°å½•åŸå§‹å†…å®¹å’Œå¼‚å¸¸ä¿¡æ¯ï¼Œä¾¿äºåç»­æ’æŸ¥
                    logger.warning(f"è§£æå¸‚å€¼å‡ºé”™: {mc_value}, é”™è¯¯: {str(e)}")
            # 3.7 æœ€ç»ˆä¿é™©ï¼Œchainç»ä¸ä¸ºEVM
            if info.chain == 'EVM' or not info.chain or info.chain == 'UNKNOWN':
                chain_from_msg = extract_chain_from_message(message_text)
                if chain_from_msg in EVM_CHAINS:
                    info.chain = chain_from_msg
                else:
                    info.chain = 'ETH'
            results.append(info)
        return results
    except Exception as e:
        logger.error(f"æ‰¹é‡æå–åˆçº¦åœ°å€æ—¶å‡ºé”™: {str(e)}")
        logger.debug(traceback.format_exc())
        return results

def extract_single_promotion_info(message_text: str, date: datetime, chain: str = None, message_id: int = None, channel_id: int = None) -> Optional[PromotionInfo]:
    """
    å…¼å®¹æ—§æ¥å£ï¼Œåªè¿”å›ç¬¬ä¸€ä¸ªPromotionInfoå¯¹è±¡
    """
    infos = extract_promotion_info(message_text, date, chain, message_id, channel_id)
    return infos[0] if infos else None

def extract_chain_from_message(message_text: str) -> Optional[str]:
    """ä»æ¶ˆæ¯æ–‡æœ¬ä¸­æå–åŒºå—é“¾ä¿¡æ¯
    
    Args:
        message_text: éœ€è¦è§£æçš„æ¶ˆæ¯æ–‡æœ¬
        
    Returns:
        str: æå–åˆ°çš„é“¾åç§°ï¼Œæœªæ‰¾åˆ°åˆ™è¿”å›None
    """
    if not message_text:
        return None
        
    # æ¸…ç†æ¶ˆæ¯æ–‡æœ¬ï¼Œä¾¿äºåŒ¹é…
    text = message_text.lower()
    
    # é¦–å…ˆæ£€æŸ¥URLä¸­æ˜¯å¦åŒ…å«é“¾ä¿¡æ¯ï¼Œè¿™é€šå¸¸æœ€å¯é 
    urls = find_urls_in_text(text)
    for url in urls:
        chain = get_chain_from_url(url)
        if chain:
            logger.info(f"ä»URL '{url}' æ£€æµ‹åˆ°é“¾: {chain}")
            return chain
    
    # é€šè¿‡å¸‚å€¼å•ä½åˆ¤æ–­é“¾ï¼ˆé«˜ä¼˜å…ˆçº§åˆ¤æ–­ï¼‰
    mc_patterns = {
        'BSC': r'(\bmc\b|\bmarket\s*cap\b|å¸‚å€¼)[ï¼š:]*\s*[`\'"]*\d+(?:\.\d+)?\s*(?:bnb|BNB)',
        'ETH': r'(\bmc\b|\bmarket\s*cap\b|å¸‚å€¼)[ï¼š:]*\s*[`\'"]*\d+(?:\.\d+)?\s*(?:eth|ETH)',
        'SOL': r'(\bmc\b|\bmarket\s*cap\b|å¸‚å€¼)[ï¼š:]*\s*[`\'"]*\d+(?:\.\d+)?\s*(?:sol|SOL)',
        'ARB': r'(\bmc\b|\bmarket\s*cap\b|å¸‚å€¼)[ï¼š:]*\s*[`\'"]*\d+(?:\.\d+)?\s*(?:arb|ARB)',
        'AVAX': r'(\bmc\b|\bmarket\s*cap\b|å¸‚å€¼)[ï¼š:]*\s*[`\'"]*\d+(?:\.\d+)?\s*(?:avax|AVAX)',
        'MATIC': r'(\bmc\b|\bmarket\s*cap\b|å¸‚å€¼)[ï¼š:]*\s*[`\'"]*\d+(?:\.\d+)?\s*(?:matic|MATIC|polygon)',
        'OP': r'(\bmc\b|\bmarket\s*cap\b|å¸‚å€¼)[ï¼š:]*\s*[`\'"]*\d+(?:\.\d+)?\s*(?:op|OP|optimism)'
    }
    
    for chain, pattern in mc_patterns.items():
        if re.search(pattern, text, re.IGNORECASE):
            logger.info(f"ä»å¸‚å€¼å•ä½åˆ¤æ–­ä¸º{chain}é“¾")
            return chain
    
    # æ£€æŸ¥æ¨ç‰¹è´¦å·å…³é”®è¯
    twitter_match = re.search(r'(?:twitter\.com|x\.com)/([^/\s]+)', text)
    if twitter_match:
        twitter_user = twitter_match.group(1).lower()
        if twitter_user in TWITTER_CHAIN_MAP:
            logger.info(f"ä»æ¨ç‰¹è´¦å· '{twitter_user}' è¯†åˆ«é“¾ä¸º: {TWITTER_CHAIN_MAP[twitter_user]}")
            return TWITTER_CHAIN_MAP[twitter_user]
    
    # æ£€æŸ¥æœºå™¨äººå¼•ç”¨æˆ–é¢‘é“åç§°
    bot_patterns = {
        'SOL': [r'solana_trojanbot', r'solana.*bot', r'sol.*alert'],
        'BSC': [r'ape\.bot', r'sigma_buybot.*bsc', r'pancakeswap_bot', r'bnb.*bot', r'bsc.*alert'],
        'ETH': [r'uniswap_bot', r'sigma_buybot.*eth', r'eth.*alert', r'ethereum.*bot'],
        'ARB': [r'arb.*bot', r'arbitrum.*alert'],
        'AVAX': [r'avax.*bot', r'avalanche.*alert'],
        'MATIC': [r'polygon.*bot', r'matic.*alert'],
        'BASE': [r'base.*bot', r'base.*alert'],
        'OP': [r'optimism.*bot', r'op.*alert']
    }
    
    for chain, patterns in bot_patterns.items():
        for pattern in patterns:
            if re.search(pattern, text, re.IGNORECASE):
                logger.info(f"ä»æœºå™¨äºº/é¢‘é“åç§°å¼•ç”¨æå–åˆ°é“¾ä¿¡æ¯: {chain}, åŒ¹é…æ¨¡å¼: {pattern}")
                return chain
    
    # æ£€æŸ¥æ˜ç¡®çš„é“¾æ ‡è¯†ç¬¦ï¼ˆå¦‚#BSCã€#ETHã€#SOLç­‰)
    tag_patterns = {
        'BSC': r'(?:^|\s)#(?:bsc|bnb|binance)(?:\s|$)',
        'ETH': r'(?:^|\s)#(?:eth|ethereum)(?:\s|$)',
        'SOL': r'(?:^|\s)#(?:sol|solana)(?:\s|$)',
        'ARB': r'(?:^|\s)#(?:arb|arbitrum)(?:\s|$)',
        'AVAX': r'(?:^|\s)#(?:avax|avalanche)(?:\s|$)',
        'MATIC': r'(?:^|\s)#(?:matic|polygon)(?:\s|$)',
        'BASE': r'(?:^|\s)#(?:base)(?:\s|$)',
        'OP': r'(?:^|\s)#(?:op|optimism)(?:\s|$)'
    }
    
    for chain, pattern in tag_patterns.items():
        if re.search(pattern, text, re.IGNORECASE):
            logger.info(f"ä»æ ‡ç­¾æå–åˆ°é“¾ä¿¡æ¯: {chain}")
            return chain
    
    # æ£€æŸ¥æ¶ˆæ¯ä¸­æ˜¯å¦åŒ…å«åœ°å€æ ¼å¼ï¼Œç”¨äºæ¨æ–­é“¾ç±»å‹
    evm_address = re.search(r'\b0x[0-9a-fA-F]{40}\b', text)
    if evm_address:
        # å¦‚æœæ‰¾åˆ°EVMåœ°å€ï¼Œå°è¯•ä»ä¸Šä¸‹æ–‡ç¡®å®šå…·ä½“é“¾
        for chain, keywords in CHAINS.items():
            if chain in EVM_CHAINS:  # ä»…æ£€æŸ¥EVMé“¾
                for keyword in keywords:
                    if keyword.lower() in text:
                        logger.info(f"ä»ä¸Šä¸‹æ–‡({keyword})å’ŒEVMåœ°å€æ ¼å¼æ¨æ–­ä¸º{chain}é“¾")
                        return chain
        
        # å¦‚æœæ²¡æœ‰ç‰¹å®šå…³é”®è¯ï¼Œå°è¯•æ£€æŸ¥ç½‘ç»œè´¹ç”¨ç›¸å…³æœ¯è¯­
        fee_patterns = {
            'ETH': [r'gas\s+(?:fee|price)', r'gwei', r'gas\s+limit'],
            'BSC': [r'bnb\s+(?:fee|gas)', r'gwei.*bnb'],
            'ARB': [r'arb\s+(?:fee|gas)', r'gwei.*arb'],
            'MATIC': [r'matic\s+(?:fee|gas)', r'gwei.*matic'],
        }
        
        for chain, patterns in fee_patterns.items():
            for pattern in patterns:
                if re.search(pattern, text, re.IGNORECASE):
                    logger.info(f"ä»ç½‘ç»œè´¹ç”¨æœ¯è¯­æ¨æ–­ä¸º{chain}é“¾")
                    return chain
        
        # æ£€æŸ¥å¸å¯¹æè¿°
        pair_patterns = {
            'BSC': [r'\b(?:bnb|busd)/[a-z0-9]+\b', r'\b[a-z0-9]+/(?:bnb|busd)\b'],
            'ETH': [r'\b(?:eth|usdt)/[a-z0-9]+\b', r'\b[a-z0-9]+/(?:eth|usdt)\b'],
            'SOL': [r'\b(?:sol|usdc)/[a-z0-9]+\b', r'\b[a-z0-9]+/(?:sol|usdc)\b']
        }
        
        for chain, patterns in pair_patterns.items():
            for pattern in patterns:
                if re.search(pattern, text, re.IGNORECASE):
                    logger.info(f"ä»äº¤æ˜“å¯¹æè¿°æ¨æ–­ä¸º{chain}é“¾")
                    return chain
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰è¯†åˆ«åˆ°ï¼Œé»˜è®¤è¿”å›BSCï¼ˆä½œä¸ºæœ€å¸¸è§çš„EVMé“¾ï¼‰
        logger.warning("æ£€æµ‹åˆ°EVMæ ¼å¼åœ°å€ä½†æ— æ³•ç¡®å®šå…·ä½“é“¾ï¼Œé»˜è®¤è®¾ç½®ä¸ºBSC(æœ€å¸¸è§çš„EVMé“¾)")
        return 'BSC'
    
    # æ£€æŸ¥æ˜¯å¦æœ‰Solanaæ ¼å¼åœ°å€
    solana_address = re.search(r'\b[1-9A-HJ-NP-Za-km-z]{32,44}\b', text)
    if solana_address and ('sol' in text or 'solana' in text):
        logger.info("ä»åˆçº¦åœ°å€æ ¼å¼å’ŒSOLå…³é”®è¯æ¨æ–­ä¸ºSOLé“¾")
        return 'SOL'
    
    # ä»æ¶ˆæ¯æ–‡æœ¬ä¸­å¯»æ‰¾æœ€é¢‘ç¹å‡ºç°çš„é“¾ç›¸å…³è¯æ±‡
    chain_mentions = {}
    for chain, keywords in CHAINS.items():
        mentions = 0
        for keyword in keywords:
            mentions += len(re.findall(rf'\b{re.escape(keyword.lower())}\b', text))
        if mentions > 0:
            chain_mentions[chain] = mentions
    
    # å¦‚æœå­˜åœ¨é“¾æåŠï¼Œè¿”å›æåŠæœ€å¤šçš„é“¾
    if chain_mentions:
        most_mentioned = max(chain_mentions.items(), key=lambda x: x[1])
        logger.info(f"ä»å…³é”®è¯é¢‘ç‡åˆ†æï¼Œ'{most_mentioned[0]}'é“¾è¢«æåŠ{most_mentioned[1]}æ¬¡ï¼Œåˆ¤å®šä¸ºè¯¥é“¾")
        return most_mentioned[0]
    
    # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›None
    logger.debug("æ— æ³•ä»æ¶ˆæ¯ä¸­æå–é“¾ä¿¡æ¯")
    return None

def extract_url_from_text(text: str, keyword: str = '') -> Optional[str]:
    """ä»æ–‡æœ¬ä¸­æå–URLé“¾æ¥
    
    Args:
        text: è¦å¤„ç†çš„æ–‡æœ¬
        keyword: å¯é€‰çš„å…³é”®è¯ï¼Œç”¨äºç­›é€‰URL
        
    Returns:
        æå–å‡ºçš„URLï¼Œæˆ–None
    """
    if not text:
        return None
    
    try:
        # ä½¿ç”¨è¾…åŠ©å‡½æ•°è·å–æ‰€æœ‰URL
        urls = find_urls_in_text(text)
        
        if not urls:
            return None
            
        if keyword:
            # å¦‚æœæŒ‡å®šäº†å…³é”®è¯ï¼Œä¼˜å…ˆè¿”å›åŒ…å«å…³é”®è¯çš„URL
            for url in urls:
                if keyword.lower() in url.lower():
                    return url
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šå…³é”®è¯æˆ–æ²¡æœ‰æ‰¾åˆ°åŒ…å«å…³é”®è¯çš„URLï¼Œè¿”å›ç¬¬ä¸€ä¸ªURL
        return urls[0] if urls else None

    except Exception as e:
        logger.error(f"ä»æ–‡æœ¬ä¸­æå–URLæ—¶å‡ºé”™: {str(e)}")
        return None

def extract_contract_from_url(url: str) -> Tuple[Optional[str], Optional[str]]:
    """ä»URLä¸­æå–åˆçº¦åœ°å€å’Œé“¾ä¿¡æ¯
    
    Args:
        url: ç½‘å€
        
    Returns:
        Tuple[Optional[str], Optional[str]]: åˆçº¦åœ°å€å’Œé“¾ä¿¡æ¯çš„å…ƒç»„ï¼Œæœªæ‰¾åˆ°åˆ™è¿”å›(None, None)
    """
    if not url:
        return None, None
    
    try:
        # å¤„ç†URLä¸­å¸¸è§çš„éæ³•å­—ç¬¦å’Œæ ¼å¼é—®é¢˜
        url = url.split('#')[0].split('?')[0]  # ç§»é™¤URLä¸­çš„fragmentå’Œqueryéƒ¨åˆ†
        url_lower = url.lower()
        # 1. å¤„ç†ä¸“é—¨çš„ä»£å¸ä¿¡æ¯å¹³å°URL
        # GMGN.aiæ ¼å¼çš„URL
        # æ ¼å¼å¦‚: https://gmgn.ai/bsc/token/0x04e8f6a9e5765df0e5105bbc7ba6b562f8104444
        gmgn_match = re.search(r'(?:https?://)?(?:www\.)?gmgn\.ai(?:/[^/]+)?/([^/]+)/token/([a-zA-Z0-9]{20,})', url, re.IGNORECASE)
        if gmgn_match:
            chain_str = gmgn_match.group(1).upper()
            contract = gmgn_match.group(2)
            # æ˜ å°„åˆ°æ ‡å‡†é“¾æ ‡è¯†
            chain_map = {
                'BSC': 'BSC',
                'ETH': 'ETH',
                'ETHEREUM': 'ETH',
                'ARBITRUM': 'ARB',
                'BASE': 'BASE',
                'SOLANA': 'SOL',
                'POLYGON': 'MATIC',
                'OPTIMISM': 'OP',
                'AVALANCHE': 'AVAX'
            }
            chain = chain_map.get(chain_str, chain_str)
            return contract, chain
        # DexScreeneræ ¼å¼
        # ä¾‹å¦‚: https://dexscreener.com/solana/8WJ2ngd7FpHVkWiQTNyJ3N9j1oDmjR5e6MFdDAKQNinF
        dexscreener_pattern = r'(?:https?://)?(?:www\.)?dexscreener\.com/([a-zA-Z0-9]+)/([a-zA-Z0-9]{20,})'
        match = re.search(dexscreener_pattern, url)
        if match:
            chain_str = match.group(1).lower()
            contract = match.group(2)
            # æ˜ å°„åˆ°å†…éƒ¨é“¾æ ‡è¯†
            dexscreener_map = {
                'solana': 'SOL',
                'ethereum': 'ETH',
                'bsc': 'BSC',
                'arbitrum': 'ARB',
                'base': 'BASE',
                'avalanche': 'AVAX',
                'polygon': 'MATIC',
                'optimism': 'OP'
            }
            chain = dexscreener_map.get(chain_str)
            return contract, chain
        # GeckoTerminalæ ¼å¼
        # ä¾‹å¦‚: https://www.geckoterminal.com/eth/pools/0x1234...
        geckoterminal_pattern = r'(?:https?://)?(?:www\.)?geckoterminal\.com/([a-zA-Z0-9]+)/(?:pools|tokens)/([a-zA-Z0-9]{20,})'
        match = re.search(geckoterminal_pattern, url_lower)
        if match:
            chain_str = match.group(1).lower()
            contract = match.group(2)
            geckoterminal_map = {
                'sol': 'SOL',
                'solana': 'SOL',
                'eth': 'ETH',
                'ethereum': 'ETH',
                'bsc': 'BSC',
                'arb': 'ARB',
                'arbitrum': 'ARB',
                'base': 'BASE',
                'avax': 'AVAX',
                'avalanche': 'AVAX',
                'matic': 'MATIC',
                'polygon': 'MATIC',
                'op': 'OP',
                'optimism': 'OP'
            }
            chain = geckoterminal_map.get(chain_str)
            return contract, chain
        # CoinGeckoæ ¼å¼
        # ä¾‹å¦‚: https://www.coingecko.com/en/coins/ethereum/0x1234...
        coingecko_pattern = r'(?:https?://)?(?:www\.)?coingecko\.com/[^/]+/coins/([a-zA-Z0-9-]+)/([a-zA-Z0-9]{20,})'
        match = re.search(coingecko_pattern, url_lower)
        if match:
            chain_str = match.group(1).lower()
            contract = match.group(2)
            coingecko_map = {
                'solana': 'SOL',
                'ethereum': 'ETH',
                'binance-smart-chain': 'BSC',
                'arbitrum-one': 'ARB',
                'base': 'BASE',
                'avalanche': 'AVAX',
                'polygon-pos': 'MATIC',
                'optimistic-ethereum': 'OP'
            }
            chain = coingecko_map.get(chain_str)
            return contract, chain
        # 2. å¤„ç†åŒºå—æµè§ˆå™¨URL
        # å¾ªç¯æ£€æŸ¥å„ä¸ªåŒºå—æµè§ˆå™¨
        for chain, explorers in CHAIN_EXPLORERS.items():
            for explorer in explorers:
                if explorer in url_lower:
                    # æå–åˆçº¦åœ°å€
                    explorer_pattern = rf'(?:https?://)?(?:www\.)?{re.escape(explorer)}/(?:token|address|account|contracts)/([a-zA-Z0-9]{{20,}})'
                    explorer_match = re.search(explorer_pattern, url_lower)
                    if explorer_match:
                        contract = explorer_match.group(1)
                        # æ£€æŸ¥åˆçº¦åœ°å€æ ¼å¼
                        if chain != 'SOL' and contract.startswith('0x') and len(contract) >= 40:
                            return contract, chain
                        elif chain == 'SOL' and not contract.startswith('0x'):
                            return contract, chain
                        else:
                            # å°è¯•åœ¨URLä¸­å¯»æ‰¾æ­£ç¡®æ ¼å¼çš„åœ°å€
                            if chain != 'SOL':
                                evm_address = re.search(r'0x[a-fA-F0-9]{40}', url)
                                if evm_address:
                                    return evm_address.group(0), chain
                            else:
                                solana_address = re.search(r'[1-9A-HJ-NP-Za-km-z]{32,44}', url)
                                if solana_address:
                                    return solana_address.group(0), 'SOL'
        # 3. å¤„ç†DEXå’ŒæµåŠ¨æ€§å¹³å°URL
        # æ£€æŸ¥DEXå¹³å°URL
        for chain, patterns in DEX_PATTERNS.items():
            for pattern in patterns:
                if re.search(pattern, url_lower):
                    # æå–åˆçº¦åœ°å€
                    dex_contract = re.search(r'/([a-zA-Z0-9]{20,})', url)
                    if dex_contract:
                        contract = dex_contract.group(1)
                        return contract, chain
                    # å¦‚æœæ²¡æœ‰ç›´æ¥æ‰¾åˆ°ï¼Œå°è¯•æ ¹æ®é“¾ç±»å‹å¯»æ‰¾ç›¸åº”æ ¼å¼çš„åœ°å€
                    if chain != 'SOL':
                        evm_address = re.search(r'0x[a-fA-F0-9]{40}', url)
                        if evm_address:
                            return evm_address.group(0), chain
                    else:
                        solana_address = re.search(r'[1-9A-HJ-NP-Za-km-z]{32,44}', url)
                        if solana_address:
                            return solana_address.group(0), 'SOL'
        # 4. å¤„ç†å…¶ä»–å¸¸è§çš„ä»£å¸ä¿¡æ¯URLæ ¼å¼
        # æ¯”å¦‚: https://coinmarketcap.com/currencies/[token-name]/
        # æˆ– https://www.mexc.com/exchange/[TOKEN]_USDT
        exchange_patterns = [
            # Coinmarketcap - ä¸åŒ…å«åˆçº¦åœ°å€ï¼Œä½†å¯èƒ½æœ‰åŠ©äºç¡®å®šé“¾
            (r'(?:https?://)?(?:www\.)?coinmarketcap\.com/currencies/([a-zA-Z0-9-]+)', None),
            # Binance
            (r'(?:https?://)?(?:www\.)?binance\.com/[^/]+/trade/([A-Z0-9]+)_([A-Z0-9]+)', 'BSC'),
            # MEXC
            (r'(?:https?://)?(?:www\.)?mexc\.com/exchange/([A-Z0-9]+)_([A-Z0-9]+)', None)
        ]
        for pattern, default_chain in exchange_patterns:
            match = re.search(pattern, url)
            if match:
                if default_chain:
                    # å°è¯•ä»URLçš„å…¶ä»–éƒ¨åˆ†æå–åˆçº¦åœ°å€
                    evm_address = re.search(r'0x[a-fA-F0-9]{40}', url)
                    if evm_address:
                        return evm_address.group(0), default_chain
                    solana_address = re.search(r'[1-9A-HJ-NP-Za-km-z]{32,44}', url)
                    if solana_address and default_chain == 'SOL':
                        return solana_address.group(0), 'SOL'
        # 5. æœ€åå°è¯•ç›´æ¥ä»URLä¸­æå–åˆçº¦åœ°å€æ ¼å¼
        # è·å–URLä¸­æš—ç¤ºçš„é“¾ä¿¡æ¯
        chain_from_url = get_chain_from_url(url)
        # æ ¹æ®é“¾ç±»å‹å°è¯•æå–å¯¹åº”æ ¼å¼çš„åœ°å€
        if chain_from_url == 'SOL':
            solana_match = re.search(r'[1-9A-HJ-NP-Za-km-z]{32,44}', url)
            if solana_match:
                contract = solana_match.group(0)
                return contract, 'SOL'
        elif chain_from_url in EVM_CHAINS:
            evm_match = re.search(r'0x[a-fA-F0-9]{40}', url)
            if evm_match:
                contract = evm_match.group(0)
                return contract, chain_from_url
        else:
            # æ²¡æœ‰æ˜ç¡®çš„é“¾ä¿¡æ¯ï¼Œå°è¯•æå–ä»»ä½•æ ¼å¼çš„åœ°å€
            evm_match = re.search(r'0x[a-fA-F0-9]{40}', url)
            if evm_match:
                contract = evm_match.group(0)
                # å°è¯•ä»URLå…³é”®è¯åˆ¤æ–­é“¾
                if 'bsc' in url_lower or 'binance' in url_lower:
                    return contract, 'BSC'
                elif 'eth' in url_lower or 'ethereum' in url_lower:
                    return contract, 'ETH'
                # æ‰©å±•æ”¯æŒå…¶ä»–é“¾
                elif 'arb' in url_lower or 'arbitrum' in url_lower:
                    return contract, 'ARB'
                elif 'base' in url_lower:
                    return contract, 'BASE'
                elif 'matic' in url_lower or 'polygon' in url_lower:
                    return contract, 'MATIC'
                elif 'avax' in url_lower or 'avalanche' in url_lower:
                    return contract, 'AVAX'
                elif 'op' in url_lower or 'optimism' in url_lower:
                    return contract, 'OP'
                else:
                    return contract, None
            # å°è¯•æå–Solanaæ ¼å¼åœ°å€
            solana_match = re.search(r'[1-9A-HJ-NP-Za-km-z]{32,44}', url)
            if solana_match and ('sol' in url_lower or 'solana' in url_lower):
                contract = solana_match.group(0)
                return contract, 'SOL'
        logger.debug(f"æœªèƒ½ä»URLä¸­æå–åˆçº¦åœ°å€: {url}")
        return None, None
    except Exception as e:
        logger.error(f"ä»URLä¸­æå–åˆçº¦åœ°å€æ—¶å‡ºé”™: {str(e)}")
        import traceback
        logger.debug(traceback.format_exc())
        return None, None

def get_db_performance_stats():
    """è·å–æ•°æ®åº“æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        æ€§èƒ½ç»Ÿè®¡æ•°æ®å­—å…¸
    """
    stats = db_performance_stats.copy()
    
    # è®¡ç®—æ¯ä¸ªæ“ä½œçš„å¹³å‡æ‰§è¡Œæ—¶é—´
    avg_times = {}
    for op_name, total_time in stats['operation_times'].items():
        count = stats['operation_counts'].get(op_name, 0)
        if count > 0:
            avg_times[op_name] = total_time / count
        else:
            avg_times[op_name] = 0
    
    stats['average_times'] = avg_times
    
    # æ·»åŠ SupabaseçŠ¶æ€ä¿¡æ¯
    try:
        from src.database.db_factory import get_db_adapter
        db_adapter = get_db_adapter()
        stats['adapter_type'] = 'supabase'
        stats['database_uri'] = 'supabase://*****' # éšè—æ•æ„Ÿä¿¡æ¯
    except Exception as e:
        logger.error(f"è·å–æ•°æ®åº“çŠ¶æ€ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        stats['adapter_error'] = str(e)
    
    return stats

def reset_db_performance_stats():
    """é‡ç½®æ•°æ®åº“æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
    global db_performance_stats
    db_performance_stats = {
        'operation_counts': {},
        'operation_times': {},
        'lock_errors': 0,
        'total_retries': 0
    }

# æ·»åŠ ç¼ºå¤±çš„æ¸…ç†æ‰¹å¤„ç†ä»»åŠ¡å‡½æ•°
async def cleanup_batch_tasks():
    """æ¸…ç†æ‰€æœ‰æ‰¹å¤„ç†ä»»åŠ¡é˜Ÿåˆ—ï¼Œç¡®ä¿æ‰€æœ‰å¾…å¤„ç†æ•°æ®éƒ½è¢«ä¿å­˜
    
    åœ¨ç¨‹åºå…³é—­å‰è°ƒç”¨æ­¤å‡½æ•°ï¼Œä»¥é˜²æ­¢æ•°æ®ä¸¢å¤±
    """
    global message_batch, token_batch
    
    try:
        # å¤„ç†æ¶ˆæ¯æ‰¹å¤„ç†é˜Ÿåˆ—
        if message_batch:
            logger.info(f"æ¸…ç† {len(message_batch)} æ¡æœªå¤„ç†çš„æ¶ˆæ¯...")
            await process_message_batch()
        
        # å¤„ç†ä»£å¸æ‰¹å¤„ç†é˜Ÿåˆ—
        if token_batch:
            logger.info(f"æ¸…ç† {len(token_batch)} æ¡æœªå¤„ç†çš„ä»£å¸ä¿¡æ¯...")
            try:
                # å¤åˆ¶å½“å‰é˜Ÿåˆ—å¹¶æ¸…ç©ºå…¨å±€é˜Ÿåˆ—
                local_batch = token_batch.copy()
                token_batch = []
                
                # å®‰å…¨åœ°å¤„ç†æ¯ä¸ªä»£å¸æ•°æ®
                processed_count = 0
                for token_data in local_batch:
                    try:
                        if isinstance(token_data, dict) and 'contract' in token_data and token_data['contract']:
                            # ä½¿ç”¨å¢å¼ºçš„save_token_infoå‡½æ•°å¤„ç†å•ä¸ªä»£å¸ä¿¡æ¯
                            if save_token_info(token_data):
                                processed_count += 1
                        else:
                            logger.warning(f"è·³è¿‡æ— æ•ˆçš„ä»£å¸æ•°æ®: {token_data}")
                    except Exception as e:
                        logger.error(f"å¤„ç†å•ä¸ªä»£å¸æ•°æ®æ—¶å‡ºé”™: {str(e)}")
                
                logger.info(f"æˆåŠŸæ¸…ç† {processed_count}/{len(local_batch)} æ¡ä»£å¸ä¿¡æ¯")
            except Exception as e:
                logger.error(f"æ¸…ç†ä»£å¸ä¿¡æ¯é˜Ÿåˆ—æ—¶å‡ºé”™: {str(e)}")
                import traceback
                logger.debug(traceback.format_exc())
        
        logger.info("æ‰¹å¤„ç†ä»»åŠ¡é˜Ÿåˆ—æ¸…ç†å®Œæˆ")
        return True
    except Exception as e:
        logger.error(f"æ¸…ç†æ‰¹å¤„ç†ä»»åŠ¡é˜Ÿåˆ—æ—¶å‡ºé”™: {str(e)}")
        import traceback
        logger.debug(traceback.format_exc())
        return False

def calculate_community_reach(token_symbol: str, session=None):
    """è®¡ç®—ä»£å¸çš„ç¤¾ç¾¤è¦†ç›–äººæ•°
    
    è®¡ç®—æ–¹å¼ï¼š
    1. æ ¹æ®token_symbolæŸ¥è¯¢tokens_markè¡¨ä¸­çš„æ‰€æœ‰æ¡ç›®
    2. ç»Ÿè®¡æŸ¥è¯¢ç»“æœä¸­çš„å”¯ä¸€channel_id
    3. æ ¹æ®channel_idæŸ¥è¯¢telegram_channelsè¡¨ä¸­çš„member_count
    4. å°†member_countç›¸åŠ å¾—åˆ°community_reach
    
    Args:
        token_symbol: ä»£å¸ç¬¦å·
        session: åºŸå¼ƒå‚æ•°ï¼Œä¸ºäº†å…¼å®¹æ€§ä¿ç•™
        
    Returns:
        int: è®¡ç®—å¾—åˆ°çš„ç¤¾ç¾¤è¦†ç›–äººæ•°
    """
    # ä½¿ç”¨ç¼“å­˜é¿å…é¢‘ç¹è®¡ç®—ç›¸åŒçš„token
    cache_key = f"community_reach_{token_symbol}"
    if hasattr(calculate_community_reach, 'cache') and cache_key in calculate_community_reach.cache:
        # ç¼“å­˜æœ‰æ•ˆæœŸä¸º5åˆ†é’Ÿ
        cache_time, value = calculate_community_reach.cache[cache_key]
        if (datetime.now() - cache_time).total_seconds() < 300:  # 5åˆ†é’Ÿå†…
            return value

    # åˆå§‹åŒ–ç¼“å­˜å­—å…¸(å¦‚æœä¸å­˜åœ¨)
    if not hasattr(calculate_community_reach, 'cache'):
        calculate_community_reach.cache = {}
    
    try:
        # ä½¿ç”¨Supabaseé€‚é…å™¨
        from src.database.db_factory import get_db_adapter
        db_adapter = get_db_adapter()
        
        # ä½¿ç”¨å¼‚æ­¥è°ƒç”¨ä½†åœ¨åŒæ­¥ç¯å¢ƒä¸­æ‰§è¡Œ
        async def get_community_reach():
            # 1. è·å–æ‰€æœ‰æåŠè¯¥ä»£å¸çš„token_markè®°å½•
            token_marks = await db_adapter.execute_query(
                'tokens_mark',
                'select',
                filters={'token_symbol': token_symbol}
            )
            
            # å¦‚æœæ²¡æœ‰è®°å½•ï¼Œè¿”å›0
            if not token_marks:
                return 0
                
            # 2. æå–å”¯ä¸€çš„channel_id
            channel_ids = []
            for mark in token_marks:
                if isinstance(mark, dict) and mark.get('channel_id') and mark['channel_id'] not in channel_ids:
                    channel_ids.append(mark['channel_id'])
            
            # å¦‚æœæ²¡æœ‰channel_idï¼Œè¿”å›0
            if not channel_ids:
                return 0
                
            # 3. è·å–è¿™äº›é¢‘é“çš„æˆå‘˜æ•°
            total_reach = 0
            for channel_id in channel_ids:
                channel_info = await db_adapter.get_channel_by_id(channel_id)
                if channel_info and channel_info.get('member_count'):
                    total_reach += channel_info['member_count']
            
            return total_reach
        
        # æ‰§è¡Œå¼‚æ­¥å‡½æ•°ï¼Œä½¿ç”¨äº‹ä»¶å¾ªç¯
        loop = asyncio.new_event_loop()
        try:
            total_reach = loop.run_until_complete(get_community_reach())
        finally:
            loop.close()
                
        # å­˜å…¥ç¼“å­˜
        calculate_community_reach.cache[cache_key] = (datetime.now(), total_reach)
        
        # é™åˆ¶ç¼“å­˜å¤§å°ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
        if len(calculate_community_reach.cache) > 1000:  # æœ€å¤šç¼“å­˜1000ä¸ªtoken
            # åˆ é™¤æœ€æ—©çš„20%ç¼“å­˜
            sorted_keys = sorted(
                calculate_community_reach.cache.keys(),
                key=lambda k: calculate_community_reach.cache[k][0]
            )
            for key in sorted_keys[:200]:  # åˆ é™¤æœ€æ—©çš„200ä¸ª
                del calculate_community_reach.cache[key]
        
        # è¿”å›æ€»è¦†ç›–äººæ•°
        return total_reach
        
    except Exception as e:
        logger.error(f"è®¡ç®—ä»£å¸ {token_symbol} çš„ç¤¾ç¾¤è¦†ç›–äººæ•°æ—¶å‡ºé”™: {str(e)}")
        logger.debug(traceback.format_exc())
        return 0

# æ·»åŠ ç®€å•çš„ä»£å¸æ•°æ®éªŒè¯å‡½æ•°
def validate_token_data(token_data: Dict[str, Any]) -> Tuple[bool, str]:
    """
    éªŒè¯ä»£å¸æ•°æ®çš„å®Œæ•´æ€§
    
    Args:
        token_data: ä»£å¸æ•°æ®
        
    Returns:
        (bool, str): æ˜¯å¦æœ‰æ•ˆï¼Œé”™è¯¯ä¿¡æ¯
    """
    required_fields = ['chain', 'token_symbol', 'contract']
    
    # æ£€æŸ¥å¿…è¦å­—æ®µ
    for field in required_fields:
        if field not in token_data or not token_data[field]:
            return False, f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}"
    
    return True, ""

# æµ‹è¯•å‡½æ•°ï¼Œç”¨äºéªŒè¯é‡æ„æ˜¯å¦æ­£å¸¸å·¥ä½œ
def test_message_extraction():
    """
    æµ‹è¯•å‡½æ•°ï¼Œç”¨äºéªŒè¯é‡æ„åçš„ä¿¡æ¯æå–åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ
    æ­¤å‡½æ•°ä»…ç”¨äºæµ‹è¯•ï¼Œä¸åº”è¯¥åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è°ƒç”¨
    
    Returns:
        dict: æµ‹è¯•ç»“æœ
    """
    test_results = {}
    
    # æµ‹è¯•URLæå–
    test_results['url_extraction'] = {}
    url_tests = [
        {
            'text': 'è¯·æŸ¥çœ‹ä»¥ä¸‹é“¾æ¥: https://etherscan.io/token/0x1234567890abcdef1234567890abcdef12345678',
            'expected': 'https://etherscan.io/token/0x1234567890abcdef1234567890abcdef12345678'
        },
        {
            'text': 'ç½‘ç«™: www.example.comï¼Œè”ç³»æˆ‘ä»¬',
            'expected': 'www.example.com'
        },
        {
            'text': 'æ²¡æœ‰URLçš„æ–‡æœ¬',
            'expected': None
        }
    ]
    
    for i, test in enumerate(url_tests):
        result = extract_url_from_text(test['text'])
        test_results['url_extraction'][f'test_{i+1}'] = {
            'input': test['text'],
            'expected': test['expected'],
            'result': result,
            'passed': result == test['expected']
        }
    
    # æµ‹è¯•é“¾æå–
    test_results['chain_extraction'] = {}
    chain_tests = [
        {
            'text': 'è¿™æ˜¯BSCä¸Šçš„æ–°ä»£å¸ï¼Œé“¾æ¥: https://bscscan.com',
            'expected': 'BSC'
        },
        {
            'text': 'ç´¢æ‹‰çº³ä¸Šçš„NFTé¡¹ç›®å¾ˆçƒ­é—¨',
            'expected': 'SOL'
        },
        {
            'text': 'å¸‚å€¼: 100 BNBï¼Œä»·æ ¼...',
            'expected': 'BSC'
        },
        {
            'text': 'åˆçº¦åœ°å€: 0x1234567890abcdef1234567890abcdef12345678',
            'expected': 'BSC'  # é»˜è®¤EVMåœ°å€ä¸ºBSC
        }
    ]
    
    for i, test in enumerate(chain_tests):
        result = extract_chain_from_message(test['text'])
        test_results['chain_extraction'][f'test_{i+1}'] = {
            'input': test['text'],
            'expected': test['expected'],
            'result': result,
            'passed': result == test['expected']
        }
    
    # æµ‹è¯•åˆçº¦åœ°å€æå–
    test_results['contract_extraction'] = {}
    contract_tests = [
        {
            'url': 'https://etherscan.io/token/0x1234567890abcdef1234567890abcdef12345678',
            'expected_contract': '0x1234567890abcdef1234567890abcdef12345678',
            'expected_chain': 'ETH'
        },
        {
            'url': 'https://bscscan.com/address/0xabcdef1234567890abcdef1234567890abcdef12',
            'expected_contract': '0xabcdef1234567890abcdef1234567890abcdef12',
            'expected_chain': 'BSC'
        },
        {
            'url': 'https://solscan.io/token/8WJ2ngd7FpHVkWiQTNyJ3N9j1oDmjR5e6MFdDAKQNinF',
            'expected_contract': '8WJ2ngd7FpHVkWiQTNyJ3N9j1oDmjR5e6MFdDAKQNinF',
            'expected_chain': 'SOL'
        }
    ]
    
    for i, test in enumerate(contract_tests):
        contract, chain = extract_contract_from_url(test['url'])
        test_results['contract_extraction'][f'test_{i+1}'] = {
            'input': test['url'],
            'expected_contract': test['expected_contract'],
            'expected_chain': test['expected_chain'],
            'result_contract': contract,
            'result_chain': chain,
            'passed': contract == test['expected_contract'] and chain == test['expected_chain']
        }
    
    # æµ‹è¯•å®Œæ•´çš„ä»£å¸ä¿¡æ¯æå–
    test_results['promotion_info_extraction'] = {}
    promotion_tests = [
        {
            'text': 'New Token $ABC\nåˆçº¦åœ°å€: 0x1234567890abcdef1234567890abcdef12345678\né“¾: BSC\nå¸‚å€¼: 100 BNB',
            'date': datetime.now(),
            'expected_contract': '0x1234567890abcdef1234567890abcdef12345678',
            'expected_chain': 'BSC',
            'expected_symbol': 'ABC'
        },
        {
            'text': 'SOLä»£å¸ï¼Œç¬¦å·: $XYZ\nåˆçº¦: 8WJ2ngd7FpHVkWiQTNyJ3N9j1oDmjR5e6MFdDAKQNinF\nå¸‚å€¼: 50 SOL',
            'date': datetime.now(),
            'expected_contract': '8WJ2ngd7FpHVkWiQTNyJ3N9j1oDmjR5e6MFdDAKQNinF',
            'expected_chain': 'SOL',
            'expected_symbol': 'XYZ'
        }
    ]
    
    for i, test in enumerate(promotion_tests):
        info = extract_promotion_info(test['text'], test['date'])
        test_passed = False
        if info:
            test_passed = (
                info.contract_address == test['expected_contract'] and
                info.chain == test['expected_chain'] and
                info.token_symbol == test['expected_symbol']
            )
        
        test_results['promotion_info_extraction'][f'test_{i+1}'] = {
            'input': test['text'],
            'expected_contract': test['expected_contract'],
            'expected_chain': test['expected_chain'],
            'expected_symbol': test['expected_symbol'],
            'result': info.__dict__ if info else None,
            'passed': test_passed
        }
    
    # ç»Ÿè®¡æµ‹è¯•ç»“æœ
    total_tests = sum(len(category) for category in test_results.values())
    passed_tests = sum(
        sum(1 for test in category.values() if test['passed'])
        for category in test_results.values()
    )
    
    test_results['summary'] = {
        'total_tests': total_tests,
        'passed_tests': passed_tests,
        'success_rate': f"{(passed_tests / total_tests * 100):.2f}%" if total_tests > 0 else "N/A"
    }
    
    return test_results
