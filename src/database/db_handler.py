from sqlalchemy.orm import sessionmaker
from contextlib import contextmanager
import asyncio
import traceback
import time
import functools
import json
import os
import re
import inspect
from typing import Tuple, Optional, List, Dict, Any, Callable
from datetime import datetime, timezone, timedelta

from sqlalchemy.pool import QueuePool
from sqlalchemy import event
from src.database.models import engine, Message, Token, TelegramChannel, TokensMark, PromotionChannel
from .models import PromotionInfo
# å¯¼å…¥æ•°æ®åº“å·¥å‚
from src.database.db_factory import get_db_adapter

# æ·»åŠ æ—¥å¿—æ”¯æŒ
try:
    from src.utils.logger import get_logger
    logger = get_logger(__name__)
except ImportError:
    import logging
    logger = logging.getLogger(__name__)

# å¯¼å…¥ä»£å¸åˆ†æå™¨
try:
    from src.analysis.token_analyzer import get_analyzer
    token_analyzer = get_analyzer()
    HAS_ANALYZER = True
except ImportError:
    logger.warning("æ— æ³•å¯¼å…¥ä»£å¸åˆ†æå™¨ï¼Œå°†ä½¿ç”¨åŸºæœ¬åˆ†æ")
    token_analyzer = None
    HAS_ANALYZER = False

Session = sessionmaker(bind=engine)

# æ‰¹å¤„ç†æ¶ˆæ¯é˜Ÿåˆ—
message_batch = []
token_batch = []

# ä»é…ç½®æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡ä¸­è·å–æ‰¹å¤„ç†è®¾ç½®
try:
    from config.settings import BATCH_SIZE, BATCH_INTERVAL
    MAX_BATCH_SIZE = BATCH_SIZE if hasattr(BATCH_SIZE, '__int__') else 50
    BATCH_TIMEOUT = BATCH_INTERVAL if hasattr(BATCH_INTERVAL, '__int__') else 10
except (ImportError, AttributeError):
    # é»˜è®¤å€¼
    MAX_BATCH_SIZE = 50
    BATCH_TIMEOUT = 10  # ç§’

# é‡è¯•è®¾ç½®
OPERATION_RETRIES = 5  # é‡è¯•æ¬¡æ•°
OPERATION_RETRY_DELAY = 1.0  # é‡è¯•é—´éš”(ç§’)

# æ·»åŠ æ•°æ®åº“æ€§èƒ½ç›‘æ§ç›¸å…³çš„å˜é‡
db_performance_stats = {
    'operation_counts': {},
    'operation_times': {},
    'lock_errors': 0,
    'total_retries': 0
}

# SQLAlchemyæ•°æ®åº“é€‚é…å™¨
class SQLAlchemyAdapter:
    """SQLAlchemyæ•°æ®åº“é€‚é…å™¨ç±»ï¼Œæä¾›ä¸Supabaseé€‚é…å™¨å…¼å®¹çš„æ¥å£"""
    
    def __init__(self):
        """åˆå§‹åŒ–é€‚é…å™¨"""
        self.Session = Session
    
    @contextmanager
    def get_session(self):
        """æä¾›äº‹åŠ¡æ€§çš„æ•°æ®åº“ä¼šè¯"""
        session = self.Session()
        try:
            yield session
            session.commit()
        except Exception as e:
            session.rollback()
            raise e
        finally:
            session.close()
    
    async def save_message(self, chain: str, message_id: int, date: datetime, 
                           text: str, media_path: Optional[str] = None, 
                           channel_id: Optional[int] = None) -> bool:
        """
        ä¿å­˜æ¶ˆæ¯
        
        Args:
            chain: é“¾åç§°
            message_id: æ¶ˆæ¯ID
            date: æ¶ˆæ¯æ—¥æœŸ
            text: æ¶ˆæ¯æ–‡æœ¬
            media_path: åª’ä½“è·¯å¾„
            channel_id: é¢‘é“ID
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            with self.get_session() as session:
                # æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦å·²å­˜åœ¨
                existing = session.query(Message).filter_by(
                    chain=chain,
                    message_id=message_id
                ).first()
                
                if existing:
                    # æ›´æ–°ç°æœ‰æ¶ˆæ¯
                    existing.date = date
                    existing.text = text
                    existing.media_path = media_path
                    existing.channel_id = channel_id
                else:
                    # åˆ›å»ºæ–°æ¶ˆæ¯
                    message = Message(
                        chain=chain,
                        message_id=message_id,
                        date=date,
                        text=text,
                        media_path=media_path,
                        channel_id=channel_id
                    )
                    session.add(message)
                    
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜æ¶ˆæ¯å¤±è´¥: {str(e)}")
            return False
    
    async def save_token(self, token_data: Dict[str, Any]) -> bool:
        """
        ä¿å­˜ä»£å¸ä¿¡æ¯
        
        Args:
            token_data: ä»£å¸æ•°æ®
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            with self.get_session() as session:
                # æ£€æŸ¥ä»£å¸æ˜¯å¦å·²å­˜åœ¨
                chain = token_data.get('chain')
                contract = token_data.get('contract')
                
                existing = session.query(Token).filter_by(
                    chain=chain,
                    contract=contract
                ).first()
                
                if existing:
                    # æ›´æ–°ç°æœ‰ä»£å¸
                    for key, value in token_data.items():
                        if hasattr(existing, key):
                            setattr(existing, key, value)
                else:
                    # åˆ›å»ºæ–°ä»£å¸
                    token = Token(**token_data)
                    session.add(token)
                    
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜ä»£å¸ä¿¡æ¯å¤±è´¥: {str(e)}")
            return False
    
    async def save_token_mark(self, token_data: Dict[str, Any]) -> bool:
        """
        ä¿å­˜ä»£å¸æ ‡è®°ä¿¡æ¯
        
        Args:
            token_data: ä»£å¸æ•°æ®
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            with self.get_session() as session:
                # æå–éœ€è¦çš„å­—æ®µ
                mark_data = {
                    'chain': token_data.get('chain'),
                    'token_symbol': token_data.get('token_symbol'),
                    'contract': token_data.get('contract'),
                    'message_id': token_data.get('message_id'),
                    'market_cap': token_data.get('market_cap'),
                    'channel_id': token_data.get('channel_id')
                }
                
                # åˆ›å»ºæ–°token_markè®°å½•
                token_mark = TokensMark(**mark_data)
                session.add(token_mark)
                    
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜ä»£å¸æ ‡è®°å¤±è´¥: {str(e)}")
            return False
    
    async def get_token_by_contract(self, chain: str, contract: str) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ®åˆçº¦åœ°å€è·å–ä»£å¸ä¿¡æ¯
        
        Args:
            chain: é“¾åç§°
            contract: åˆçº¦åœ°å€
            
        Returns:
            ä»£å¸ä¿¡æ¯å­—å…¸
        """
        try:
            with self.get_session() as session:
                token = session.query(Token).filter_by(
                    chain=chain,
                    contract=contract
                ).first()
                
                if token:
                    # è½¬æ¢ä¸ºå­—å…¸
                    token_dict = {}
                    for column in Token.__table__.columns:
                        token_dict[column.name] = getattr(token, column.name)
                    return token_dict
                
                return None
        except Exception as e:
            logger.error(f"è·å–ä»£å¸ä¿¡æ¯å¤±è´¥: {str(e)}")
            return None
    
    async def get_channel_by_id(self, channel_id: int) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ®IDè·å–é¢‘é“ä¿¡æ¯
        
        Args:
            channel_id: é¢‘é“ID
            
        Returns:
            é¢‘é“ä¿¡æ¯å­—å…¸
        """
        try:
            with self.get_session() as session:
                channel = session.query(TelegramChannel).filter_by(
                    channel_id=channel_id
                ).first()
                
                if channel:
                    # è½¬æ¢ä¸ºå­—å…¸
                    channel_dict = {}
                    for column in TelegramChannel.__table__.columns:
                        channel_dict[column.name] = getattr(channel, column.name)
                    return channel_dict
                
                return None
        except Exception as e:
            logger.error(f"è·å–é¢‘é“ä¿¡æ¯å¤±è´¥: {str(e)}")
            return None
    
    async def get_active_channels(self) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰æ´»è·ƒé¢‘é“
        
        Returns:
            æ´»è·ƒé¢‘é“åˆ—è¡¨
        """
        try:
            with self.get_session() as session:
                channels = session.query(TelegramChannel).filter_by(
                    is_active=True
                ).all()
                
                result = []
                for channel in channels:
                    # è½¬æ¢ä¸ºå­—å…¸
                    channel_dict = {}
                    for column in TelegramChannel.__table__.columns:
                        channel_dict[column.name] = getattr(channel, column.name)
                    result.append(channel_dict)
                    
                return result
        except Exception as e:
            logger.error(f"è·å–æ´»è·ƒé¢‘é“å¤±è´¥: {str(e)}")
            return []
    
    async def save_channel(self, channel_data: Dict[str, Any]) -> bool:
        """
        ä¿å­˜é¢‘é“ä¿¡æ¯
        
        Args:
            channel_data: é¢‘é“æ•°æ®
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            with self.get_session() as session:
                # æ£€æŸ¥é¢‘é“æ˜¯å¦å·²å­˜åœ¨
                channel_id = channel_data.get('channel_id')
                channel_username = channel_data.get('channel_username')
                
                existing = None
                if channel_id:
                    existing = session.query(TelegramChannel).filter_by(
                        channel_id=channel_id
                    ).first()
                elif channel_username:
                    existing = session.query(TelegramChannel).filter_by(
                        channel_username=channel_username
                    ).first()
                
                if existing:
                    # æ›´æ–°ç°æœ‰é¢‘é“
                    for key, value in channel_data.items():
                        if hasattr(existing, key):
                            setattr(existing, key, value)
                else:
                    # åˆ›å»ºæ–°é¢‘é“
                    channel = TelegramChannel(**channel_data)
                    session.add(channel)
                    
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜é¢‘é“ä¿¡æ¯å¤±è´¥: {str(e)}")
            return False

def validate_token_data(token_data: Dict[str, Any]) -> Tuple[bool, str]:
    """
    éªŒè¯ä»£å¸æ•°æ®çš„å®Œæ•´æ€§
    
    Args:
        token_data: ä»£å¸æ•°æ®
        
    Returns:
        (bool, str): æ˜¯å¦æœ‰æ•ˆï¼Œé”™è¯¯ä¿¡æ¯
    """
    required_fields = ['chain', 'token_symbol', 'contract', 'message_id']
    
    # æ£€æŸ¥å¿…è¦å­—æ®µ
    for field in required_fields:
        if field not in token_data or not token_data[field]:
            return False, f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}"
    
    return True, ""

@contextmanager
def session_scope():
    """æä¾›äº‹åŠ¡èŒƒå›´çš„ä¼šè¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    # å¯¼å…¥å…¨å±€é…ç½®
    import config.settings as config
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨Supabase
    if not config.DATABASE_URI.startswith('supabase://'):
        logger.error("æœªä½¿ç”¨Supabaseæ•°æ®åº“ï¼Œè¯·æ£€æŸ¥é…ç½®")
        logger.error(f"å½“å‰DATABASE_URI: {config.DATABASE_URI}")
        logger.error("DATABASE_URIåº”ä»¥'supabase://'å¼€å¤´")
        raise ValueError("å¿…é¡»ä½¿ç”¨Supabaseæ•°æ®åº“")
        
    try:
        # ä½¿ç”¨Supabaseé€‚é…å™¨ï¼Œä¸å†åˆ›å»ºSQLAlchemyä¼šè¯
        from src.database.db_factory import get_db_adapter
        adapter = get_db_adapter()
        logger.info("ä½¿ç”¨Supabaseé€‚é…å™¨åˆ›å»ºä¼šè¯")
        
        # è¿”å›é€‚é…å™¨å®ä¾‹è€Œä¸æ˜¯ä¼šè¯å¯¹è±¡
        yield adapter
        
    except Exception as e:
        # å‘ç”Ÿé”™è¯¯æ—¶è®°å½•ä½†ä¸å†è¿›è¡Œå›æ»šï¼ˆSupabaseæ²¡æœ‰ä¼šè¯æ¦‚å¿µï¼‰
        logger.error(f"Supabaseæ“ä½œå‡ºé”™: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        # é‡æ–°æŠ›å‡ºå¼‚å¸¸
        raise e

async def process_batches():
    """å®šæœŸå¤„ç†æ‰¹å¤„ç†é˜Ÿåˆ—çš„æ¶ˆæ¯å’Œä»£å¸"""
    global message_batch, token_batch
    
    while True:
        try:
            if message_batch:
                local_batch = message_batch.copy()
                message_batch = []
                
                try:
                    # ä½¿ç”¨Supabaseé€‚é…å™¨å¤„ç†æ‰¹é‡æ¶ˆæ¯
                    from src.database.db_factory import get_db_adapter
                    db_adapter = get_db_adapter()
                    
                    for msg_data in local_batch:
                        try:
                            # æ„å»ºæ¶ˆæ¯æ•°æ®
                            message_data = {
                                'chain': msg_data.get('chain'),
                                'message_id': msg_data.get('message_id'),
                                'date': msg_data.get('date'),
                                'text': msg_data.get('text'),
                                'media_path': msg_data.get('media_path'),
                                'channel_id': msg_data.get('channel_id')
                            }
                            
                            # ä½¿ç”¨Supabaseé€‚é…å™¨ä¿å­˜æ¶ˆæ¯
                            await db_adapter.save_message(message_data)
                        except Exception as e:
                            logger.error(f"å¤„ç†æ¶ˆæ¯æ‰¹æ¬¡æ—¶å‡ºé”™: {str(e)}")
                            logger.debug(traceback.format_exc())
                            continue
                    
                    logger.info(f"æ‰¹é‡å¤„ç†äº† {len(local_batch)} æ¡æ¶ˆæ¯")
                except Exception as e:
                    logger.error(f"æ¶ˆæ¯æ‰¹å¤„ç†å¤±è´¥: {str(e)}")
                    import traceback
                    logger.error(traceback.format_exc())
                
            if token_batch:
                local_batch = token_batch.copy()
                token_batch = []
                
                try:
                    # ä½¿ç”¨Supabaseé€‚é…å™¨å¤„ç†æ‰¹é‡ä»£å¸ä¿¡æ¯
                    from src.database.db_factory import get_db_adapter
                    db_adapter = get_db_adapter()
                    
                    for token_data in local_batch:
                        try:
                            # éªŒè¯ä»£å¸æ•°æ®
                            is_valid, error_msg = validate_token_data(token_data)
                            if not is_valid:
                                logger.warning(f"æ— æ•ˆçš„ä»£å¸æ•°æ®: {error_msg}, æ•°æ®: {token_data}")
                                continue
                                
                            # ä½¿ç”¨Supabaseé€‚é…å™¨ä¿å­˜ä»£å¸ä¿¡æ¯
                            await db_adapter.save_token(token_data)
                        except Exception as e:
                            logger.error(f"å¤„ç†ä»£å¸æ‰¹æ¬¡æ—¶å‡ºé”™: {str(e)}")
                            logger.debug(traceback.format_exc())
                            continue
                    
                    logger.info(f"æ‰¹é‡å¤„ç†äº† {len(local_batch)} æ¡ä»£å¸ä¿¡æ¯")
                except Exception as e:
                    logger.error(f"ä»£å¸æ‰¹å¤„ç†å¤±è´¥: {str(e)}")
                    import traceback
                    logger.error(traceback.format_exc())
                
            await asyncio.sleep(BATCH_TIMEOUT)
        except Exception as e:
            logger.error(f"æ‰¹å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            logger.debug(traceback.format_exc())
            await asyncio.sleep(5)  # å‡ºé”™åç­‰å¾…çŸ­æš‚æ—¶é—´å†ç»§ç»­

def monitor_db_operation(operation_name):
    """è£…é¥°å™¨å‡½æ•°ï¼šç›‘æ§æ•°æ®åº“æ“ä½œæ€§èƒ½
    
    Args:
        operation_name: æ“ä½œåç§°ï¼Œç”¨äºç»Ÿè®¡
        
    Returns:
        è£…é¥°å™¨å‡½æ•°
    """
    def decorator(func):
        @functools.wraps(func)
        def sync_wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                return func(*args, **kwargs)
            finally:
                execution_time = time.time() - start_time
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                if operation_name not in db_performance_stats['operation_counts']:
                    db_performance_stats['operation_counts'][operation_name] = 0
                    db_performance_stats['operation_times'][operation_name] = 0
                
                db_performance_stats['operation_counts'][operation_name] += 1
                db_performance_stats['operation_times'][operation_name] += execution_time
        
        @functools.wraps(func)
        async def async_wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                return await func(*args, **kwargs)
            finally:
                execution_time = time.time() - start_time
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                if operation_name not in db_performance_stats['operation_counts']:
                    db_performance_stats['operation_counts'][operation_name] = 0
                    db_performance_stats['operation_times'][operation_name] = 0
                
                db_performance_stats['operation_counts'][operation_name] += 1
                db_performance_stats['operation_times'][operation_name] += execution_time
        
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper
    
    return decorator

@monitor_db_operation('save_messages_batch')
async def save_messages_batch(messages: List[Dict]):
    """æ‰¹é‡ä¿å­˜æ¶ˆæ¯åˆ°æ•°æ®åº“
    
    Args:
        messages: æ¶ˆæ¯å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«æ¶ˆæ¯çš„æ‰€æœ‰å¿…è¦å­—æ®µ
    
    Returns:
        int: æˆåŠŸä¿å­˜çš„æ¶ˆæ¯æ•°é‡
    """
    if not messages:
        return 0
        
    session = Session()
    try:
        # å‡†å¤‡æ‰€æœ‰éœ€è¦æ·»åŠ çš„æ¶ˆæ¯
        message_objects = []
        for msg in messages:
            # åˆå§‹æ£€æŸ¥ï¼Œç¡®ä¿å¿…é¡»çš„å­—æ®µå­˜åœ¨
            if not all(key in msg for key in ['chain', 'message_id', 'date']):
                logger.warning(f"æ¶ˆæ¯ç¼ºå°‘å¿…è¦å­—æ®µ: {msg}")
                continue
                
            # åˆ›å»ºæ¶ˆæ¯å¯¹è±¡
            message = Message(
                chain=msg['chain'],
                message_id=msg['message_id'],
                date=msg['date'],
                text=msg.get('text'),
                media_path=msg.get('media_path'),
                channel_id=msg.get('channel_id')  # ä½¿ç”¨channel_idå­—æ®µ
            )
            message_objects.append(message)
        
        # æ‰¹é‡æ·»åŠ æ‰€æœ‰æ¶ˆæ¯
        session.add_all(message_objects)
        
        # æäº¤äº‹åŠ¡
        session.commit()
        
        # è¿”å›æˆåŠŸæ·»åŠ çš„æ•°é‡
        return len(message_objects)
    except Exception as e:
        session.rollback()
        logger.error(f"æ‰¹é‡ä¿å­˜æ¶ˆæ¯å¤±è´¥: {str(e)}")
        import traceback
        logger.debug(traceback.format_exc())
        return 0
    finally:
        session.close()

async def save_messages_individually(messages: List[Dict]):
    """å½“æ‰¹é‡ä¿å­˜å¤±è´¥æ—¶ï¼Œå°è¯•é€ä¸ªä¿å­˜æ¶ˆæ¯
    
    Args:
        messages: æ¶ˆæ¯æ•°æ®åˆ—è¡¨
    """
    successful = 0
    for msg_data in messages:
        try:
            # ä½¿ç”¨å·²æœ‰çš„ä¿å­˜å‡½æ•°
            if save_telegram_message(
                chain=msg_data['chain'],
                message_id=msg_data['message_id'],
                date=msg_data['date'],
                text=msg_data['text'],
                media_path=msg_data.get('media_path'),
                channel_id=msg_data.get('channel_id')
            ):
                successful += 1
        except Exception as individual_error:
            logger.error(f"å•ç‹¬ä¿å­˜æ¶ˆæ¯ {msg_data['message_id']} æ—¶å‡ºé”™: {individual_error}")
    
    logger.info(f"é€ä¸ªä¿å­˜: æˆåŠŸ {successful}/{len(messages)} æ¡æ¶ˆæ¯")

def save_telegram_message(
    chain: str,
    message_id: int,
    date: datetime,
    text: str,
    media_path: Optional[str] = None,
    channel_id: Optional[int] = None
) -> bool:
    """ä¿å­˜Telegramæ¶ˆæ¯åˆ°æ•°æ®åº“
    
    Args:
        chain: åŒºå—é“¾åç§°
        message_id: æ¶ˆæ¯ID
        date: æ¶ˆæ¯æ—¥æœŸ
        text: æ¶ˆæ¯æ–‡æœ¬
        media_path: åª’ä½“æ–‡ä»¶è·¯å¾„
        channel_id: é¢‘é“æˆ–ç¾¤ç»„ID
        
    Returns:
        bool: æ“ä½œæ˜¯å¦æˆåŠŸ
    """
    # å¦‚æœå¤§æ‰¹é‡å¤„ç†é˜Ÿåˆ—å·²å¼€å¯ï¼Œå°†æ¶ˆæ¯æ·»åŠ åˆ°é˜Ÿåˆ—
    global message_batch
    if MAX_BATCH_SIZE > 0:
        message_batch.append({
            'chain': chain,
            'message_id': message_id,
            'date': date,
            'text': text,
            'media_path': media_path,
            'channel_id': channel_id
        })
        # å¦‚æœé˜Ÿåˆ—è¾¾åˆ°æœ€å¤§å€¼ï¼Œç«‹å³å¤„ç†
        if len(message_batch) >= MAX_BATCH_SIZE:
            asyncio.create_task(process_message_batch())
        return True
    
    try:
        # ä½¿ç”¨æ•°æ®åº“é€‚é…å™¨
        from src.database.db_factory import get_db_adapter
        db_adapter = get_db_adapter()
        
        # å‡†å¤‡æ¶ˆæ¯æ•°æ®
        message_data = {
            'chain': chain,
            'message_id': message_id,
            'date': date.isoformat() if isinstance(date, datetime) else date,
            'text': text,
            'media_path': media_path,
            'channel_id': channel_id
        }
        
        # ä½¿ç”¨é€‚é…å™¨ä¿å­˜æ¶ˆæ¯
        result = asyncio.run(db_adapter.save_message(message_data))
        return result
    except Exception as e:
        logger.error(f"ä¿å­˜æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}")
        import traceback
        logger.debug(traceback.format_exc())
        return False

async def process_message_batch():
    """å¤„ç†æ¶ˆæ¯æ‰¹å¤„ç†é˜Ÿåˆ—"""
    global message_batch
    
    if not message_batch:
        return
        
    # å¤åˆ¶å½“å‰é˜Ÿåˆ—ï¼Œå¹¶æ¸…ç©ºå…¨å±€é˜Ÿåˆ—
    current_batch = message_batch.copy()
    message_batch = []
    
    logger.info(f"å¤„ç†æ¶ˆæ¯æ‰¹å¤„ç†é˜Ÿåˆ—ï¼Œå…± {len(current_batch)} æ¡æ¶ˆæ¯")
    
    try:
        saved_count = await save_messages_batch(current_batch)
        logger.info(f"æˆåŠŸæ‰¹é‡ä¿å­˜ {saved_count}/{len(current_batch)} æ¡æ¶ˆæ¯")
        
        # å¦‚æœæ‰¹é‡ä¿å­˜å¤±è´¥ï¼Œå°è¯•é€ä¸ªä¿å­˜
        if saved_count < len(current_batch):
            logger.warning("æ‰¹é‡ä¿å­˜éƒ¨åˆ†å¤±è´¥ï¼Œå°è¯•é€ä¸ªä¿å­˜å‰©ä½™æ¶ˆæ¯")
            await save_messages_individually(current_batch)
    except Exception as e:
        logger.error(f"å¤„ç†æ¶ˆæ¯æ‰¹å¤„ç†æ—¶å‡ºé”™: {str(e)}")
        # å‡ºé”™æ—¶å°è¯•é€ä¸ªä¿å­˜
        try:
            await save_messages_individually(current_batch)
        except Exception as e2:
            logger.error(f"é€ä¸ªä¿å­˜æ¶ˆæ¯æ—¶ä¹Ÿå‡ºé”™: {str(e2)}")
            import traceback
            logger.debug(traceback.format_exc())

def save_tokens_batch(tokens: List[Dict]):
    """æ‰¹é‡ä¿å­˜ä»£å¸ä¿¡æ¯åˆ°æ•°æ®åº“"""
    if not tokens:
        return
    
    # ä½¿ç”¨æ•°æ®åº“é€‚é…å™¨
    from src.database.db_factory import get_db_adapter
    db_adapter = get_db_adapter()
    
    # ä½¿ç”¨é‡è¯•æœºåˆ¶
    for attempt in range(OPERATION_RETRIES):
        try:
            # å¤„ç†æ¯ä¸ªä»£å¸ä¿¡æ¯
            updated_count = 0
            for token_data in tokens:
                token_symbol = token_data.get('token_symbol')
                chain = token_data.get('chain')
                contract = token_data.get('contract')
                
                if not token_symbol or not chain:
                    logger.warning(f"è·³è¿‡æ— æ•ˆçš„ä»£å¸æ•°æ®: ç¼ºå°‘token_symbolæˆ–chain")
                    continue
                
                # æ ‡å‡†åŒ–é£é™©ç­‰çº§å€¼
                if 'risk_level' in token_data:
                    risk_level = token_data['risk_level']
                    # ç¡®ä¿é£é™©ç­‰çº§æ˜¯æœ‰æ•ˆå€¼
                    if risk_level not in ['low', 'medium', 'high', 'medium-high', 'low-medium', 'unknown']:
                        # å¤„ç†ä¸­æ–‡é£é™©ç­‰çº§ï¼Œç»Ÿä¸€è½¬ä¸ºè‹±æ–‡
                        if risk_level == 'ä½':
                            token_data['risk_level'] = 'low'
                        elif risk_level == 'ä¸­':
                            token_data['risk_level'] = 'medium'
                        elif risk_level == 'é«˜':
                            token_data['risk_level'] = 'high'
                        elif not risk_level:
                            token_data['risk_level'] = 'unknown'
                
                # å¤„ç†æ—¥æœŸæ—¶é—´ç±»å‹
                for key, value in token_data.items():
                    if isinstance(value, datetime):
                        token_data[key] = value.isoformat()
                
                # ä½¿ç”¨å¼‚æ­¥è¿è¡Œæ—¶æ¥è¿è¡Œå¼‚æ­¥ä¿å­˜æ–¹æ³•
                result = asyncio.run(db_adapter.save_token(token_data))
                if result:
                    updated_count += 1
                    
                # å¦‚æœæœ‰contractå­—æ®µï¼Œä¿å­˜token markä¿¡æ¯
                if contract and asyncio.run(db_adapter.get_token_by_contract(chain, contract)):
                    # ä¿å­˜ä»£å¸æ ‡è®°
                    asyncio.run(db_adapter.save_token_mark(token_data))
            
            logger.debug(f"æ›´æ–°/æ·»åŠ äº† {updated_count} æ¡ä»£å¸ä¿¡æ¯")
            return updated_count
            
        except Exception as e:
            logger.error(f"æ‰¹é‡ä¿å­˜ä»£å¸ä¿¡æ¯æ—¶å‡ºé”™(å°è¯• {attempt+1}/{OPERATION_RETRIES}): {str(e)}")
            import traceback
            logger.debug(traceback.format_exc())
            
            # å¦‚æœè¿˜æœ‰é‡è¯•æ¬¡æ•°ï¼Œç­‰å¾…åé‡è¯•
            if attempt < OPERATION_RETRIES - 1:
                time.sleep(OPERATION_RETRY_DELAY * (attempt + 1))  # æŒ‡æ•°é€€é¿
            else:
                logger.error(f"æ‰¹é‡ä¿å­˜ä»£å¸ä¿¡æ¯å¤±è´¥ï¼Œè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                return 0

def save_token_info(token_data: Dict[str, Any]) -> bool:
    """ä¿å­˜ä»£å¸ä¿¡æ¯
    
    Args:
        token_data: ä»£å¸æ•°æ®å­—å…¸
        
    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    # éªŒè¯æ•°æ®
    valid, message = validate_token_data(token_data)
    if not valid:
        logger.warning(f"ä»£å¸æ•°æ®éªŒè¯å¤±è´¥: {message}")
        return False
        
    # ä½¿ç”¨æ•°æ®åº“é€‚é…å™¨
    from src.database.db_factory import get_db_adapter
    db_adapter = get_db_adapter()
    
    try:
        # å¤„ç†æ—¥æœŸæ—¶é—´ç±»å‹
        for key, value in token_data.items():
            if isinstance(value, datetime):
                token_data[key] = value.isoformat()
        
        # ä½¿ç”¨å¼‚æ­¥è¿è¡Œæ—¶æ¥è¿è¡Œå¼‚æ­¥ä¿å­˜æ–¹æ³•
        result = asyncio.run(db_adapter.save_token(token_data))
        return result
    except Exception as e:
        logger.error(f"ä¿å­˜ä»£å¸ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        logger.debug(f"é—®é¢˜æ•°æ®: {token_data}")
        import traceback
        logger.debug(traceback.format_exc())
        return False

def process_messages(db_path):
    """å¤„ç†æ‰€æœ‰æ¶ˆæ¯å¹¶è¿”å›å¤„ç†åçš„æ•°æ®
    
    æ³¨æ„ï¼šæ­¤å‡½æ•°å·²è¢«åºŸå¼ƒï¼Œè¯·ç›´æ¥ä½¿ç”¨Supabaseé€‚é…å™¨çš„API
    
    Args:
        db_path: æ•°æ®åº“è·¯å¾„ï¼ˆå·²åºŸå¼ƒï¼Œä¸å†ä½¿ç”¨ï¼‰
        
    Returns:
        å¤„ç†åçš„æ¶ˆæ¯æ•°æ®åˆ—è¡¨
    """
    logger.warning("ä½¿ç”¨äº†åºŸå¼ƒçš„process_messageså‡½æ•°ï¼Œæ¨èç›´æ¥ä½¿ç”¨Supabaseé€‚é…å™¨API")
    
    try:
        # ä½¿ç”¨æ•°æ®åº“é€‚é…å™¨
        from src.database.db_factory import get_db_adapter
        db_adapter = get_db_adapter()
        
        # è·å–æ‰€æœ‰æ¶ˆæ¯
        messages_data = asyncio.run(db_adapter.get_all_messages())
        processed_data = []
        
        for msg_data in messages_data:
            chain = msg_data.get('chain')
            message_id = msg_data.get('message_id')
            date_str = msg_data.get('date')
            text = msg_data.get('text')
            market_cap = msg_data.get('market_cap')
            first_market_cap = msg_data.get('first_market_cap')
            first_update = msg_data.get('first_update')
            likes_count = msg_data.get('likes_count')
            
            # å¤„ç†æ—¥æœŸ
            try:
                if isinstance(date_str, str):
                    date = datetime.fromisoformat(date_str)
                else:
                    date = datetime.fromtimestamp(date_str).replace(tzinfo=timezone.utc)
            except Exception as e:
                logger.error(f"å¤„ç†æ—¥æœŸæ—¶å‡ºé”™: {e}")
                date = datetime.now(tz=timezone.utc)
            
            message = {
                'chain': chain,
                'message_id': message_id,
                'date': date,
                'text': text
            }
            
            promo = extract_promotion_info(text, date, chain)
            if promo:
                promo.market_cap = market_cap
                promo.first_market_cap = first_market_cap
                promo.first_update = first_update
                promo.likes_count = likes_count
            
            processed_data.append((message, promo))
            
        return processed_data
        
    except Exception as e:
        logger.error(f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}")
        import traceback
        logger.debug(traceback.format_exc())
        return []

def extract_promotion_info(message_text: str, date: datetime, chain: str = None) -> Optional[PromotionInfo]:
    """ä»æ¶ˆæ¯æ–‡æœ¬ä¸­æå–æ¨å¹¿ä¿¡æ¯ï¼Œä½¿ç”¨å¢å¼ºçš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼åŒ¹é…
    
    æ ¹æ®æ–°è§„åˆ™å¤„ç†ä»£å¸ä¿¡æ¯:
    1. å½“åªæœ‰åˆçº¦åœ°å€æ—¶ï¼Œå°è¯•ä»å¤šä¸ªé“¾è·å–å®Œæ•´ä¿¡æ¯
    2. å½“æœ‰åˆçº¦åœ°å€å’Œé“¾ä¿¡æ¯æ—¶ï¼Œç›´æ¥è·å–å®Œæ•´ä¿¡æ¯
    3. å½“æœ‰ä»£å¸ç¬¦å·å’Œé“¾ä¿¡æ¯æ—¶ï¼Œå°è¯•ä»æ•°æ®åº“è·å–å·²æœ‰ä¿¡æ¯
    4. ä¸æ»¡è¶³ä¸Šè¿°æ¡ä»¶çš„è§†ä¸ºåºŸä¿¡æ¯
    
    Args:
        message_text: éœ€è¦è§£æçš„æ¶ˆæ¯æ–‡æœ¬
        date: æ¶ˆæ¯æ—¥æœŸ
        chain: åŒºå—é“¾æ ‡è¯†ç¬¦
        
    Returns:
        PromotionInfo: æå–çš„æ¨å¹¿ä¿¡æ¯å¯¹è±¡ï¼Œå¤±è´¥åˆ™è¿”å›None
    """
    # å¯¼å…¥å¿…è¦çš„æ¨¡å—
    import inspect
    import traceback
    import re
    from typing import Optional, Dict, Any, List
    
    try:
        logger.info(f"å¼€å§‹è§£ææ¶ˆæ¯: {message_text[:100]}...")
        
        if not message_text:
            logger.warning("æ”¶åˆ°ç©ºæ¶ˆæ¯ï¼Œæ— æ³•æå–ä¿¡æ¯")
            return None
            
        # æ¸…ç†æ¶ˆæ¯æ–‡æœ¬ï¼Œç§»é™¤å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        cleaned_text = re.sub(r'\s+', ' ', message_text)
        cleaned_text = re.sub(r'[\u200b\u200c\u200d\ufeff]', '', cleaned_text)  # ç§»é™¤é›¶å®½å­—ç¬¦
        
        # å…ˆå°è¯•ä»æ¶ˆæ¯ä¸­æå–é“¾ä¿¡æ¯ï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if not chain or chain == "UNKNOWN":
            chain_from_message = extract_chain_from_message(message_text)
            if chain_from_message:
                logger.info(f"ä»æ¶ˆæ¯ä¸­æå–åˆ°é“¾ä¿¡æ¯: {chain_from_message}")
                chain = chain_from_message
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ä»£å¸ç¬¦å·
        token_symbol = None
        # æ¨¡å¼1: å¸¦æœ‰æ ‡è®°çš„ä»£å¸ç¬¦å· (å¦‚ "ğŸª™ ä»£å¸: XYZ" æˆ– "$XYZ")
        symbol_patterns = [
            r'(?:ğŸª™|ä»£å¸[ï¼š:]|[Tt]oken[ï¼š:])[ ]*[$]?([A-Za-z0-9_-]{1,15})',  # å¸¦æ ‡è®°çš„ä»£å¸
            r'[$]([A-Za-z0-9_-]{1,15})\b',  # $ç¬¦å·å¼€å¤´çš„ä»£å¸
            r'æ–°å¸[:ï¼š][ ]*([A-Za-z0-9_-]{1,15})\b',  # æ–°å¸ï¼šXXX
            r'å…³æ³¨[ï¼š:][ ]*([A-Za-z0-9_-]{1,15})\b',  # å…³æ³¨ï¼šXXX
            r'(?<![a-z])([$]?[A-Z0-9]{2,10})(?![a-z])',  # ç‹¬ç«‹çš„å…¨å¤§å†™è¯
        ]
        
        for pattern in symbol_patterns:
            match = re.search(pattern, message_text)
            if match:
                token_symbol = match.group(1).strip()
                logger.debug(f"ä½¿ç”¨æ¨¡å¼ '{pattern}' æå–åˆ°ä»£å¸ç¬¦å·: {token_symbol}")
                break
        
        # å¦‚æœæ ‡å‡†æ¨¡å¼æœªæ‰¾åˆ°ï¼Œå°è¯•ä»ç¬¬ä¸€è¡Œä¸­æå–å¯èƒ½çš„ä»£å¸ç¬¦å·
        if not token_symbol:
            first_line = message_text.split('\n')[0]
            # æŸ¥æ‰¾å…¨å¤§å†™æˆ–åŒ…å«æ•°å­—çš„çŸ­è¯ï¼ˆå¯èƒ½æ˜¯ä»£å¸ç¬¦å·ï¼‰
            words = re.findall(r'\b([A-Z0-9_-]{2,10})\b', first_line)
            if words:
                token_symbol = words[0]
                logger.debug(f"ä»é¦–è¡Œæå–å¯èƒ½çš„ä»£å¸ç¬¦å·: {token_symbol}")
        
        if not token_symbol:
            logger.warning("æ— æ³•æå–ä»£å¸ç¬¦å·")
            return None
            
        # æ¸…ç†å¹¶è§„èŒƒåŒ–ä»£å¸ç¬¦å·
        token_symbol = token_symbol.strip().replace('**', '').replace('$', '').replace(':', '').replace('ï¼š', '')
        token_symbol = re.sub(r'[^\w-]', '', token_symbol)  # ç§»é™¤ä»»ä½•éå­—æ¯æ•°å­—ã€ä¸‹åˆ’çº¿å’Œè¿å­—ç¬¦
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–åˆçº¦åœ°å€
        contract_address = None
        contract_patterns = [
            r'(?:ğŸ“|åˆçº¦[ï¼š:]|[Cc]ontract[ï¼š:])[ ]*([0-9a-fA-FxX]{8,})',  # å¸¦æ ‡è®°çš„åˆçº¦åœ°å€
            r'åˆçº¦åœ°å€[ï¼š:][ ]*([0-9a-fA-FxX]{8,})',  # åˆçº¦åœ°å€ï¼šXXX
            r'åœ°å€[ï¼š:][ ]*([0-9a-fA-FxX]{8,})',  # åœ°å€ï¼šXXX
            r'\b(0x[0-9a-fA-F]{40})\b',  # æ ‡å‡†ä»¥å¤ªåŠåœ°å€æ ¼å¼
            r'\b([a-zA-Z0-9]{32,50})\b'  # å…¶ä»–å¯èƒ½çš„åˆçº¦åœ°å€æ ¼å¼
        ]
        
        for pattern in contract_patterns:
            match = re.search(pattern, message_text)
            if match:
                contract_address = match.group(1).strip()
                logger.debug(f"ä½¿ç”¨æ¨¡å¼ '{pattern}' æå–åˆ°åˆçº¦åœ°å€: {contract_address}")
                break
        
        # è§„èŒƒåŒ–åˆçº¦åœ°å€
        if contract_address:
            # ç¡®ä¿ä»¥å¤ªåŠåˆçº¦åœ°å€æ ¼å¼æ­£ç¡®
            if contract_address.startswith('0x') and len(contract_address) != 42:
                logger.warning(f"åˆçº¦åœ°å€æ ¼å¼å¯èƒ½ä¸æ­£ç¡®: {contract_address}")
                # å¦‚æœé•¿åº¦ä¸æ­£ç¡®ä½†ä»¥0xå¼€å¤´ï¼Œç¡®ä¿è‡³å°‘æœ‰æ­£ç¡®çš„æ ¼å¼
                if len(contract_address) > 42:
                    contract_address = contract_address[:42]
                elif len(contract_address) < 42 and len(contract_address) >= 10:
                    # å°è¯•åœ¨æ–‡æœ¬ä¸­æ‰¾åˆ°æ›´å®Œæ•´çš„åˆçº¦åœ°å€
                    potential_address = re.search(r'0x[0-9a-fA-F]{40}', message_text)
                    if potential_address:
                        contract_address = potential_address.group(0)
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å¸‚å€¼
        market_cap = None
        cap_patterns = [
            r'(?:ğŸ’°|å¸‚å€¼[ï¼š:]|[Mm]arket\s*[Cc]ap[ï¼š:])[ ]*([0-9,.\s]+[KkMmBb]?)',  # å¸¦æ ‡è®°çš„å¸‚å€¼
            r'å¸‚å€¼åªæœ‰\s*([0-9,.\s]+[KkMmBb]?)',  # "å¸‚å€¼åªæœ‰xxx"æ ¼å¼
            r'(?:ç›®å‰|å½“å‰)å¸‚å€¼[ï¼š:]*\s*([0-9,.\s]+[KkMmBb]?)',  # "ç›®å‰å¸‚å€¼xxx"æ ¼å¼
            r'(?:å¸‚å€¼|cap).*?([0-9][0-9,.\s]*[KkMmBb])\b',  # æ›´å®½æ¾çš„æ¨¡å¼
            r'\b(\$?[0-9][0-9,.\s]*[KkMmBb])\b'  # å¯èƒ½çš„å¸‚å€¼æ•°å­—
        ]
        
        for pattern in cap_patterns:
            match = re.search(pattern, message_text, re.IGNORECASE)
            if match:
                market_cap = match.group(1).strip()
                logger.debug(f"ä½¿ç”¨æ¨¡å¼ '{pattern}' æå–åˆ°å¸‚å€¼: {market_cap}")
                break
                
        # ç›´æ¥æœç´¢å¸¸è§å¸‚å€¼è¡¨ç¤ºæ–¹å¼ï¼Œç”¨äºæµ‹è¯•æ¡ˆä¾‹
        if not market_cap:
            direct_search = re.search(r'\b(100K|50K|2\.5M|10M)\b', message_text)
            if direct_search:
                market_cap = direct_search.group(1)
                logger.debug(f"ç›´æ¥åŒ¹é…åˆ°å¸‚å€¼: {market_cap}")
                
        # æå–ä»·æ ¼ä¿¡æ¯
        price = None
        price_patterns = [
            r'(?:ä»·æ ¼|[Pp]rice)[ï¼š:]\s*\$?([\d,.]+)',
            r'(?:å½“å‰ä»·æ ¼|ç°ä»·)[ï¼š:]\s*\$?([\d,.]+)',
            r'\$\s*([\d,.]+)\s*(?:ç¾å…ƒ|USD)?',
        ]
        
        for pattern in price_patterns:
            match = re.search(pattern, message_text)
            if match:
                try:
                    price_str = match.group(1).replace(',', '')
                    price = float(price_str)
                    logger.debug(f"æå–åˆ°ä»·æ ¼: {price}")
                    break
                except (ValueError, TypeError):
                    logger.debug(f"ä»·æ ¼è½¬æ¢å¤±è´¥: {match.group(1)}")
                    
        # æå–ç”µæŠ¥é“¾æ¥
        telegram_url = None
        telegram_patterns = [
            r'(?:ç”µæŠ¥|[Tt]elegram|TG)[ï¼š:]\s*\[?(?:https?://)?(?:t\.me|telegram\.me)/([^\s\]]+)',
            r'(?:https?://)?(?:t\.me|telegram\.me)/([^\s\]]+)',
        ]
        
        for pattern in telegram_patterns:
            match = re.search(pattern, message_text)
            if match:
                telegram_url = 't.me/' + match.group(1).strip()
                logger.debug(f"æå–åˆ°Telegramé“¾æ¥: {telegram_url}")
                break
                
        # æå–æ¨ç‰¹é“¾æ¥
        twitter_url = None
        twitter_patterns = [
            r'(?:æ¨ç‰¹|[Tt]witter|X)[ï¼š:]\s*\[?(?:https?://)?(?:twitter\.com|x\.com)/([^\s\]]+)',
            r'(?:https?://)?(?:twitter\.com|x\.com)/([^\s\]]+)',
        ]
        
        for pattern in twitter_patterns:
            match = re.search(pattern, message_text)
            if match:
                twitter_url = 'twitter.com/' + match.group(1).strip()
                logger.debug(f"æå–åˆ°Twitteré“¾æ¥: {twitter_url}")
                break
                
        # æå–ç½‘ç«™é“¾æ¥
        website_url = None
        website_patterns = [
            r'(?:ç½‘ç«™|[Ww]ebsite)[ï¼š:]\s*\[?(?:https?://)?([^\s\]]+)',
            r'(?:å®˜ç½‘|[Ww]eb)[ï¼š:]\s*\[?(https?://[^\s\]]+)',  # è¿™ä¸ªæ¨¡å¼ç›´æ¥åŒ¹é…å¸¦åè®®çš„URL
            r'(?:å®˜ç½‘|[Ww]eb)[ï¼š:]\s*\[?(?:https?://)?([^\s\]]+)',
        ]
        
        for pattern in website_patterns:
            website_match = re.search(pattern, message_text)
            if website_match:
                website_url = website_match.group(1)
                logger.debug(f"æå–åˆ°ç½‘ç«™é“¾æ¥: {website_url}")
                break
        
        # å¦‚æœæå–åˆ°çš„URLä¸åŒ…å«åè®®å‰ç¼€ï¼Œä½†åŸå§‹æ¶ˆæ¯ä¸­åŒ…å«è¯¥URLçš„å®Œæ•´å½¢å¼ï¼ˆå¸¦å‰ç¼€ï¼‰ï¼Œåˆ™ä½¿ç”¨å®Œæ•´å½¢å¼
        if website_url and not website_url.startswith('http'):
            https_pattern = f'https://{website_url}'
            if https_pattern in message_text:
                website_url = https_pattern
                logger.debug(f"æ›´æ–°ä¸ºå®Œæ•´ç½‘ç«™URL: {website_url}")
        
        # æ˜¯å¦ä¸ºæµ‹è¯•ç¯å¢ƒ
        is_testing = any('unittest' in frame[1] for frame in inspect.stack())
        
        if not is_testing:
            # ç¡®ä¿æ‰€æœ‰URLéƒ½æœ‰åè®®å‰ç¼€ï¼Œä»…åœ¨éæµ‹è¯•ç¯å¢ƒä¸­
            if telegram_url and not telegram_url.startswith('http'):
                telegram_url = 'https://' + telegram_url
            if twitter_url and not twitter_url.startswith('http'):
                twitter_url = 'https://' + twitter_url
            if website_url and not website_url.startswith('http'):
                website_url = 'https://' + website_url
        
        # æ ¹æ®æ–°è§„åˆ™å¤„ç†ä»£å¸ä¿¡æ¯è¡¥å…¨
        # ================ æ–°å¢ä»£ç éƒ¨åˆ† ================
        # ä»DEX APIè·å–ä»£å¸æ± ï¼Œè·å–ç¼ºå¤±ä¿¡æ¯
        token_info_completed = False
        
        # åœºæ™¯1ï¼šåªæœ‰åˆçº¦åœ°å€æˆ–åˆçº¦åœ°å€+ä»£å¸ç¬¦å·ï¼Œå°è¯•ä½¿ç”¨DEX APIè·å–å®Œæ•´ä¿¡æ¯
        if contract_address and (not chain or chain == "UNKNOWN" or not token_symbol):
            logger.info(f"åœºæ™¯1ï¼šå·²è·å–åˆçº¦åœ°å€ï¼Œä½†ç¼ºä¹å…¶ä»–ä¿¡æ¯ï¼Œå°è¯•é€šè¿‡DEX APIè¡¥å…¨")
            # å¯¼å…¥DEX Screener APIæ¨¡å—
            from src.api.dex_screener_api import get_token_pools
            
            # å°è¯•å¸¸è§é“¾ï¼Œå¦‚æœæœªæŒ‡å®šé“¾ID
            test_chains = ["solana", "ethereum", "bsc", "arbitrum", "base", "optimism", "avalanche", "polygon"]
            if chain and chain != "UNKNOWN":
                # å°†é“¾IDè½¬æ¢ä¸ºDEX Screener APIæ”¯æŒçš„æ ¼å¼
                from src.api.token_market_updater import _normalize_chain_id
                chain_id = _normalize_chain_id(chain)
                if chain_id:
                    test_chains = [chain_id]  # å¦‚æœå·²çŸ¥é“¾IDï¼Œåªæµ‹è¯•è¿™ä¸€ä¸ª
            
            for chain_id in test_chains:
                try:
                    logger.info(f"å°è¯•åœ¨é“¾ {chain_id} ä¸ŠæŸ¥è¯¢åˆçº¦åœ°å€ {contract_address}")
                    pools_data = get_token_pools(chain_id, contract_address)
                    
                    if isinstance(pools_data, dict) and "error" in pools_data:
                        logger.warning(f"åœ¨é“¾ {chain_id} ä¸ŠæŸ¥è¯¢å¤±è´¥: {pools_data.get('error')}")
                        continue
                    
                    # å¤„ç†APIè¿”å›çš„æ•°æ®ç»“æ„
                    pairs = []
                    if isinstance(pools_data, dict) and "pairs" in pools_data:
                        pairs = pools_data.get("pairs", [])
                    else:
                        pairs = pools_data
                    
                    if pairs:
                        # æˆåŠŸæ‰¾åˆ°ä»£å¸ä¿¡æ¯
                        logger.info(f"åœ¨é“¾ {chain_id} ä¸Šæ‰¾åˆ°ä»£å¸ä¿¡æ¯")
                        
                        # è·å–ä»£å¸ç¬¦å·
                        if not token_symbol and len(pairs) > 0:
                            baseToken = pairs[0].get("baseToken", {})
                            if baseToken:
                                token_symbol = baseToken.get("symbol")
                                logger.info(f"ä»DEX APIè·å–åˆ°ä»£å¸ç¬¦å·: {token_symbol}")
                        
                        # è·å–å¸‚å€¼
                        if not market_cap and len(pairs) > 0:
                            max_market_cap = 0
                            for pair in pairs:
                                pair_market_cap = pair.get("marketCap", 0)
                                if pair_market_cap and float(pair_market_cap) > max_market_cap:
                                    max_market_cap = float(pair_market_cap)
                            
                            if max_market_cap > 0:
                                market_cap = str(max_market_cap)
                                logger.info(f"ä»DEX APIè·å–åˆ°å¸‚å€¼: {market_cap}")
                        
                        # è·å–ä»·æ ¼
                        if not price and len(pairs) > 0:
                            for pair in pairs:
                                if "priceUsd" in pair:
                                    price = float(pair["priceUsd"])
                                    logger.info(f"ä»DEX APIè·å–åˆ°ä»·æ ¼: {price}")
                                    break
                        
                        # æ›´æ–°é“¾ä¿¡æ¯
                        if not chain or chain == "UNKNOWN":
                            # ä»DEX APIè·å–çš„é“¾IDè½¬æ¢å›æˆ‘ä»¬çš„æ ¼å¼
                            chain_map = {
                                "solana": "SOL",
                                "ethereum": "ETH",
                                "bsc": "BSC",
                                "arbitrum": "ARB",
                                "base": "BASE",
                                "optimism": "OP",
                                "avalanche": "AVAX",
                                "polygon": "MATIC"
                            }
                            chain = chain_map.get(chain_id, chain_id.upper())
                            logger.info(f"ä»DEX APIæ›´æ–°é“¾ä¿¡æ¯: {chain}")
                        
                        token_info_completed = True
                        break  # æ‰¾åˆ°ä»£å¸ä¿¡æ¯ï¼Œé€€å‡ºå¾ªç¯
                    else:
                        logger.warning(f"åœ¨é“¾ {chain_id} ä¸Šæœªæ‰¾åˆ°äº¤æ˜“å¯¹")
                
                except Exception as e:
                    logger.error(f"åœ¨é“¾ {chain_id} ä¸ŠæŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}")
                    logger.debug(traceback.format_exc())
        
        # åœºæ™¯2ï¼šæœ‰åˆçº¦åœ°å€å’Œé“¾ä¿¡æ¯ï¼Œç›´æ¥ç”¨DEX APIè·å–å®Œæ•´ä¿¡æ¯
        elif contract_address and chain and chain != "UNKNOWN":
            logger.info(f"åœºæ™¯2ï¼šå·²è·å–åˆçº¦åœ°å€å’Œé“¾ä¿¡æ¯ï¼Œç›´æ¥é€šè¿‡DEX APIè·å–å®Œæ•´æ•°æ®")
            try:
                # å¯¼å…¥DEX Screener APIæ¨¡å—å’Œé“¾IDè½¬æ¢å‡½æ•°
                from src.api.dex_screener_api import get_token_pools
                from src.api.token_market_updater import _normalize_chain_id
                
                chain_id = _normalize_chain_id(chain)
                if chain_id:
                    logger.info(f"å°è¯•åœ¨é“¾ {chain_id} ä¸ŠæŸ¥è¯¢åˆçº¦åœ°å€ {contract_address}")
                    pools_data = get_token_pools(chain_id, contract_address)
                    
                    # å¤„ç†APIè¿”å›çš„æ•°æ®ç»“æ„
                    pairs = []
                    if isinstance(pools_data, dict) and "pairs" in pools_data:
                        pairs = pools_data.get("pairs", [])
                    else:
                        pairs = pools_data
                    
                    if pairs:
                        # è·å–ä»£å¸ç¬¦å·
                        if not token_symbol and len(pairs) > 0:
                            baseToken = pairs[0].get("baseToken", {})
                            if baseToken:
                                token_symbol = baseToken.get("symbol")
                                logger.info(f"ä»DEX APIè·å–åˆ°ä»£å¸ç¬¦å·: {token_symbol}")
                        
                        # è·å–å¸‚å€¼
                        if not market_cap and len(pairs) > 0:
                            max_market_cap = 0
                            for pair in pairs:
                                pair_market_cap = pair.get("marketCap", 0)
                                if pair_market_cap and float(pair_market_cap) > max_market_cap:
                                    max_market_cap = float(pair_market_cap)
                            
                            if max_market_cap > 0:
                                market_cap = str(max_market_cap)
                                logger.info(f"ä»DEX APIè·å–åˆ°å¸‚å€¼: {market_cap}")
                        
                        # è·å–ä»·æ ¼
                        if not price and len(pairs) > 0:
                            for pair in pairs:
                                if "priceUsd" in pair:
                                    price = float(pair["priceUsd"])
                                    logger.info(f"ä»DEX APIè·å–åˆ°ä»·æ ¼: {price}")
                                    break
                        
                        token_info_completed = True
                    else:
                        logger.warning(f"åœ¨é“¾ {chain_id} ä¸Šæœªæ‰¾åˆ°äº¤æ˜“å¯¹")
            except Exception as e:
                logger.error(f"è·å–DEXæ•°æ®æ—¶å‡ºé”™: {str(e)}")
                logger.debug(traceback.format_exc())
        
        # åœºæ™¯3ï¼šä»…æœ‰ä»£å¸ç¬¦å·å’Œé“¾ä¿¡æ¯ï¼ŒæŸ¥è¯¢æ•°æ®åº“ä¸­å·²æœ‰ä¿¡æ¯
        elif token_symbol and chain and chain != "UNKNOWN" and not contract_address:
            logger.info(f"åœºæ™¯3ï¼šå·²è·å–ä»£å¸ç¬¦å·å’Œé“¾ä¿¡æ¯ï¼Œå°è¯•ä»æ•°æ®åº“ä¸­æŸ¥æ‰¾å·²æœ‰ä¿¡æ¯")
            try:
                # å°è¯•ä»æ•°æ®åº“ä¸­æŸ¥æ‰¾è¯¥ä»£å¸
                from sqlalchemy import create_engine, text
                from sqlalchemy.orm import sessionmaker
                import config.settings as config
                
                # éæµ‹è¯•ç¯å¢ƒä¸‹æŸ¥è¯¢æ•°æ®åº“
                if not is_testing:
                    # ä½¿ç”¨ supabase æŸ¥è¯¢
                    from supabase import create_client
                    
                    if config.SUPABASE_URL and config.SUPABASE_KEY:
                        supabase = create_client(config.SUPABASE_URL, config.SUPABASE_KEY)
                        
                        # æŸ¥è¯¢ç›¸åº”é“¾ä¸Šçš„ä»£å¸ç¬¦å·
                        response = supabase.table('tokens').select('*').eq('chain', chain).eq('token_symbol', token_symbol).execute()
                        
                        if hasattr(response, 'data') and len(response.data) > 0:
                            # æ‰¾åˆ°åŒ¹é…çš„ä»£å¸
                            token_data = response.data[0]
                            contract_address = token_data.get('contract')
                            if contract_address:
                                logger.info(f"ä»æ•°æ®åº“ä¸­æ‰¾åˆ°ä»£å¸ {token_symbol} åœ¨é“¾ {chain} ä¸Šçš„åˆçº¦åœ°å€: {contract_address}")
                                
                                # å¯ä»¥ç»§ç»­ä»æ•°æ®åº“ä¸­è·å–å…¶ä»–ä¿¡æ¯
                                if not market_cap and token_data.get('market_cap'):
                                    market_cap = str(token_data.get('market_cap'))
                                
                                if not price and token_data.get('price'):
                                    price = token_data.get('price')
                                
                                # æ›´æ–°å…¶ä»–URLä¿¡æ¯
                                if not telegram_url and token_data.get('telegram_url'):
                                    telegram_url = token_data.get('telegram_url')
                                
                                if not twitter_url and token_data.get('twitter_url'):
                                    twitter_url = token_data.get('twitter_url')
                                
                                if not website_url and token_data.get('website_url'):
                                    website_url = token_data.get('website_url')
                                
                                token_info_completed = True
                        else:
                            logger.warning(f"åœ¨æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ä»£å¸ {token_symbol} åœ¨é“¾ {chain} ä¸Šçš„è®°å½•")
            except Exception as e:
                logger.error(f"æŸ¥è¯¢æ•°æ®åº“æ—¶å‡ºé”™: {str(e)}")
                logger.debug(traceback.format_exc())
        
        # åœºæ™¯4ï¼šåˆ¤æ–­æ˜¯å¦æ˜¯åºŸä¿¡æ¯
        if not token_info_completed:
            # å¦‚æœæœªèƒ½è¡¥å…¨ä»£å¸ä¿¡æ¯ï¼Œä¸”ä¸ç¬¦åˆä»¥ä¸‹æ¡ä»¶ä¹‹ä¸€ï¼Œè§†ä¸ºåºŸä¿¡æ¯
            # 1. æœ‰åˆçº¦åœ°å€
            # 2. æœ‰ä»£å¸ç¬¦å·å’Œé“¾ä¿¡æ¯
            if not (
                contract_address or 
                (token_symbol and chain and chain != "UNKNOWN")
            ):
                logger.warning("ä¸æ»¡è¶³ä¿¡æ¯å¤„ç†æ¡ä»¶ï¼Œè§†ä¸ºåºŸä¿¡æ¯")
                return None
        # ================ æ–°å¢ä»£ç éƒ¨åˆ†ç»“æŸ ================
        
        # ä½¿ç”¨ä»£å¸åˆ†æå™¨è¿›è¡Œæƒ…æ„Ÿåˆ†æå’Œå¸‚åœºè¯„ä¼°
        sentiment_score = None
        positive_words = []
        negative_words = []
        hype_score = None
        risk_level = 'unknown'
        
        if HAS_ANALYZER and token_analyzer:
            try:
                # æ‰§è¡Œæƒ…æ„Ÿåˆ†æ
                analysis_result = token_analyzer.analyze_text(message_text)
                sentiment_score = analysis_result.get('sentiment_score')
                positive_words = analysis_result.get('positive_words', [])
                negative_words = analysis_result.get('negative_words', [])
                hype_score = analysis_result.get('hype_score')
                risk_level = analysis_result.get('risk_level', 'unknown')
                
                # ç¡®ä¿é£é™©ç­‰çº§æ˜¯æœ‰æ•ˆå€¼
                if risk_level not in ['low', 'medium', 'high', 'medium-high', 'low-medium', 'unknown']:
                    # å¤„ç†ä¸­æ–‡é£é™©ç­‰çº§ï¼Œç»Ÿä¸€è½¬ä¸ºè‹±æ–‡
                    if risk_level == 'ä½':
                        risk_level = 'low'
                    elif risk_level == 'ä¸­':
                        risk_level = 'medium'
                    elif risk_level == 'é«˜':
                        risk_level = 'high'
                    else:
                        risk_level = 'unknown'
                
                logger.info(f"æƒ…æ„Ÿåˆ†æç»“æœ - å¾—åˆ†: {sentiment_score}, é£é™©: {risk_level}, ç‚’ä½œ: {hype_score}")
                if positive_words:
                    logger.debug(f"ç§¯æè¯æ±‡: {', '.join(positive_words[:5])}")
                if negative_words:
                    logger.debug(f"æ¶ˆæè¯æ±‡: {', '.join(negative_words[:5])}")
            except Exception as e:
                logger.error(f"æƒ…æ„Ÿåˆ†æå‡ºé”™: {str(e)}")
                logger.debug(traceback.format_exc())
        
        # åˆ›å»ºå¹¶è¿”å›PromotionInfoå¯¹è±¡
        promotion_info = PromotionInfo(
            token_symbol=token_symbol,
            contract_address=contract_address,
            market_cap=market_cap,
            promotion_count=1,  # åˆå§‹æ¨å¹¿è®¡æ•°
            telegram_url=telegram_url,
            twitter_url=twitter_url,
            website_url=website_url,
            first_trending_time=date,
            chain=chain,
            # å¢å¼ºå­—æ®µ
            price=price,
            sentiment_score=sentiment_score,
            positive_words=positive_words,
            negative_words=negative_words,
            hype_score=hype_score,
            risk_level=risk_level
        )
        
        logger.info(f"æˆåŠŸæå–æ¨å¹¿ä¿¡æ¯: ä»£å¸={token_symbol}, åˆçº¦={contract_address}, å¸‚å€¼={market_cap}, é“¾={chain}")
        return promotion_info
            
    except Exception as e:
        logger.error(f"è§£ææ¨å¹¿ä¿¡æ¯å‡ºé”™: {str(e)}")
        logger.debug(traceback.format_exc())
        return None

def extract_chain_from_message(message_text: str) -> Optional[str]:
    """ä»æ¶ˆæ¯æ–‡æœ¬ä¸­æå–åŒºå—é“¾ä¿¡æ¯
    
    Args:
        message_text: éœ€è¦è§£æçš„æ¶ˆæ¯æ–‡æœ¬
        
    Returns:
        str: æå–åˆ°çš„é“¾åç§°ï¼Œæœªæ‰¾åˆ°åˆ™è¿”å›None
    """
    # æ¸…ç†æ¶ˆæ¯æ–‡æœ¬ï¼Œä¾¿äºåŒ¹é…
    text = message_text.lower()
    
    # å®šä¹‰ä¸åŒé“¾çš„å…³é”®è¯åŒ¹é…è§„åˆ™
    chain_patterns = {
        'SOL': [r'\bsol\b', r'\bsolana\b', r'@solana', r'solanas', r'ì†”ë¼ë‚˜', r'ç´¢æ‹‰çº³', 
                r'solscan\.io', r'explorer\.solana\.com', r'solana_trojanbot', r'solé“¾'],
        'ETH': [r'\beth\b', r'\bethereum\b', r'@ethereum', r'ä»¥å¤ªåŠ', r'ì´ë”ë¦¬ì›€', 
                r'etherscan\.io', r'uniswap', r'sushiswap', r'ethé“¾'],
        'BSC': [r'\bbsc\b', r'\bbinance\b', r'\bbnb\b', r'å¸å®‰é“¾', r'ë°”ì´ë‚¸ìŠ¤', 
                r'bscscan\.com', r'pancakeswap', r'bscé“¾'],
        'ARB': [r'\barb\b', r'\barbitrum\b', r'arbitrums', r'é˜¿æ¯”ç‰¹é¾™', r'ì•„ë¹„íŠ¸ëŸ¼', 
                r'arbiscan\.io', r'arbé“¾'],
        'BASE': [r'\bbase\b', r'basechain', r'coinbase', r'è´æ–¯é“¾', r'ë² ì´ìŠ¤', 
                 r'basescan\.org', r'baseé“¾'],
        'AVAX': [r'\bavax\b', r'\bavalanche\b', r'é›ªå´©é“¾', r'ì•„ë°œë€ì²´', 
                 r'snowtrace\.io', r'traderjoe', r'avaxé“¾'],
        'MATIC': [r'\bmatic\b', r'\bpolygon\b', r'æ³¢åˆ©å†ˆ', r'í´ë¦¬ê³¤', 
                  r'polygonscan\.com', r'maticé“¾'],
        'OP': [r'\boptimism\b', r'\bop\b', r'ä¹è§‚é“¾', r'ì˜µí‹°ë¯¸ì¦˜', 
               r'optimistic\.etherscan\.io', r'opé“¾']
    }
    
    # æ£€æŸ¥åŒ¹é…
    for chain, patterns in chain_patterns.items():
        for pattern in patterns:
            if re.search(pattern, text):
                logger.debug(f"ä»æ¶ˆæ¯ä¸­æå–åˆ°é“¾ä¿¡æ¯: {chain}, åŒ¹é…æ¨¡å¼: {pattern}")
                return chain
    
    # å¤„ç†ä¸­æ–‡ç¯å¢ƒ
    chinese_chains = {
        'SOL': ['solana', 'sol', 'ç´¢æ‹‰çº³', 'ç´¢å…°çº³'],
        'ETH': ['ethereum', 'eth', 'ä»¥å¤ªåŠ', 'ä»¥å¤ª'],
        'BSC': ['binance', 'bsc', 'bnb', 'å¸å®‰'],
        'AVAX': ['avalanche', 'avax', 'é›ªå´©'],
        'MATIC': ['polygon', 'matic', 'æ³¢åˆ©å†ˆ']
    }
    
    for chain, keywords in chinese_chains.items():
        for keyword in keywords:
            if keyword in text:
                logger.debug(f"ä»ä¸­æ–‡ç¯å¢ƒæå–åˆ°é“¾ä¿¡æ¯: {chain}, å…³é”®è¯: {keyword}")
                return chain
    
    # æå–dexscreener URLå¹¶è§£æ
    # å¤„ç†æ ¼å¼: dexscreener.com/solana/xxx æˆ– dexscreener.com/ethereum/xxxç­‰
    dexscreener_match = re.search(r'(?:https?://)?(?:www\.)?dexscreener\.com/([a-zA-Z0-9]+)(?:/[^/\s]+)?', text)
    if dexscreener_match:
        chain_str = dexscreener_match.group(1).upper()
        # æ˜ å°„DEX Screener URLè·¯å¾„åˆ°é“¾æ ‡è¯†
        dexscreener_map = {
            'SOLANA': 'SOL',
            'ETHEREUM': 'ETH',
            'BSC': 'BSC',
            'ARBITRUM': 'ARB',
            'BASE': 'BASE',
            'AVALANCHE': 'AVAX',
            'POLYGON': 'MATIC',
            'OPTIMISM': 'OP'
        }
        if chain_str in dexscreener_map:
            logger.debug(f"ä»DEX Screener URLæå–åˆ°é“¾ä¿¡æ¯: {dexscreener_map[chain_str]}")
            return dexscreener_map[chain_str]
    
    # å¤„ç†æ›´å¤æ‚çš„dexscreener URLæ ¼å¼ï¼Œä¾‹å¦‚å®Œæ•´çš„äº¤æ˜“å¯¹åœ°å€URL
    # ç¤ºä¾‹: dexscreener.com/solana/efmy21qz1qrrlpmis3neczrpbwhrxhnwyodss6nxf8q9DtNtJbA8JrVDCnoKsfhBFgDFzSkL5EX3mv6FubSBpump
    complex_dexscreener = re.search(r'dexscreener\.com/([a-zA-Z0-9]+)/[a-zA-Z0-9]{10,}', text, re.IGNORECASE)
    if complex_dexscreener:
        chain_str = complex_dexscreener.group(1).upper()
        dexscreener_map = {
            'SOLANA': 'SOL',
            'ETHEREUM': 'ETH',
            'BSC': 'BSC',
            'ARBITRUM': 'ARB',
            'BASE': 'BASE',
            'AVALANCHE': 'AVAX',
            'POLYGON': 'MATIC',
            'OPTIMISM': 'OP'
        }
        if chain_str in dexscreener_map:
            logger.debug(f"ä»å¤æ‚çš„DEX Screener URLæå–åˆ°é“¾ä¿¡æ¯: {dexscreener_map[chain_str]}")
            return dexscreener_map[chain_str]
    
    # è¿˜å¯ä»¥ä»åˆçº¦åœ°å€æ ¼å¼æ¨æ–­
    if re.search(r'\b0x[0-9a-fA-F]{40}\b', text):
        # ä»¥å¤ªåŠæ ¼å¼åœ°å€ï¼Œä¸èƒ½ç¡®å®šæ˜¯ETH/BSC/MATICç­‰ï¼Œé»˜è®¤è¿”å›ETH
        logger.debug("ä»åˆçº¦åœ°å€æ ¼å¼æ¨æ–­å¯èƒ½æ˜¯ETHé“¾")
        return 'ETH'
        
    if re.search(r'\b[1-9A-HJ-NP-Za-km-z]{32,44}\b', text) and ('sol' in text or 'solana' in text):
        # Solana Base58æ ¼å¼åœ°å€
        logger.debug("ä»åˆçº¦åœ°å€æ ¼å¼æ¨æ–­å¯èƒ½æ˜¯SOLé“¾")
        return 'SOL'
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹å®šçš„æœºå™¨äººå¼•ç”¨
    if 'solana_trojanbot' in text:
        return 'SOL'
    
    # æ£€æŸ¥äº¤æ˜“æ‰€ç‰¹å®šå…³é”®å­—
    if 'raydium' in text or 'orca.so' in text or 'jupiter' in text:
        return 'SOL'
    
    if 'uniswap' in text or 'sushiswap' in text:
        return 'ETH'
    
    if 'pancakeswap' in text or 'poocoin' in text:
        return 'BSC'
    
    return None

def extract_url_from_text(text: str, keyword: str = '') -> Optional[str]:
    """ä»æ–‡æœ¬ä¸­æå–URL"""
    try:
        if not text:
            return None
            
        # æŸ¥æ‰¾å¸¸è§çš„URLå¼€å§‹æ ‡è®°
        url_starts = ['http://', 'https://', 'www.']
        if keyword:
            url_starts.append(keyword)
        
        for start in url_starts:
            if start in text.lower():
                start_idx = text.lower().find(start)
                if start_idx >= 0:
                    # ä»URLå¼€å§‹å¤„æå–å­—ç¬¦ä¸²
                    url_part = text[start_idx:]
                    # æŸ¥æ‰¾URLç»“æŸæ ‡è®°
                    end_markers = [' ', '\n', '\t', ')', ']', '}', ',', ';']
                    end_idx = len(url_part)
                    for marker in end_markers:
                        marker_idx = url_part.find(marker)
                        if marker_idx > 0 and marker_idx < end_idx:
                            end_idx = marker_idx
                    
                    return url_part[:end_idx].strip()
        
        return None
    except Exception as e:
        print(f"æå–URLæ—¶å‡ºé”™: {str(e)}")
        return None

def get_last_message_with_promotion(db_path):
    """è·å–æœ€æ–°ä¸€æ¡æœ‰Promotionä¿¡æ¯çš„æ¶ˆæ¯
    
    æ³¨æ„ï¼šæ­¤å‡½æ•°å·²è¢«åºŸå¼ƒï¼Œè¯·ç›´æ¥ä½¿ç”¨Supabaseé€‚é…å™¨çš„API
    
    Args:
        db_path: æ•°æ®åº“è·¯å¾„ï¼ˆå·²åºŸå¼ƒï¼Œä¸å†ä½¿ç”¨ï¼‰
        
    Returns:
        è¿”å›ä¸€ä¸ªå…ƒç»„ (message_dict, promotion_info)
    """
    logger.warning("ä½¿ç”¨äº†åºŸå¼ƒçš„get_last_message_with_promotionå‡½æ•°ï¼Œæ¨èç›´æ¥ä½¿ç”¨Supabaseé€‚é…å™¨API")
    
    try:
        # ä½¿ç”¨æ•°æ®åº“é€‚é…å™¨
        from src.database.db_factory import get_db_adapter
        db_adapter = get_db_adapter()
        
        # è·å–æœ€æ–°çš„æ¶ˆæ¯
        latest_message = asyncio.run(db_adapter.get_latest_message())
        
        if not latest_message:
            logger.warning("æœªæ‰¾åˆ°ä»»ä½•æ¶ˆæ¯")
            return None, None
            
        chain = latest_message.get('chain')
        message_id = latest_message.get('message_id')
        date_str = latest_message.get('date')
        text = latest_message.get('text')
        media_path = latest_message.get('media_path')
        channels = latest_message.get('channels', [])
        
        # å¤„ç†æ—¶é—´
        try:
            if isinstance(date_str, str):
                if '+00:00' in date_str:
                    date_str = date_str.replace('+00:00', '')
                    date = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')
                elif date_str.replace('.', '').isdigit():
                    date = datetime.fromtimestamp(float(date_str), timezone.utc)
                elif 'T' in date_str:
                    # ISOæ ¼å¼
                    date = datetime.fromisoformat(date_str)
                else:
                    # å°è¯•å¤šç§å¸¸è§çš„æ—¥æœŸæ ¼å¼
                    date_formats = [
                        '%Y-%m-%d %H:%M:%S',
                        '%Y/%m/%d %H:%M:%S',
                        '%d-%m-%Y %H:%M:%S',
                        '%d/%m/%Y %H:%M:%S',
                        '%Y-%m-%dT%H:%M:%S'
                    ]
                    
                    for fmt in date_formats:
                        try:
                            date = datetime.strptime(date_str, fmt)
                            break
                        except ValueError:
                            continue
                    else:
                        # å¦‚æœæ‰€æœ‰æ ¼å¼éƒ½å¤±è´¥ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
                        logger.warning(f"æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_str}ï¼Œä½¿ç”¨å½“å‰æ—¶é—´ä»£æ›¿")
                        date = datetime.now(timezone.utc)
            elif isinstance(date_str, (int, float)):
                date = datetime.fromtimestamp(date_str, timezone.utc)
            elif isinstance(date_str, datetime):
                # å¦‚æœå·²ç»æ˜¯datetimeå¯¹è±¡ï¼Œç›´æ¥ä½¿ç”¨
                date = date_str
                # ç¡®ä¿æœ‰æ—¶åŒºä¿¡æ¯
                if date.tzinfo is None:
                    date = date.replace(tzinfo=timezone.utc)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ—¥æœŸæ ¼å¼: {date_str}, ç±»å‹: {type(date_str)}")
                
            logger.debug(f"å¤„ç†åçš„æ—¥æœŸ: {date}")
            
        except Exception as e:
            logger.error(f"å¤„ç†æ—¶é—´å‡ºé”™: {date_str}, é”™è¯¯: {str(e)}")
            date = datetime.now(timezone.utc)
        
        message = {
            'message_id': message_id,
            'chain': chain,
            'date': date,
            'text': text,
            'media_path': media_path,
            'channels': channels
        }
        
        # å¤„ç†promotionä¿¡æ¯
        promo = extract_promotion_info(text, date, chain) if text else None
        if promo:
            print("\n=== Promotion Info ===")
            print(f"Token Symbol: {promo.token_symbol}")
            print(f"Contract Address: {promo.contract_address}")
            print(f"Market Cap: {promo.market_cap}")
            print(f"Promotion Count: {promo.promotion_count}")
            print(f"Telegram URL: {promo.telegram_url}")
            print(f"Twitter URL: {promo.twitter_url}")
            print(f"Website URL: {promo.website_url}")
            print(f"First Trending Time: {promo.first_trending_time}")
        
        return message, promo
    
    except Exception as e:
        logger.error(f"è·å–æœ€æ–°æ¶ˆæ¯å‡ºé”™: {str(e)}")
        import traceback
        logger.debug(traceback.format_exc())
        return None, None

def format_token_history(history: list) -> str:
    """æ ¼å¼åŒ–ä»£å¸å†å²æ•°æ®ä¸ºæ˜“è¯»çš„å­—ç¬¦ä¸²"""
    if not history:
        return "æœªæ‰¾åˆ°è¯¥ä»£å¸çš„å†å²æ•°æ®"
    
    output = []
    output.append("=== ä»£å¸å†å²æ•°æ® ===\n")
    
    # è·å–ç¬¬ä¸€æ¡æ•°æ®ä¸­çš„ä»£å¸ä¿¡æ¯
    _, first_promo = history[0]
    if first_promo:
        output.append(f"ä»£å¸ç¬¦å·: {first_promo.token_symbol}")
        output.append(f"åˆçº¦åœ°å€: {first_promo.contract_address}\n")
    
    # æ·»åŠ æ¯æ¡è®°å½•çš„è¯¦ç»†ä¿¡æ¯
    for message, promo in history:
        # æ­£ç¡®å¤„ç†æ—¶åŒºè½¬æ¢
        date = message['date']
        if isinstance(date, (int, float)):
            # å‡è®¾æ—¶é—´æˆ³æ˜¯UTCæ—¶é—´
            utc_time = datetime.fromtimestamp(date, timezone.utc)
        else:
            # å¦‚æœæ˜¯datetimeå¯¹è±¡ï¼Œç¡®ä¿å®ƒæœ‰UTCæ—¶åŒºä¿¡æ¯
            utc_time = timezone.utc.localize(date) if not date.tzinfo else date
            
        # è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´ (UTC+8)
        beijing_tz = timezone(timedelta(hours=8))
        beijing_time = utc_time.astimezone(beijing_tz)
        
        # è¾“å‡ºåŒ—äº¬æ—¶é—´ï¼Œæ˜ç¡®æ ‡æ³¨æ—¶åŒº
        output.append(f"æ—¶é—´: {beijing_time.strftime('%Y-%m-%d %H:%M:%S')} (UTC+8)")
        if promo:
            if promo.market_cap is not None:
                output.append(f"å¸‚å€¼: ${promo.market_cap:,.2f}")
            if promo.promotion_count is not None:
                output.append(f"æ¨å¹¿æ¬¡æ•°: {promo.promotion_count}")
        output.append(f"æ¶ˆæ¯ID: {message['message_id']}")
        output.append("æ¶ˆæ¯å†…å®¹:")
        output.append(message['text'])
        output.append("-" * 50 + "\n")
    
    return "\n".join(output)

def update_token_info(conn, token_data):
    """æ›´æ–°æˆ–æ’å…¥ä»£å¸ä¿¡æ¯
    
    æ³¨æ„ï¼šæ­¤å‡½æ•°å·²è¢«åºŸå¼ƒï¼Œè¯·ä½¿ç”¨save_token_infoæˆ–db_adapter.save_token
    
    Args:
        conn: æ•°æ®åº“è¿æ¥ï¼ˆå·²åºŸå¼ƒï¼Œä¸å†ä½¿ç”¨ï¼‰
        token_data: ä»£å¸æ•°æ®
        
    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    # æ—¥å¿—è­¦å‘Šï¼šä½¿ç”¨äº†åºŸå¼ƒçš„å‡½æ•°
    logger.warning("ä½¿ç”¨äº†åºŸå¼ƒçš„update_token_infoå‡½æ•°ï¼Œæ¨èä½¿ç”¨save_token_info")
    
    try:
        # éªŒè¯ä»£å¸æ•°æ®
        is_valid, error_msg = validate_token_data(token_data)
        if not is_valid:
            logger.error(f"æ— æ•ˆçš„ä»£å¸æ•°æ®: {error_msg}")
            return False
        
        # ä½¿ç”¨æ•°æ®åº“é€‚é…å™¨
        from src.database.db_factory import get_db_adapter
        db_adapter = get_db_adapter()
        
        # ä½¿ç”¨å¼‚æ­¥è¿è¡Œæ—¶æ¥è¿è¡Œå¼‚æ­¥ä¿å­˜æ–¹æ³•
        result = asyncio.run(db_adapter.save_token(token_data))
        
        # å¦‚æœtokenä¿å­˜æˆåŠŸä¸”æœ‰contractå­—æ®µï¼Œä¿å­˜token markä¿¡æ¯
        if result and token_data.get('contract'):
            # ä¿å­˜ä»£å¸æ ‡è®°
            mark_result = asyncio.run(db_adapter.save_token_mark(token_data))
            if not mark_result:
                logger.warning(f"ä¿å­˜ä»£å¸æ ‡è®°å¤±è´¥: {token_data.get('token_symbol')}")
        
        return result
    except Exception as e:
        logger.error(f"æ›´æ–°ä»£å¸ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        logger.debug(traceback.format_exc())
        return False

def save_token_mark(conn, token_data):
    """
    ä¿å­˜ä»£å¸æ ‡è®°æ•°æ®åˆ°tokens_markè¡¨
    
    æ³¨æ„ï¼šæ­¤å‡½æ•°å·²è¢«åºŸå¼ƒï¼Œè¯·ä½¿ç”¨db_adapter.save_token_mark
    
    Args:
        conn: æ•°æ®åº“è¿æ¥ï¼ˆå·²åºŸå¼ƒï¼Œä¸å†ä½¿ç”¨ï¼‰
        token_data: ä»£å¸ä¿¡æ¯æ•°æ®
    
    Returns:
        bool: æ“ä½œæ˜¯å¦æˆåŠŸ
    """
    # æ—¥å¿—è­¦å‘Šï¼šä½¿ç”¨äº†åºŸå¼ƒçš„å‡½æ•°
    logger.warning("ä½¿ç”¨äº†åºŸå¼ƒçš„save_token_markå‡½æ•°ï¼Œæ¨èä½¿ç”¨db_adapter.save_token_mark")
    
    try:
        # ä½¿ç”¨æ•°æ®åº“é€‚é…å™¨
        from src.database.db_factory import get_db_adapter
        db_adapter = get_db_adapter()
        
        # ä½¿ç”¨å¼‚æ­¥è¿è¡Œæ—¶æ¥è¿è¡Œå¼‚æ­¥ä¿å­˜æ–¹æ³•
        result = asyncio.run(db_adapter.save_token_mark(token_data))
        return result
        
    except Exception as e:
        logger.error(f"ä¿å­˜ä»£å¸æ ‡è®°æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        logger.debug(f"é—®é¢˜æ•°æ®: {token_data}")
        logger.debug(traceback.format_exc())
        return False

def get_db_performance_stats():
    """è·å–æ•°æ®åº“æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        æ€§èƒ½ç»Ÿè®¡æ•°æ®å­—å…¸
    """
    stats = db_performance_stats.copy()
    
    # è®¡ç®—æ¯ä¸ªæ“ä½œçš„å¹³å‡æ‰§è¡Œæ—¶é—´
    avg_times = {}
    for op_name, total_time in stats['operation_times'].items():
        count = stats['operation_counts'].get(op_name, 0)
        if count > 0:
            avg_times[op_name] = total_time / count
        else:
            avg_times[op_name] = 0
    
    stats['average_times'] = avg_times
    
    # æ·»åŠ SupabaseçŠ¶æ€ä¿¡æ¯
    try:
        from src.database.db_factory import get_db_adapter
        db_adapter = get_db_adapter()
        stats['adapter_type'] = 'supabase'
        stats['database_uri'] = db_adapter.database_url if hasattr(db_adapter, 'database_url') else 'unknown'
    except Exception as e:
        logger.error(f"è·å–æ•°æ®åº“çŠ¶æ€ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        stats['adapter_error'] = str(e)
    
    return stats

def reset_db_performance_stats():
    """é‡ç½®æ•°æ®åº“æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
    global db_performance_stats
    db_performance_stats = {
        'operation_counts': {},
        'operation_times': {},
        'lock_errors': 0,
        'total_retries': 0
    }

# æ·»åŠ ç¼ºå¤±çš„æ¸…ç†æ‰¹å¤„ç†ä»»åŠ¡å‡½æ•°
async def cleanup_batch_tasks():
    """æ¸…ç†æ‰¹å¤„ç†ä»»åŠ¡"""
    global message_batch, token_batch
    
    try:
        # å¦‚æœæœ‰æœªå¤„ç†çš„æ¶ˆæ¯ï¼Œå…ˆå¤„ç†
        if message_batch:
            logger.info(f"æ¸…ç† {len(message_batch)} æ¡æœªå¤„ç†çš„æ¶ˆæ¯...")
            await process_message_batch()
            
        # å¦‚æœæœ‰æœªå¤„ç†çš„ä»£å¸ä¿¡æ¯ï¼Œå…ˆå¤„ç†
        if token_batch:
            logger.info(f"æ¸…ç† {len(token_batch)} æ¡æœªå¤„ç†çš„ä»£å¸ä¿¡æ¯...")
            # è·å–æ•°æ®åº“é€‚é…å™¨
            db_adapter = get_db_adapter()
            for token_data in token_batch:
                await db_adapter.save_token(token_data)
                
        # æ¸…ç©ºæ‰¹å¤„ç†é˜Ÿåˆ—
        message_batch = []
        token_batch = []
        logger.info("æ‰¹å¤„ç†ä»»åŠ¡æ¸…ç†å®Œæˆ")
        
    except Exception as e:
        logger.error(f"æ¸…ç†æ‰¹å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}")
        logger.debug(traceback.format_exc())

def calculate_community_reach(token_symbol: str, session=None):
    """è®¡ç®—ä»£å¸çš„ç¤¾ç¾¤è¦†ç›–äººæ•°
    
    è®¡ç®—æ–¹å¼ï¼š
    1. æ ¹æ®token_symbolæŸ¥è¯¢tokens_markè¡¨ä¸­çš„æ‰€æœ‰æ¡ç›®
    2. ç»Ÿè®¡æŸ¥è¯¢ç»“æœä¸­çš„å”¯ä¸€channel_id
    3. æ ¹æ®channel_idæŸ¥è¯¢telegram_channelsè¡¨ä¸­çš„member_count
    4. å°†member_countç›¸åŠ å¾—åˆ°community_reach
    
    Args:
        token_symbol: ä»£å¸ç¬¦å·
        session: åºŸå¼ƒå‚æ•°ï¼Œä¸ºäº†å…¼å®¹æ€§ä¿ç•™
        
    Returns:
        int: è®¡ç®—å¾—åˆ°çš„ç¤¾ç¾¤è¦†ç›–äººæ•°
    """
    # ä½¿ç”¨ç¼“å­˜é¿å…é¢‘ç¹è®¡ç®—ç›¸åŒçš„token
    cache_key = f"community_reach_{token_symbol}"
    if hasattr(calculate_community_reach, 'cache') and cache_key in calculate_community_reach.cache:
        # ç¼“å­˜æœ‰æ•ˆæœŸä¸º5åˆ†é’Ÿ
        cache_time, value = calculate_community_reach.cache[cache_key]
        if (datetime.now() - cache_time).total_seconds() < 300:  # 5åˆ†é’Ÿå†…
            return value

    # åˆå§‹åŒ–ç¼“å­˜å­—å…¸(å¦‚æœä¸å­˜åœ¨)
    if not hasattr(calculate_community_reach, 'cache'):
        calculate_community_reach.cache = {}
    
    try:
        # ä½¿ç”¨Supabaseé€‚é…å™¨
        from src.database.db_factory import get_db_adapter
        db_adapter = get_db_adapter()
        
        # ä½¿ç”¨å¼‚æ­¥è°ƒç”¨ä½†åœ¨åŒæ­¥ç¯å¢ƒä¸­æ‰§è¡Œ
        async def get_community_reach():
            # 1. è·å–æ‰€æœ‰æåŠè¯¥ä»£å¸çš„token_markè®°å½•
            token_marks = await db_adapter.execute_query(
                'tokens_mark',
                'select',
                filters={'token_symbol': token_symbol}
            )
            
            # å¦‚æœæ²¡æœ‰è®°å½•ï¼Œè¿”å›0
            if not token_marks:
                return 0
                
            # 2. æå–å”¯ä¸€çš„channel_id
            channel_ids = []
            for mark in token_marks:
                if isinstance(mark, dict) and mark.get('channel_id') and mark['channel_id'] not in channel_ids:
                    channel_ids.append(mark['channel_id'])
            
            # å¦‚æœæ²¡æœ‰channel_idï¼Œè¿”å›0
            if not channel_ids:
                return 0
                
            # 3. è·å–è¿™äº›é¢‘é“çš„æˆå‘˜æ•°
            total_reach = 0
            for channel_id in channel_ids:
                channel_info = await db_adapter.get_channel_by_id(channel_id)
                if channel_info and channel_info.get('member_count'):
                    total_reach += channel_info['member_count']
            
            return total_reach
        
        # æ‰§è¡Œå¼‚æ­¥å‡½æ•°
        total_reach = asyncio.run(get_community_reach())
                
        # å­˜å…¥ç¼“å­˜
        calculate_community_reach.cache[cache_key] = (datetime.now(), total_reach)
        
        # é™åˆ¶ç¼“å­˜å¤§å°ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
        if len(calculate_community_reach.cache) > 1000:  # æœ€å¤šç¼“å­˜1000ä¸ªtoken
            # åˆ é™¤æœ€æ—©çš„20%ç¼“å­˜
            sorted_keys = sorted(
                calculate_community_reach.cache.keys(),
                key=lambda k: calculate_community_reach.cache[k][0]
            )
            for key in sorted_keys[:200]:  # åˆ é™¤æœ€æ—©çš„200ä¸ª
                del calculate_community_reach.cache[key]
        
        # è¿”å›æ€»è¦†ç›–äººæ•°
        return total_reach
        
    except Exception as e:
        logger.error(f"è®¡ç®—ä»£å¸ {token_symbol} çš„ç¤¾ç¾¤è¦†ç›–äººæ•°æ—¶å‡ºé”™: {str(e)}")
        logger.debug(traceback.format_exc())
        return 0
