#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æ•°æ®åº“æ“ä½œå¤„ç†æ¨¡å—
ä»…æ”¯æŒSupabaseæ•°æ®åº“
"""

import asyncio
import traceback
import time
import functools
import json
import os
import re
import inspect
from typing import Tuple, Optional, List, Dict, Any, Callable
from datetime import datetime, timezone, timedelta
from contextlib import contextmanager

# å¯¼å…¥æ•°æ®åº“å·¥å‚ï¼ˆå·²ç»ç§»é™¤SQLAlchemyä¼šè¯ï¼‰
from src.database.db_factory import get_db_adapter
# å¯¼å…¥å¿…è¦çš„æ•°æ®æ¨¡å‹
from src.database.models import PromotionInfo

# æ·»åŠ æ—¥å¿—æ”¯æŒ
try:
    from src.utils.logger import get_logger
    logger = get_logger(__name__)
except ImportError:
    import logging
    logger = logging.getLogger(__name__)

# å¯¼å…¥ä»£å¸åˆ†æå™¨
try:
    from src.analysis.token_analyzer import get_analyzer
    token_analyzer = get_analyzer()
    HAS_ANALYZER = True
except ImportError:
    logger.warning("æ— æ³•å¯¼å…¥ä»£å¸åˆ†æå™¨ï¼Œå°†ä½¿ç”¨åŸºæœ¬åˆ†æ")
    token_analyzer = None
    HAS_ANALYZER = False

# æ‰¹å¤„ç†æ¶ˆæ¯é˜Ÿåˆ—
message_batch = []
token_batch = []

# ä»é…ç½®æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡ä¸­è·å–æ‰¹å¤„ç†è®¾ç½®
try:
    from config.settings import BATCH_SIZE, BATCH_INTERVAL
    MAX_BATCH_SIZE = BATCH_SIZE if hasattr(BATCH_SIZE, '__int__') else 50
    BATCH_TIMEOUT = BATCH_INTERVAL if hasattr(BATCH_INTERVAL, '__int__') else 10
except (ImportError, AttributeError):
    # é»˜è®¤å€¼
    MAX_BATCH_SIZE = 50
    BATCH_TIMEOUT = 10  # ç§’

# é‡è¯•è®¾ç½®
OPERATION_RETRIES = 5  # é‡è¯•æ¬¡æ•°
OPERATION_RETRY_DELAY = 1.0  # é‡è¯•é—´éš”(ç§’)

# æ·»åŠ æ•°æ®åº“æ€§èƒ½ç›‘æ§ç›¸å…³çš„å˜é‡
db_performance_stats = {
    'operation_counts': {},
    'operation_times': {},
    'lock_errors': 0,
    'total_retries': 0
}

@contextmanager
def session_scope():
    """æä¾›äº‹åŠ¡èŒƒå›´çš„ä¼šè¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    
    æ³¨æ„ï¼šæ­¤å‡½æ•°ä»…ä½œä¸ºå…¼å®¹å±‚ã€‚åœ¨Supabaseä¸­æ²¡æœ‰äº‹åŠ¡çš„æ¦‚å¿µï¼Œ
    è€Œæ˜¯ç›´æ¥ä½¿ç”¨adapterè¿›è¡Œæ“ä½œã€‚
    """
    # å¯¼å…¥å…¨å±€é…ç½®
    import config.settings as config
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨Supabase
    if not config.DATABASE_URI.startswith('supabase://'):
        logger.error("æœªä½¿ç”¨Supabaseæ•°æ®åº“ï¼Œè¯·æ£€æŸ¥é…ç½®")
        logger.error(f"å½“å‰DATABASE_URI: {config.DATABASE_URI}")
        logger.error("DATABASE_URIåº”ä»¥'supabase://'å¼€å¤´")
        raise ValueError("å¿…é¡»ä½¿ç”¨Supabaseæ•°æ®åº“")
        
    try:
        # ä½¿ç”¨Supabaseé€‚é…å™¨ï¼Œä¸å†åˆ›å»ºSQLAlchemyä¼šè¯
        from src.database.db_factory import get_db_adapter
        adapter = get_db_adapter()
        logger.debug("ä½¿ç”¨Supabaseé€‚é…å™¨åˆ›å»ºä¼šè¯")
        
        # è¿”å›é€‚é…å™¨å®ä¾‹è€Œä¸æ˜¯ä¼šè¯å¯¹è±¡
        yield adapter
        
    except Exception as e:
        # å‘ç”Ÿé”™è¯¯æ—¶è®°å½•ä½†ä¸å†è¿›è¡Œå›æ»šï¼ˆSupabaseæ²¡æœ‰ä¼šè¯æ¦‚å¿µï¼‰
        logger.error(f"Supabaseæ“ä½œå‡ºé”™: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        # é‡æ–°æŠ›å‡ºå¼‚å¸¸
        raise e

async def process_batches():
    """å®šæœŸå¤„ç†æ‰¹å¤„ç†é˜Ÿåˆ—çš„æ¶ˆæ¯å’Œä»£å¸"""
    global message_batch, token_batch
    
    while True:
        try:
            if message_batch:
                local_batch = message_batch.copy()
                message_batch = []
                
                try:
                    # ä½¿ç”¨Supabaseé€‚é…å™¨å¤„ç†æ‰¹é‡æ¶ˆæ¯
                    from src.database.db_factory import get_db_adapter
                    db_adapter = get_db_adapter()
                    
                    for msg_data in local_batch:
                        try:
                            # æ„å»ºæ¶ˆæ¯æ•°æ®
                            message_data = {
                                'chain': msg_data.get('chain'),
                                'message_id': msg_data.get('message_id'),
                                'date': msg_data.get('date'),
                                'text': msg_data.get('text'),
                                'media_path': msg_data.get('media_path'),
                                'channel_id': msg_data.get('channel_id')
                            }
                            
                            # ä½¿ç”¨Supabaseé€‚é…å™¨ä¿å­˜æ¶ˆæ¯
                            await db_adapter.save_message(message_data)
                        except Exception as e:
                            logger.error(f"å¤„ç†æ¶ˆæ¯æ‰¹æ¬¡æ—¶å‡ºé”™: {str(e)}")
                            logger.debug(traceback.format_exc())
                            continue
                    
                    logger.info(f"æ‰¹é‡å¤„ç†äº† {len(local_batch)} æ¡æ¶ˆæ¯")
                except Exception as e:
                    logger.error(f"æ¶ˆæ¯æ‰¹å¤„ç†å¤±è´¥: {str(e)}")
                    import traceback
                    logger.error(traceback.format_exc())
                
            if token_batch:
                local_batch = token_batch.copy()
                token_batch = []
                
                try:
                    # ä½¿ç”¨Supabaseé€‚é…å™¨å¤„ç†æ‰¹é‡ä»£å¸ä¿¡æ¯
                    from src.database.db_factory import get_db_adapter
                    db_adapter = get_db_adapter()
                    
                    for token_data in local_batch:
                        try:
                            # ç®€å•éªŒè¯ä»£å¸æ•°æ®
                            if not all(key in token_data for key in ['chain', 'token_symbol', 'contract']):
                                logger.warning(f"æ— æ•ˆçš„ä»£å¸æ•°æ®: ç¼ºå°‘å¿…è¦å­—æ®µï¼Œæ•°æ®: {token_data}")
                                continue
                                
                            # ä½¿ç”¨Supabaseé€‚é…å™¨ä¿å­˜ä»£å¸ä¿¡æ¯
                            await db_adapter.save_token(token_data)
                        except Exception as e:
                            logger.error(f"å¤„ç†ä»£å¸æ‰¹æ¬¡æ—¶å‡ºé”™: {str(e)}")
                            logger.debug(traceback.format_exc())
                            continue
                    
                    logger.info(f"æ‰¹é‡å¤„ç†äº† {len(local_batch)} æ¡ä»£å¸ä¿¡æ¯")
                except Exception as e:
                    logger.error(f"ä»£å¸æ‰¹å¤„ç†å¤±è´¥: {str(e)}")
                    import traceback
                    logger.error(traceback.format_exc())
                
            await asyncio.sleep(BATCH_TIMEOUT)
        except Exception as e:
            logger.error(f"æ‰¹å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            logger.debug(traceback.format_exc())
            await asyncio.sleep(5)  # å‡ºé”™åç­‰å¾…çŸ­æš‚æ—¶é—´å†ç»§ç»­

def monitor_db_operation(operation_name):
    """è£…é¥°å™¨å‡½æ•°ï¼šç›‘æ§æ•°æ®åº“æ“ä½œæ€§èƒ½
    
    Args:
        operation_name: æ“ä½œåç§°ï¼Œç”¨äºç»Ÿè®¡
        
    Returns:
        è£…é¥°å™¨å‡½æ•°
    """
    def decorator(func):
        @functools.wraps(func)
        def sync_wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                return func(*args, **kwargs)
            finally:
                execution_time = time.time() - start_time
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                if operation_name not in db_performance_stats['operation_counts']:
                    db_performance_stats['operation_counts'][operation_name] = 0
                    db_performance_stats['operation_times'][operation_name] = 0
                
                db_performance_stats['operation_counts'][operation_name] += 1
                db_performance_stats['operation_times'][operation_name] += execution_time
        
        @functools.wraps(func)
        async def async_wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                return await func(*args, **kwargs)
            finally:
                execution_time = time.time() - start_time
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                if operation_name not in db_performance_stats['operation_counts']:
                    db_performance_stats['operation_counts'][operation_name] = 0
                    db_performance_stats['operation_times'][operation_name] = 0
                
                db_performance_stats['operation_counts'][operation_name] += 1
                db_performance_stats['operation_times'][operation_name] += execution_time
        
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper
    
    return decorator

@monitor_db_operation('save_messages_batch')
async def save_messages_batch(messages: List[Dict]):
    """æ‰¹é‡ä¿å­˜æ¶ˆæ¯åˆ°æ•°æ®åº“
    
    Args:
        messages: æ¶ˆæ¯å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«æ¶ˆæ¯çš„æ‰€æœ‰å¿…è¦å­—æ®µ
    
    Returns:
        int: æˆåŠŸä¿å­˜çš„æ¶ˆæ¯æ•°é‡
    """
    if not messages:
        return 0
        
    try:
        # ä½¿ç”¨æ•°æ®åº“é€‚é…å™¨
        from src.database.db_factory import get_db_adapter
        db_adapter = get_db_adapter()
        
        # ä½¿ç”¨é€‚é…å™¨æ‰¹é‡ä¿å­˜æ¶ˆæ¯
        successful = 0
        for msg in messages:
            # åˆå§‹æ£€æŸ¥ï¼Œç¡®ä¿å¿…é¡»çš„å­—æ®µå­˜åœ¨
            if not all(key in msg for key in ['chain', 'message_id', 'date']):
                logger.warning(f"æ¶ˆæ¯ç¼ºå°‘å¿…è¦å­—æ®µ: {msg}")
                continue
                
            # å‡†å¤‡æ¶ˆæ¯æ•°æ®
            message_data = {
                'chain': msg['chain'],
                'message_id': msg['message_id'],
                'date': msg['date'].isoformat() if isinstance(msg['date'], datetime) else msg['date'],
                'text': msg.get('text'),
                'media_path': msg.get('media_path'),
                'channel_id': msg.get('channel_id')
            }
            
            # ä¿å­˜æ¶ˆæ¯
            result = await db_adapter.save_message(message_data)
            if result:
                successful += 1
                
        # è¿”å›æˆåŠŸæ·»åŠ çš„æ•°é‡
        return successful
    except Exception as e:
        logger.error(f"æ‰¹é‡ä¿å­˜æ¶ˆæ¯å¤±è´¥: {str(e)}")
        import traceback
        logger.debug(traceback.format_exc())
        return 0

async def save_messages_individually(messages: List[Dict]):
    """å½“æ‰¹é‡ä¿å­˜å¤±è´¥æ—¶ï¼Œå°è¯•é€ä¸ªä¿å­˜æ¶ˆæ¯
    
    Args:
        messages: æ¶ˆæ¯æ•°æ®åˆ—è¡¨
    """
    successful = 0
    for msg_data in messages:
        try:
            # ä½¿ç”¨å·²æœ‰çš„ä¿å­˜å‡½æ•°
            if save_telegram_message(
                chain=msg_data['chain'],
                message_id=msg_data['message_id'],
                date=msg_data['date'],
                text=msg_data['text'],
                media_path=msg_data.get('media_path'),
                channel_id=msg_data.get('channel_id')
            ):
                successful += 1
        except Exception as individual_error:
            logger.error(f"å•ç‹¬ä¿å­˜æ¶ˆæ¯ {msg_data['message_id']} æ—¶å‡ºé”™: {individual_error}")
    
    logger.info(f"é€ä¸ªä¿å­˜: æˆåŠŸ {successful}/{len(messages)} æ¡æ¶ˆæ¯")

def save_telegram_message(
    chain: str,
    message_id: int,
    date: datetime,
    text: str,
    media_path: Optional[str] = None,
    channel_id: Optional[int] = None
) -> bool:
    """ä¿å­˜Telegramæ¶ˆæ¯åˆ°æ•°æ®åº“
    
    Args:
        chain: åŒºå—é“¾åç§°
        message_id: æ¶ˆæ¯ID
        date: æ¶ˆæ¯æ—¥æœŸ
        text: æ¶ˆæ¯æ–‡æœ¬
        media_path: åª’ä½“æ–‡ä»¶è·¯å¾„
        channel_id: é¢‘é“æˆ–ç¾¤ç»„ID
        
    Returns:
        bool: æ“ä½œæ˜¯å¦æˆåŠŸ
    """
    # å¦‚æœå¤§æ‰¹é‡å¤„ç†é˜Ÿåˆ—å·²å¼€å¯ï¼Œå°†æ¶ˆæ¯æ·»åŠ åˆ°é˜Ÿåˆ—
    global message_batch
    if MAX_BATCH_SIZE > 0:
        message_batch.append({
            'chain': chain,
            'message_id': message_id,
            'date': date,
            'text': text,
            'media_path': media_path,
            'channel_id': channel_id
        })
        # å¦‚æœé˜Ÿåˆ—è¾¾åˆ°æœ€å¤§å€¼ï¼Œç«‹å³å¤„ç†
        if len(message_batch) >= MAX_BATCH_SIZE:
            asyncio.create_task(process_message_batch())
        return True
    
    try:
        # ä½¿ç”¨æ•°æ®åº“é€‚é…å™¨
        from src.database.db_factory import get_db_adapter
        db_adapter = get_db_adapter()
        
        # å‡†å¤‡æ¶ˆæ¯æ•°æ®
        message_data = {
            'chain': chain,
            'message_id': message_id,
            'date': date.isoformat() if isinstance(date, datetime) else date,
            'text': text,
            'media_path': media_path,
            'channel_id': channel_id
        }
        
        # ä½¿ç”¨å¼‚æ­¥æ–¹å¼ä¿å­˜æ¶ˆæ¯
        # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯æ‰§è¡Œå¼‚æ­¥æ“ä½œ
        loop = asyncio.new_event_loop()
        try:
            result = loop.run_until_complete(db_adapter.save_message(message_data))
            return result
        finally:
            loop.close()
    except Exception as e:
        logger.error(f"ä¿å­˜æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}")
        import traceback
        logger.debug(traceback.format_exc())
        return False

async def process_message_batch():
    """å¤„ç†æ¶ˆæ¯æ‰¹å¤„ç†é˜Ÿåˆ—"""
    global message_batch
    
    if not message_batch:
        return
        
    # å¤åˆ¶å½“å‰é˜Ÿåˆ—ï¼Œå¹¶æ¸…ç©ºå…¨å±€é˜Ÿåˆ—
    current_batch = message_batch.copy()
    message_batch = []
    
    logger.info(f"å¤„ç†æ¶ˆæ¯æ‰¹å¤„ç†é˜Ÿåˆ—ï¼Œå…± {len(current_batch)} æ¡æ¶ˆæ¯")
    
    try:
        saved_count = await save_messages_batch(current_batch)
        logger.info(f"æˆåŠŸæ‰¹é‡ä¿å­˜ {saved_count}/{len(current_batch)} æ¡æ¶ˆæ¯")
        
        # å¦‚æœæ‰¹é‡ä¿å­˜å¤±è´¥ï¼Œå°è¯•é€ä¸ªä¿å­˜
        if saved_count < len(current_batch):
            logger.warning("æ‰¹é‡ä¿å­˜éƒ¨åˆ†å¤±è´¥ï¼Œå°è¯•é€ä¸ªä¿å­˜å‰©ä½™æ¶ˆæ¯")
            await save_messages_individually(current_batch)
    except Exception as e:
        logger.error(f"å¤„ç†æ¶ˆæ¯æ‰¹å¤„ç†æ—¶å‡ºé”™: {str(e)}")
        # å‡ºé”™æ—¶å°è¯•é€ä¸ªä¿å­˜
        try:
            await save_messages_individually(current_batch)
        except Exception as e2:
            logger.error(f"é€ä¸ªä¿å­˜æ¶ˆæ¯æ—¶ä¹Ÿå‡ºé”™: {str(e2)}")
            import traceback
            logger.debug(traceback.format_exc())

def save_tokens_batch(tokens: List[Dict]):
    """æ‰¹é‡ä¿å­˜ä»£å¸ä¿¡æ¯åˆ°æ•°æ®åº“
    
    è­¦å‘Šï¼šæ­¤å‡½æ•°å·²è¢«åºŸå¼ƒï¼Œä¸åº”å†ä½¿ç”¨ã€‚
    æ­¤å‡½æ•°åŸæœ¬ç”¨äºæ‰¹é‡å¤„ç†ä»£å¸ä¿¡æ¯ï¼Œä½†ç°åœ¨å·²ç”±process_batcheså‡½æ•°æ›¿ä»£ã€‚
    åœ¨ä»£ç ä¸­ç›´æ¥ä½¿ç”¨process_batcheså¤„ç†token_batchå˜é‡ï¼Œè€Œä¸è¦è°ƒç”¨æ­¤å‡½æ•°ã€‚
    
    ä»…ä¸ºäº†ä¿æŒå‘åå…¼å®¹æ€§è€Œä¿ç•™ã€‚å°†åœ¨ä¸‹ä¸€æ¬¡ä¸»è¦ç‰ˆæœ¬æ›´æ–°ä¸­å½»åº•ç§»é™¤ã€‚
    
    Args:
        tokens: ä»£å¸ä¿¡æ¯åˆ—è¡¨
        
    Returns:
        æ›´æ–°çš„ä»£å¸æ•°é‡
    """
    logger.warning("è°ƒç”¨äº†åºŸå¼ƒçš„save_tokens_batchå‡½æ•°ï¼Œè¯·æ”¹ç”¨process_batcheså¤„ç†token_batch")
    
    if not tokens:
        return 0
    
    # ä½¿ç”¨æ•°æ®åº“é€‚é…å™¨
    from src.database.db_factory import get_db_adapter
    db_adapter = get_db_adapter()
    
    # ä½¿ç”¨é‡è¯•æœºåˆ¶
    for attempt in range(OPERATION_RETRIES):
        try:
            # å¤„ç†æ¯ä¸ªä»£å¸ä¿¡æ¯
            updated_count = 0
            for token_data in tokens:
                token_symbol = token_data.get('token_symbol')
                chain = token_data.get('chain')
                contract = token_data.get('contract')
                
                if not token_symbol or not chain:
                    logger.warning(f"è·³è¿‡æ— æ•ˆçš„ä»£å¸æ•°æ®: ç¼ºå°‘token_symbolæˆ–chain")
                    continue
                
                # æ ‡å‡†åŒ–é£é™©ç­‰çº§å€¼
                if 'risk_level' in token_data:
                    risk_level = token_data['risk_level']
                    # ç¡®ä¿é£é™©ç­‰çº§æ˜¯æœ‰æ•ˆå€¼
                    if risk_level not in ['low', 'medium', 'high', 'medium-high', 'low-medium', 'unknown']:
                        # å¤„ç†ä¸­æ–‡é£é™©ç­‰çº§ï¼Œç»Ÿä¸€è½¬ä¸ºè‹±æ–‡
                        if risk_level == 'ä½':
                            token_data['risk_level'] = 'low'
                        elif risk_level == 'ä¸­':
                            token_data['risk_level'] = 'medium'
                        elif risk_level == 'é«˜':
                            token_data['risk_level'] = 'high'
                        elif not risk_level:
                            token_data['risk_level'] = 'unknown'
                
                # å¤„ç†æ—¥æœŸæ—¶é—´ç±»å‹
                for key, value in token_data.items():
                    if isinstance(value, datetime):
                        token_data[key] = value.isoformat()
                
                # ä½¿ç”¨äº‹ä»¶å¾ªç¯æ‰§è¡Œå¼‚æ­¥ä¿å­˜æ–¹æ³•
                loop = asyncio.new_event_loop()
                try:
                    result = loop.run_until_complete(db_adapter.save_token(token_data))
                    if result:
                        updated_count += 1
                        
                    # å¦‚æœæœ‰contractå­—æ®µï¼Œä¿å­˜token markä¿¡æ¯
                    if contract:
                        token_exists = loop.run_until_complete(db_adapter.get_token_by_contract(chain, contract))
                        if token_exists:
                            # ä¿å­˜ä»£å¸æ ‡è®°
                            loop.run_until_complete(db_adapter.save_token_mark(token_data))
                finally:
                    loop.close()
            
            logger.debug(f"æ›´æ–°/æ·»åŠ äº† {updated_count} æ¡ä»£å¸ä¿¡æ¯")
            return updated_count
            
        except Exception as e:
            logger.error(f"æ‰¹é‡ä¿å­˜ä»£å¸ä¿¡æ¯æ—¶å‡ºé”™(å°è¯• {attempt+1}/{OPERATION_RETRIES}): {str(e)}")
            import traceback
            logger.debug(traceback.format_exc())
            
            # å¦‚æœè¿˜æœ‰é‡è¯•æ¬¡æ•°ï¼Œç­‰å¾…åé‡è¯•
            if attempt < OPERATION_RETRIES - 1:
                time.sleep(OPERATION_RETRY_DELAY * (attempt + 1))  # æŒ‡æ•°é€€é¿
            else:
                logger.error(f"æ‰¹é‡ä¿å­˜ä»£å¸ä¿¡æ¯å¤±è´¥ï¼Œè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                return 0

def save_token_info(token_data: Dict[str, Any]) -> bool:
    """ä¿å­˜ä»£å¸ä¿¡æ¯
    
    Args:
        token_data: ä»£å¸æ•°æ®å­—å…¸
        
    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    # å¢å¼ºæ•°æ®éªŒè¯
    if not isinstance(token_data, dict):
        logger.error(f"token_dataå¿…é¡»æ˜¯å­—å…¸ï¼Œä½†æ”¶åˆ°äº†: {type(token_data)}")
        return False
        
    # éªŒè¯å¿…éœ€å­—æ®µ
    required_fields = ['chain', 'token_symbol']
    for field in required_fields:
        if field not in token_data or not token_data[field]:
            logger.error(f"ä¿å­˜ä»£å¸ä¿¡æ¯å¤±è´¥: ç¼ºå°‘å¿…éœ€å­—æ®µ '{field}'")
            return False
            
    # ç‰¹åˆ«éªŒè¯contractå­—æ®µ - è¿™æ˜¯ä¸€ä¸ªå…³é”®å­—æ®µ
    if 'contract' not in token_data or not token_data['contract']:
        logger.error(f"ä¿å­˜ä»£å¸ä¿¡æ¯å¤±è´¥: ç¼ºå°‘å¿…éœ€å­—æ®µ 'contract'ï¼ˆä¸èƒ½ä¸ºnullï¼‰")
        return False
        
    # å¸¸è§„éªŒè¯
    valid, message = validate_token_data(token_data)
    if not valid:
        logger.warning(f"ä»£å¸æ•°æ®éªŒè¯å¤±è´¥: {message}")
        return False
        
    # ä½¿ç”¨æ•°æ®åº“é€‚é…å™¨
    from src.database.db_factory import get_db_adapter
    db_adapter = get_db_adapter()
    
    try:
        # å¤„ç†æ—¥æœŸæ—¶é—´ç±»å‹
        for key, value in token_data.items():
            if isinstance(value, datetime):
                token_data[key] = value.isoformat()
        
        # ä½¿ç”¨äº‹ä»¶å¾ªç¯æ‰§è¡Œå¼‚æ­¥ä¿å­˜æ–¹æ³•
        loop = asyncio.new_event_loop()
        try:
            result = loop.run_until_complete(db_adapter.save_token(token_data))
            return result
        finally:
            loop.close()
    except Exception as e:
        logger.error(f"ä¿å­˜ä»£å¸ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        logger.debug(f"é—®é¢˜æ•°æ®: {token_data}")
        import traceback
        logger.debug(traceback.format_exc())
        return False

def extract_promotion_info(message_text: str, date: datetime, chain: str = None, message_id: int = None, channel_id: int = None) -> Optional[PromotionInfo]:
    """ä»æ¶ˆæ¯æ–‡æœ¬ä¸­æå–åˆçº¦åœ°å€ä¿¡æ¯
    
    ä¸“æ³¨äºæå–åˆçº¦åœ°å€ï¼Œå…¶ä»–ä¿¡æ¯é€šè¿‡DEX APIè·å–
    
    Args:
        message_text: æ¶ˆæ¯æ–‡æœ¬
        date: æ¶ˆæ¯æ—¥æœŸ
        chain: å¯é€‰çš„é“¾åç§°
        message_id: æ¶ˆæ¯ID
        channel_id: é¢‘é“ID
        
    Returns:
        PromotionInfo: åŒ…å«åˆçº¦åœ°å€å’Œé“¾ä¿¡æ¯çš„æ•°æ®å¯¹è±¡ï¼Œå¦‚æœæœªæå–åˆ°åˆ™è¿”å›None
    """
    
    try:
        # æ¸…ç†æ¶ˆæ¯æ–‡æœ¬
        cleaned_text = re.sub(r'\s+', ' ', message_text)
        cleaned_text = re.sub(r'[\u200b\u200c\u200d\ufeff]', '', cleaned_text)  # ç§»é™¤é›¶å®½å­—ç¬¦
        
        # å¦‚æœæœªæä¾›é“¾ä¿¡æ¯ï¼Œå°è¯•ä»æ¶ˆæ¯ä¸­æå–
        if not chain or chain == "UNKNOWN":
            # å…ˆå°è¯•æ£€æµ‹å¸‚å€¼å•ä½ï¼Œè¿™æ˜¯æœ€å¯é çš„é“¾æ ‡è¯†
            mc_pattern = re.search(r'(\bmc\b|\bmarket\s*cap\b|å¸‚å€¼)[ï¼š:]*\s*[`\'"]*\d+(?:\.\d+)?\s*(?:bnb|BNB)', cleaned_text, re.IGNORECASE)
            if mc_pattern:
                logger.info(f"ä»å¸‚å€¼å•ä½(BNB)åˆ¤æ–­ä¸ºBSCé“¾")
                chain = 'BSC'
            else:
                # ä¸æ˜¯BSCï¼Œå°è¯•å…¶ä»–é“¾çš„æå–
                chain_from_message = extract_chain_from_message(message_text)
                if chain_from_message:
                    logger.info(f"ä»æ¶ˆæ¯ä¸­æå–åˆ°é“¾ä¿¡æ¯: {chain_from_message}")
                    chain = chain_from_message
        
        # æå–ä»£å¸ç¬¦å·
        # å¯»æ‰¾å¸¸è§æ ¼å¼çš„ä»£å¸ç¬¦å·ï¼Œå¦‚$XYZ
        token_symbol = None
        symbol_match = re.search(r'\$([A-Za-z0-9_]{1,20})\b', cleaned_text)
        if symbol_match:
            token_symbol = symbol_match.group(1).upper()
            logger.info(f"ä»æ¶ˆæ¯ä¸­æå–åˆ°ä»£å¸ç¬¦å·: {token_symbol}")
        
        # ä¸“æ³¨äºæå–åˆçº¦åœ°å€
        contract_address = None
        
        # ä½¿ç”¨å¢å¼ºçš„åˆçº¦åœ°å€æå–æ¨¡å¼
        contract_patterns = [
            # å¸¦æ ‡è®°çš„åˆçº¦åœ°å€
            r'(?:ğŸ“|åˆçº¦[ï¼š:]|[Cc]ontract[ï¼š:])[ ]*([0-9a-fA-FxX]{8,})',
            r'åˆçº¦åœ°å€[ï¼š:][ ]*([0-9a-fA-FxX]{8,})',
            r'åœ°å€[ï¼š:][ ]*([0-9a-fA-FxX]{8,})',
            # æ ‡å‡†ä»¥å¤ªåŠåœ°å€æ ¼å¼
            r'\b(0x[0-9a-fA-F]{40})\b',
            # å…¶ä»–å¯èƒ½çš„åˆçº¦åœ°å€æ ¼å¼
            r'\b([a-zA-Z0-9]{32,50})\b'
        ]
        
        # å°è¯•æ‰€æœ‰æ¨¡å¼æå–åˆçº¦åœ°å€
        for pattern in contract_patterns:
            match = re.search(pattern, cleaned_text)
            if match:
                potential_address = match.group(1) if '(' in pattern else match.group(0)
                logger.info(f"ä»æ¶ˆæ¯ä¸­æå–åˆ°æ½œåœ¨åˆçº¦åœ°å€: {potential_address}")
                
                # éªŒè¯åœ°å€æ ¼å¼
                if re.match(r'^0x[a-fA-F0-9]{40}$', potential_address):
                    contract_address = potential_address
                    if not chain or chain == "UNKNOWN":
                        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„é“¾æŒ‡ç¤ºå™¨
                        if re.search(r'(\bmc\b|\bmarket\s*cap\b|å¸‚å€¼)[ï¼š:]*\s*[`\'"]*\d+(?:\.\d+)?\s*(?:bnb|BNB)', cleaned_text, re.IGNORECASE):
                            logger.info("ä»å¸‚å€¼å•ä½(BNB)åˆ¤æ–­ä¸ºBSCé“¾")
                            chain = 'BSC'
                        elif re.search(r'(\bmc\b|\bmarket\s*cap\b|å¸‚å€¼)[ï¼š:]*\s*[`\'"]*\d+(?:\.\d+)?\s*(?:eth|ETH)', cleaned_text, re.IGNORECASE):
                            logger.info("ä»å¸‚å€¼å•ä½(ETH)åˆ¤æ–­ä¸ºETHé“¾")
                            chain = 'ETH'
                        elif 'bsc' in cleaned_text.lower() or 'bnb' in cleaned_text.lower() or 'pancake' in cleaned_text.lower() or 'binance' in cleaned_text.lower():
                            logger.info("ä»ä¸Šä¸‹æ–‡å…³é”®è¯åˆ¤æ–­ä¸ºBSCé“¾")
                            chain = 'BSC'
                        elif 'eth' in cleaned_text.lower() or 'ethereum' in cleaned_text.lower() or 'uniswap' in cleaned_text.lower():
                            logger.info("ä»ä¸Šä¸‹æ–‡å…³é”®è¯åˆ¤æ–­ä¸ºETHé“¾")
                            chain = 'ETH'
                        else:
                            logger.info("æ£€æµ‹åˆ°EVMç±»åœ°å€ï¼Œä½†æ— æ³•ç¡®å®šå…·ä½“é“¾ï¼Œå°è¯•é€šè¿‡DEX APIç¡®å®šå…·ä½“é“¾")
                    break
                elif re.match(r'^[1-9A-HJ-NP-Za-km-z]{32,44}$', potential_address):
                    contract_address = potential_address
                    if not chain or chain == "UNKNOWN":
                        logger.info("æ£€æµ‹åˆ°ç±»ä¼¼SOLçš„åœ°å€ï¼Œè®¾ç½®é“¾ä¸ºSOL")
                        chain = "SOL"
                    break
                elif potential_address.startswith('0x'):
                    # å°è¯•ä¿®æ­£ä¸å®Œæ•´çš„EVMåœ°å€
                    full_address = re.search(r'0x[0-9a-fA-F]{40}', cleaned_text)
                    if full_address:
                        contract_address = full_address.group(0)
                        break
        
        # å¦‚æœæœªæ‰¾åˆ°åˆçº¦åœ°å€ï¼Œå°è¯•ä»URLä¸­æå–
        if not contract_address:
            urls = re.findall(r'https?://[^\s<>"]+|www\.[^\s<>"]+', cleaned_text)
            for url in urls:
                url_clean = url.strip()
                # å¤„ç†URLæœ«å°¾å¯èƒ½çš„æ ‡ç‚¹ç¬¦å·
                for marker in [' ', '\n', '\t', ',', ')', ']', '}', '"', "'", 'ã€‚', 'ï¼Œ', 'ï¼š', 'ï¼›']:
                    if marker in url_clean:
                        url_clean = url_clean.split(marker)[0]
                
                contract_from_url, chain_from_url = extract_contract_from_url(url_clean)
                if contract_from_url:
                    contract_address = contract_from_url
                    if chain_from_url and (not chain or chain == "UNKNOWN"):
                        chain = chain_from_url
                    logger.info(f"ä»URLæå–åˆ°åˆçº¦åœ°å€: {contract_address}, é“¾: {chain}")
                    break
        
        # å¦‚æœæ‰¾åˆ°äº†åˆçº¦åœ°å€ï¼Œåˆ›å»ºå¹¶è¿”å›PromotionInfoå¯¹è±¡
        if contract_address:
            # å¦‚æœåœ¨æ­¤é˜¶æ®µä»ç„¶æ²¡æœ‰ç¡®å®šé“¾ï¼Œå¹¶ä¸”æ˜¯EVMåœ°å€ï¼Œå†å°è¯•ä¸€æ¬¡ä»æ¶ˆæ¯ä¸Šä¸‹æ–‡æ¨æ–­
            if (not chain or chain == "UNKNOWN") and contract_address.startswith('0x'):
                chain_from_context = extract_chain_from_message(message_text)
                if chain_from_context:
                    chain = chain_from_context
                    logger.info(f"ä»ä¸Šä¸‹æ–‡æ¨æ–­åˆçº¦åœ°å€ {contract_address} æ‰€åœ¨é“¾ä¸º: {chain}")
                else:
                    # ä»ç„¶æ— æ³•ç¡®å®šé“¾ï¼Œåˆ†æä¸Šä¸‹æ–‡ä¸­æ˜¯å¦æœ‰æ˜ç¡®çš„BSC/ETHå…³é”®è¯
                    text_lower = cleaned_text.lower()
                    if 'bsc' in text_lower or 'bnb' in text_lower or 'pancake' in text_lower or 'binance' in text_lower:
                        chain = 'BSC'
                        logger.info(f"ä»å…³é”®è¯åˆ¤æ–­åˆçº¦åœ°å€ {contract_address} æ‰€åœ¨é“¾ä¸ºBSC")
                    elif 'eth' in text_lower or 'ethereum' in text_lower or 'uniswap' in text_lower:
                        chain = 'ETH'
                        logger.info(f"ä»å…³é”®è¯åˆ¤æ–­åˆçº¦åœ°å€ {contract_address} æ‰€åœ¨é“¾ä¸ºETH")
                    else:
                        logger.warning(f"æ— æ³•ç¡®å®šåˆçº¦åœ°å€ {contract_address} æ‰€åœ¨çš„é“¾ï¼Œè®¾ç½®ä¸ºUNKNOWN")
                        chain = "UNKNOWN"
            
            logger.info(f"æˆåŠŸæå–åˆçº¦åœ°å€: {contract_address}, é“¾: {chain}")
            
            # åˆ›å»ºæ¨å¹¿ä¿¡æ¯å¯¹è±¡
            info = PromotionInfo(
                token_symbol=token_symbol,
                contract_address=contract_address,
                chain=chain,
                promotion_count=1,  # é»˜è®¤ä¸º1ï¼Œè¡¨ç¤ºé¦–æ¬¡è§åˆ°
                first_trending_time=date
            )
            
            # æ·»åŠ æ–°çš„å¿…è¦å­—æ®µ
            info.message_id = message_id
            info.channel_id = channel_id
            
            # å°è¯•ä»æ¶ˆæ¯ä¸­æå–é£é™©è¯„çº§
            risk_level = None
            risk_patterns = [
                r'[Rr]isk[ï¼š:]\s*([A-Za-z]+)',
                r'é£é™©[ï¼š:]\s*([A-Za-zé«˜ä¸­ä½]+)',
                r'[Ss]afe[ï¼š:]\s*([A-Za-z]+)',
                r'å®‰å…¨[ï¼š:]\s*([A-Za-zæ˜¯å¦]+)'
            ]
            
            for pattern in risk_patterns:
                match = re.search(pattern, cleaned_text)
                if match:
                    risk_text = match.group(1).strip().lower()
                    if risk_text in ['high', 'é«˜', 'high risk']:
                        risk_level = 'high'
                    elif risk_text in ['medium', 'mid', 'moderate', 'ä¸­']:
                        risk_level = 'medium'
                    elif risk_text in ['low', 'ä½', 'safe', 'yes', 'æ˜¯']:
                        risk_level = 'low'
                    break
                    
            info.risk_level = risk_level
            
            # å°è¯•ä»æ¶ˆæ¯ä¸­æå–å¸‚å€¼ä¿¡æ¯
            from src.utils.utils import parse_market_cap
            market_cap_text = re.search(r'([Mm]arket\s*[Cc]ap|å¸‚å€¼|[Mm][Cc])[ï¼š:]\s*[`\'"]?([^,\n]+)', cleaned_text)
            if market_cap_text:
                mc_value = market_cap_text.group(2).strip()
                try:
                    parsed_mc = parse_market_cap(mc_value)
                    if parsed_mc:
                        info.market_cap = str(parsed_mc)
                        # ç¬¬ä¸€æ¬¡è§åˆ°çš„å¸‚å€¼å°±æ˜¯first_market_cap
                        info.first_market_cap = parsed_mc
                        
                        # ä»å¸‚å€¼å•ä½åˆ¤æ–­é“¾
                        mc_lower = mc_value.lower()
                        if 'bnb' in mc_lower and (not chain or chain == "UNKNOWN"):
                            info.chain = 'BSC'
                            logger.info("ä»å¸‚å€¼å•ä½(BNB)ä¿®æ­£é“¾ä¿¡æ¯ä¸ºBSC")
                        elif 'eth' in mc_lower and (not chain or chain == "UNKNOWN"):
                            info.chain = 'ETH'
                            logger.info("ä»å¸‚å€¼å•ä½(ETH)ä¿®æ­£é“¾ä¿¡æ¯ä¸ºETH")
                        elif 'sol' in mc_lower and (not chain or chain == "UNKNOWN"):
                            info.chain = 'SOL'
                            logger.info("ä»å¸‚å€¼å•ä½(SOL)ä¿®æ­£é“¾ä¿¡æ¯ä¸ºSOL")
                except:
                    pass
            
            return info
            
        logger.info("æœªèƒ½ä»æ¶ˆæ¯ä¸­æå–åˆ°åˆçº¦åœ°å€")
        return None
            
    except Exception as e:
        logger.error(f"æå–ä»£å¸ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        logger.debug(traceback.format_exc())
        return None

def extract_chain_from_message(message_text: str) -> Optional[str]:
    """ä»æ¶ˆæ¯æ–‡æœ¬ä¸­æå–åŒºå—é“¾ä¿¡æ¯
    
    Args:
        message_text: éœ€è¦è§£æçš„æ¶ˆæ¯æ–‡æœ¬
        
    Returns:
        str: æå–åˆ°çš„é“¾åç§°ï¼Œæœªæ‰¾åˆ°åˆ™è¿”å›None
    """
    # æ¸…ç†æ¶ˆæ¯æ–‡æœ¬ï¼Œä¾¿äºåŒ¹é…
    text = message_text.lower()
    
    # å®šä¹‰ä¸åŒé“¾çš„å…³é”®è¯åŒ¹é…è§„åˆ™
    chain_patterns = {
        'SOL': [r'\bsol\b', r'\bsolana\b', r'@solana', r'solanas', r'ì†”ë¼ë‚˜', r'ç´¢æ‹‰çº³', 
                r'solscan\.io', r'explorer\.solana\.com', r'solana_trojanbot', r'solé“¾'],
        'BSC': [r'\bbsc\b', r'\bbinance smart chain\b', r'\bbnb\b', r'\bbnb chain\b', r'å¸å®‰é“¾', r'ë°”ì´ë‚¸ìŠ¤', 
                r'bscscan\.com', r'pancakeswap', r'poocoin', r'bscé“¾', r'\bbnb:'],
        'ETH': [r'\beth\b', r'\bethereum\b', r'@ethereum', r'ä»¥å¤ªåŠ', r'ì´ë”ë¦¬ì›€', 
                r'etherscan\.io', r'uniswap', r'sushiswap', r'ethé“¾', r'\beth:'],
        'ARB': [r'\barb\b', r'\barbitrum\b', r'arbitrums', r'é˜¿æ¯”ç‰¹é¾™', r'ì•„ë¹„íŠ¸ëŸ¼', 
                r'arbiscan\.io', r'arbé“¾'],
        'BASE': [r'\bbase\b', r'basechain', r'coinbase', r'è´æ–¯é“¾', r'ë² ì´ìŠ¤', 
                 r'basescan\.org', r'baseé“¾'],
        'AVAX': [r'\bavax\b', r'\bavalanche\b', r'é›ªå´©é“¾', r'ì•„ë°œë€ì²´', 
                 r'snowtrace\.io', r'traderjoe', r'avaxé“¾'],
        'MATIC': [r'\bmatic\b', r'\bpolygon\b', r'æ³¢åˆ©å†ˆ', r'í´ë¦¬ê³¤', 
                  r'polygonscan\.com', r'maticé“¾'],
        'OP': [r'\boptimism\b', r'\bop\b', r'ä¹è§‚é“¾', r'ì˜µí‹°ë¯¸ì¦˜', 
               r'optimistic\.etherscan\.io', r'opé“¾']
    }
    
    # ä¼˜å…ˆæ£€æŸ¥æ˜¯å¦æ˜ç¡®æåˆ°å¸‚å€¼å•ä½ä¸ºBNBï¼Œè¿™æ˜¯BSCé“¾çš„æœ€æ˜ç¡®æ ‡å¿—
    if re.search(r'(\bmc\b|\bmarket\s*cap\b|å¸‚å€¼)[ï¼š:]*\s*[`\'"]*\d+(?:\.\d+)?\s*(?:bnb|BNB)', text, re.IGNORECASE):
        logger.info("ä»å¸‚å€¼å•ä½(BNB)åˆ¤æ–­ä¸ºBSCé“¾")
        return 'BSC'
    
    # æå–dexscreener URLå¹¶è§£æï¼Œè¿™æ˜¯æ¯”åŒ¹é…ç®€å•å…³é”®è¯æ›´æ˜ç¡®çš„ä¿¡æ¯
    # å¤„ç†æ ¼å¼: dexscreener.com/solana/xxx æˆ– dexscreener.com/ethereum/xxxç­‰
    dexscreener_match = re.search(r'(?:https?://)?(?:www\.)?dexscreener\.com/([a-zA-Z0-9]+)(?:/[^/\s]+)?', text)
    if dexscreener_match:
        chain_str = dexscreener_match.group(1).upper()
        # æ˜ å°„DEX Screener URLè·¯å¾„åˆ°é“¾æ ‡è¯†
        dexscreener_map = {
            'SOLANA': 'SOL',
            'ETHEREUM': 'ETH',
            'BSC': 'BSC',
            'ARBITRUM': 'ARB',
            'BASE': 'BASE',
            'AVALANCHE': 'AVAX',
            'POLYGON': 'MATIC',
            'OPTIMISM': 'OP'
        }
        if chain_str in dexscreener_map:
            logger.info(f"ä»DEX Screener URLæå–åˆ°é“¾ä¿¡æ¯: {dexscreener_map[chain_str]}")
            return dexscreener_map[chain_str]
    
    # å¤„ç†æ›´å¤æ‚çš„dexscreener URLæ ¼å¼ï¼Œä¾‹å¦‚å®Œæ•´çš„äº¤æ˜“å¯¹åœ°å€URL
    complex_dexscreener = re.search(r'dexscreener\.com/([a-zA-Z0-9]+)/[a-zA-Z0-9]{10,}', text, re.IGNORECASE)
    if complex_dexscreener:
        chain_str = complex_dexscreener.group(1).upper()
        dexscreener_map = {
            'SOLANA': 'SOL',
            'ETHEREUM': 'ETH',
            'BSC': 'BSC',
            'ARBITRUM': 'ARB',
            'BASE': 'BASE',
            'AVALANCHE': 'AVAX',
            'POLYGON': 'MATIC',
            'OPTIMISM': 'OP'
        }
        if chain_str in dexscreener_map:
            logger.info(f"ä»å¤æ‚çš„DEX Screener URLæå–åˆ°é“¾ä¿¡æ¯: {dexscreener_map[chain_str]}")
            return dexscreener_map[chain_str]
    
    # æ£€æŸ¥åŒºå—æµè§ˆå™¨é“¾æ¥ï¼Œè¿™ä¹Ÿæ˜¯å¼ºæœ‰åŠ›çš„è¯æ®
    explorer_patterns = {
        'SOL': [r'solscan\.io', r'explorer\.solana\.com'],
        'ETH': [r'etherscan\.io'],
        'BSC': [r'bscscan\.com'],
        'ARB': [r'arbiscan\.io'],
        'BASE': [r'basescan\.org'],
        'AVAX': [r'snowtrace\.io'],
        'MATIC': [r'polygonscan\.com'],
        'OP': [r'optimistic\.etherscan\.io']
    }
    
    for chain, patterns in explorer_patterns.items():
        for pattern in patterns:
            if re.search(pattern, text):
                logger.info(f"ä»åŒºå—æµè§ˆå™¨URLæå–åˆ°é“¾ä¿¡æ¯: {chain}, åŒ¹é…æ¨¡å¼: {pattern}")
                return chain
    
    # æ£€æŸ¥ç‰¹å®šçš„DEXå…³é”®è¯
    dex_patterns = {
        'SOL': [r'raydium', r'orca\.so', r'jupiter'],
        'ETH': [r'uniswap', r'sushiswap'],
        'BSC': [r'pancakeswap', r'poocoin']
    }
    
    for chain, patterns in dex_patterns.items():
        for pattern in patterns:
            if re.search(pattern, text):
                logger.info(f"ä»DEXå…³é”®è¯æå–åˆ°é“¾ä¿¡æ¯: {chain}, åŒ¹é…æ¨¡å¼: {pattern}")
                return chain
    
    # æœ€åå†æ£€æŸ¥ä¸€èˆ¬å…³é”®è¯åŒ¹é…ï¼Œè¿™ä¸ªä¼˜å…ˆçº§è¾ƒä½
    for chain, patterns in chain_patterns.items():
        for pattern in patterns:
            if re.search(pattern, text):
                logger.info(f"ä»å…³é”®è¯åŒ¹é…æå–åˆ°é“¾ä¿¡æ¯: {chain}, åŒ¹é…æ¨¡å¼: {pattern}")
                return chain
    
    # å¤„ç†ä¸­æ–‡ç¯å¢ƒ
    chinese_chains = {
        'SOL': ['solana', 'sol', 'ç´¢æ‹‰çº³', 'ç´¢å…°çº³'],
        'ETH': ['ethereum', 'eth', 'ä»¥å¤ªåŠ', 'ä»¥å¤ª'],
        'BSC': ['binance', 'bsc', 'bnb', 'å¸å®‰'],
        'AVAX': ['avalanche', 'avax', 'é›ªå´©'],
        'MATIC': ['polygon', 'matic', 'æ³¢åˆ©å†ˆ']
    }
    
    for chain, keywords in chinese_chains.items():
        for keyword in keywords:
            if keyword in text:
                logger.info(f"ä»ä¸­æ–‡ç¯å¢ƒæå–åˆ°é“¾ä¿¡æ¯: {chain}, å…³é”®è¯: {keyword}")
                return chain
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹å®šçš„æœºå™¨äººå¼•ç”¨
    bot_patterns = {
        'SOL': [r'solana_trojanbot'],
        'BSC': [r'ape\.bot', r'sigma_buybot.*bsc', r'pancakeswap_bot'],
        'ETH': [r'uniswap_bot', r'sigma_buybot.*eth']
    }
    
    for chain, patterns in bot_patterns.items():
        for pattern in patterns:
            if re.search(pattern, text):
                logger.info(f"ä»æœºå™¨äººå¼•ç”¨æå–åˆ°é“¾ä¿¡æ¯: {chain}, åŒ¹é…æ¨¡å¼: {pattern}")
                return chain
    
    # å°è¯•ä»MCï¼ˆå¸‚å€¼ï¼‰å•ä½åˆ¤æ–­é“¾
    mc_patterns = {
        'ETH': [r'(\bmc\b|\bmarket\s*cap\b|å¸‚å€¼)[ï¼š:]*\s*[`\'"]*\d+(?:\.\d+)?\s*(?:eth|ETH)'],
        'BSC': [r'(\bmc\b|\bmarket\s*cap\b|å¸‚å€¼)[ï¼š:]*\s*[`\'"]*\d+(?:\.\d+)?\s*(?:bnb|BNB)'],
        'SOL': [r'(\bmc\b|\bmarket\s*cap\b|å¸‚å€¼)[ï¼š:]*\s*[`\'"]*\d+(?:\.\d+)?\s*(?:sol|SOL)']
    }
    
    for chain, patterns in mc_patterns.items():
        for pattern in patterns:
            if re.search(pattern, text, re.IGNORECASE):
                logger.info(f"ä»å¸‚å€¼å•ä½æå–åˆ°é“¾ä¿¡æ¯: {chain}, åŒ¹é…æ¨¡å¼: {pattern}")
                return chain
    
    # æœ€åæ‰ä»åˆçº¦åœ°å€æ ¼å¼æ¨æ–­ï¼Œä¸”éœ€è¦ç»“åˆå…¶ä»–ä¸Šä¸‹æ–‡ä¿¡æ¯
    if re.search(r'\b0x[0-9a-fA-F]{40}\b', text):
        # å°è¯•ä»å…¶ä»–ä¸Šä¸‹æ–‡åˆ¤æ–­å…·ä½“æ˜¯å“ªç§EVMé“¾
        if 'bnb' in text or 'bsc' in text or 'binance' in text or 'pancake' in text:
            logger.info("ä»åˆçº¦åœ°å€æ ¼å¼å’Œä¸Šä¸‹æ–‡ä¿¡æ¯(BSCå…³é”®è¯)æ¨æ–­ä¸ºBSCé“¾")
            return 'BSC'
        elif 'eth' in text or 'ethereum' in text or 'uniswap' in text:
            logger.info("ä»åˆçº¦åœ°å€æ ¼å¼å’Œä¸Šä¸‹æ–‡ä¿¡æ¯(ETHå…³é”®è¯)æ¨æ–­ä¸ºETHé“¾")
            return 'ETH'
        elif 'arb' in text or 'arbitrum' in text:
            logger.info("ä»åˆçº¦åœ°å€æ ¼å¼å’Œä¸Šä¸‹æ–‡ä¿¡æ¯(ARBå…³é”®è¯)æ¨æ–­ä¸ºARBé“¾")
            return 'ARB'
        elif 'matic' in text or 'polygon' in text:
            logger.info("ä»åˆçº¦åœ°å€æ ¼å¼å’Œä¸Šä¸‹æ–‡ä¿¡æ¯(MATICå…³é”®è¯)æ¨æ–­ä¸ºMATICé“¾")
            return 'MATIC'
        else:
            # ä¸å†é»˜è®¤è¿”å›ETHï¼Œè€Œæ˜¯è¿”å›Noneè¡¨ç¤ºæ— æ³•ç¡®å®š
            logger.warning("ä»åˆçº¦åœ°å€æ ¼å¼æ¨æ–­ä¸ºEVMé“¾ï¼Œä½†æ— æ³•ç¡®å®šå…·ä½“æ˜¯å“ªæ¡é“¾ï¼Œéœ€è¦æ›´å¤šä¸Šä¸‹æ–‡")
            return None
        
    if re.search(r'\b[1-9A-HJ-NP-Za-km-z]{32,44}\b', text) and ('sol' in text or 'solana' in text):
        # Solana Base58æ ¼å¼åœ°å€
        logger.info("ä»åˆçº¦åœ°å€æ ¼å¼å’ŒSOLå…³é”®è¯æ¨æ–­ä¸ºSOLé“¾")
        return 'SOL'
    
    # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›None
    logger.debug("æ— æ³•ä»æ¶ˆæ¯ä¸­æå–é“¾ä¿¡æ¯")
    return None

def extract_url_from_text(text: str, keyword: str = '') -> Optional[str]:
    """ä»æ–‡æœ¬ä¸­æå–URLé“¾æ¥
    
    Args:
        text: è¦å¤„ç†çš„æ–‡æœ¬
        keyword: å¯é€‰çš„å…³é”®è¯ï¼Œç”¨äºç­›é€‰URL
        
    Returns:
        æå–å‡ºçš„URLï¼Œæˆ–None
    """
    if not text:
        return None
    
    try:
        # å®šä¹‰URLæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        url_patterns = [
            r'https?://\S+',  # æ ‡å‡†HTTP/HTTPS URL
            r'www\.\S+',      # ä»¥wwwå¼€å¤´çš„URL
            r't\.me/\S+',     # Telegramé“¾æ¥
            r'twitter\.com/\S+'  # Twitteré“¾æ¥
        ]
        
        # åˆå¹¶æ‰€æœ‰æ¨¡å¼
        combined_pattern = '|'.join(url_patterns)
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„URL
        urls = re.findall(combined_pattern, text)
        
        if not urls:
            return None
            
        if keyword:
            # å¦‚æœæŒ‡å®šäº†å…³é”®è¯ï¼Œä¼˜å…ˆè¿”å›åŒ…å«å…³é”®è¯çš„URL
            for url in urls:
                if keyword.lower() in url.lower():
                    # å¤„ç†URLæœ«å°¾å¯èƒ½çš„æ ‡ç‚¹ç¬¦å·
                    markers = [' ', '\n', '\t', ',', ')', ']', '}', '"', "'", 'ã€‚', 'ï¼Œ', 'ï¼š', 'ï¼›']
                    url_part = url
                    
                    # æŸ¥æ‰¾æœ€æ—©å‡ºç°çš„æ ‡ç‚¹ç¬¦å·ä½ç½®
                    end_idx = len(url_part)
                    for marker in markers:
                        marker_idx = url_part.find(marker)
                        if marker_idx > 0 and marker_idx < end_idx:
                            end_idx = marker_idx
                    
                    url = url_part[:end_idx].strip()
                    return url
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šå…³é”®è¯æˆ–æ²¡æœ‰æ‰¾åˆ°åŒ…å«å…³é”®è¯çš„URLï¼Œè¿”å›ç¬¬ä¸€ä¸ªURL
        url_part = urls[0]
        # å¤„ç†URLæœ«å°¾å¯èƒ½çš„æ ‡ç‚¹ç¬¦å·
        markers = [' ', '\n', '\t', ',', ')', ']', '}', '"', "'", 'ã€‚', 'ï¼Œ', 'ï¼š', 'ï¼›']
        
        # æŸ¥æ‰¾æœ€æ—©å‡ºç°çš„æ ‡ç‚¹ç¬¦å·ä½ç½®
        end_idx = len(url_part)
        for marker in markers:
            marker_idx = url_part.find(marker)
            if marker_idx > 0 and marker_idx < end_idx:
                end_idx = marker_idx
        
        url = url_part[:end_idx].strip()
        return url

    except Exception as e:
        logger.error(f"ä»æ–‡æœ¬ä¸­æå–URLæ—¶å‡ºé”™: {str(e)}")
        return None
    
    return None

def extract_contract_from_url(url: str) -> Tuple[Optional[str], Optional[str]]:
    """ä»URLä¸­æå–åˆçº¦åœ°å€å’Œé“¾ä¿¡æ¯
    
    Args:
        url: ç½‘å€
        
    Returns:
        Tuple[Optional[str], Optional[str]]: åˆçº¦åœ°å€å’Œé“¾ä¿¡æ¯çš„å…ƒç»„ï¼Œæœªæ‰¾åˆ°åˆ™è¿”å›(None, None)
    """
    if not url:
        return None, None
    
    try:
        # å¤„ç†å„ç§å¸¸è§åŒºå—æµè§ˆå™¨å’ŒDEXçš„URL
        # DexScreeneræ ¼å¼
        # ä¾‹å¦‚: https://dexscreener.com/solana/8WJ2ngd7FpHVkWiQTNyJ3N9j1oDmjR5e6MFdDAKQNinF
        dexscreener_pattern = r'(?:https?://)?(?:www\.)?dexscreener\.com/([a-zA-Z0-9]+)/([a-zA-Z0-9]{20,})'
        match = re.search(dexscreener_pattern, url)
        if match:
            chain_str = match.group(1).lower()
            contract = match.group(2)
            
            # æ˜ å°„åˆ°å†…éƒ¨é“¾æ ‡è¯†
            dexscreener_map = {
                'solana': 'SOL',
                'ethereum': 'ETH',
                'bsc': 'BSC',
                'arbitrum': 'ARB',
                'base': 'BASE',
                'avalanche': 'AVAX',
                'polygon': 'MATIC',
                'optimism': 'OP'
            }
            
            chain = dexscreener_map.get(chain_str)
            logger.info(f"ä»DexScreener URLæå–åˆ°åˆçº¦åœ°å€: {contract}, é“¾: {chain}")
            return contract, chain
        
        # ç‰¹æ®Šæ¨¡å¼ï¼šå¸å®‰é“¾æµè§ˆå™¨
        # ä¾‹å¦‚: https://bscscan.com/token/0x123456789...
        bscscan_pattern = r'(?:https?://)?(?:www\.)?bscscan\.com/(?:token|address)/([a-zA-Z0-9]{20,})'
        match = re.search(bscscan_pattern, url)
        if match:
            contract = match.group(1)
            logger.info(f"ä»BSCScan URLæå–åˆ°åˆçº¦åœ°å€: {contract}, é“¾: BSC")
            return contract, 'BSC'
        
        # Etherscanæ ¼å¼
        # ä¾‹å¦‚: https://etherscan.io/token/0x123456789...
        etherscan_pattern = r'(?:https?://)?(?:www\.)?etherscan\.io/(?:token|address)/([a-zA-Z0-9]{20,})'
        match = re.search(etherscan_pattern, url)
        if match:
            contract = match.group(1)
            logger.info(f"ä»Etherscan URLæå–åˆ°åˆçº¦åœ°å€: {contract}, é“¾: ETH")
            return contract, 'ETH'
        
        # Solscanæ ¼å¼
        # ä¾‹å¦‚: https://solscan.io/token/8WJ2ngd7FpHVkWiQTNyJ3N9j1oDmjR5e6MFdDAKQNinF
        solscan_pattern = r'(?:https?://)?(?:www\.)?solscan\.io/(?:token|account)/([a-zA-Z0-9]{20,})'
        match = re.search(solscan_pattern, url)
        if match:
            contract = match.group(1)
            logger.info(f"ä»Solscan URLæå–åˆ°åˆçº¦åœ°å€: {contract}, é“¾: SOL")
            return contract, 'SOL'
        
        # Polygonscanæ ¼å¼
        polygonscan_pattern = r'(?:https?://)?(?:www\.)?polygonscan\.com/(?:token|address)/([a-zA-Z0-9]{20,})'
        match = re.search(polygonscan_pattern, url)
        if match:
            contract = match.group(1)
            logger.info(f"ä»Polygonscan URLæå–åˆ°åˆçº¦åœ°å€: {contract}, é“¾: MATIC")
            return contract, 'MATIC'

        # Arbiscanæ ¼å¼
        arbiscan_pattern = r'(?:https?://)?(?:www\.)?arbiscan\.io/(?:token|address)/([a-zA-Z0-9]{20,})'
        match = re.search(arbiscan_pattern, url)
        if match:
            contract = match.group(1)
            logger.info(f"ä»Arbiscan URLæå–åˆ°åˆçº¦åœ°å€: {contract}, é“¾: ARB")
            return contract, 'ARB'
        
        # Basescanæ ¼å¼
        basescan_pattern = r'(?:https?://)?(?:www\.)?basescan\.org/(?:token|address)/([a-zA-Z0-9]{20,})'
        match = re.search(basescan_pattern, url)
        if match:
            contract = match.group(1)
            logger.info(f"ä»Basescan URLæå–åˆ°åˆçº¦åœ°å€: {contract}, é“¾: BASE")
            return contract, 'BASE'
        
        # Snowtrace (Avalanche) æ ¼å¼
        snowtrace_pattern = r'(?:https?://)?(?:www\.)?snowtrace\.io/(?:token|address)/([a-zA-Z0-9]{20,})'
        match = re.search(snowtrace_pattern, url)
        if match:
            contract = match.group(1)
            logger.info(f"ä»Snowtrace URLæå–åˆ°åˆçº¦åœ°å€: {contract}, é“¾: AVAX")
            return contract, 'AVAX'
        
        # å¤„ç†Raydiumã€Orcaç­‰Solana DEXçš„URL
        solana_dex_pattern = r'(?:https?://)?(?:www\.)?(raydium\.io|orca\.so|jup\.ag)/(?:\w+)/([a-zA-Z0-9]{20,})'
        match = re.search(solana_dex_pattern, url)
        if match:
            contract = match.group(2)
            logger.info(f"ä»Solana DEX URLæå–åˆ°åˆçº¦åœ°å€: {contract}, é“¾: SOL")
            return contract, 'SOL'
        
        # å¤„ç†Uniswapã€Sushiswapç­‰ä»¥å¤ªåŠDEXçš„URL
        eth_dex_pattern = r'(?:https?://)?(?:www\.)?(uniswap\.org|app\.uniswap\.org|sushi\.com)/(?:\w+)/([a-zA-Z0-9]{20,})'
        match = re.search(eth_dex_pattern, url)
        if match:
            contract = match.group(2)
            logger.info(f"ä»ETH DEX URLæå–åˆ°åˆçº¦åœ°å€: {contract}, é“¾: ETH")
            return contract, 'ETH'
        
        logger.debug(f"æœªèƒ½ä»URLä¸­æå–åˆçº¦åœ°å€: {url}")
        return None, None
        
    except Exception as e:
        logger.error(f"ä»URLä¸­æå–åˆçº¦åœ°å€æ—¶å‡ºé”™: {str(e)}")
        return None, None

def format_token_history(history: list) -> str:
    """æ ¼å¼åŒ–ä»£å¸å†å²æ•°æ®ä¸ºæ˜“è¯»çš„å­—ç¬¦ä¸²"""
    if not history:
        return "æœªæ‰¾åˆ°è¯¥ä»£å¸çš„å†å²æ•°æ®"
    
    output = []
    output.append("=== ä»£å¸å†å²æ•°æ® ===\n")
    
    # è·å–ç¬¬ä¸€æ¡æ•°æ®ä¸­çš„ä»£å¸ä¿¡æ¯
    _, first_promo = history[0]
    if first_promo:
        output.append(f"ä»£å¸ç¬¦å·: {first_promo.token_symbol}")
        output.append(f"åˆçº¦åœ°å€: {first_promo.contract_address}\n")
    
    # æ·»åŠ æ¯æ¡è®°å½•çš„è¯¦ç»†ä¿¡æ¯
    for message, promo in history:
        # æ­£ç¡®å¤„ç†æ—¶åŒºè½¬æ¢
        date = message['date']
        if isinstance(date, (int, float)):
            # å‡è®¾æ—¶é—´æˆ³æ˜¯UTCæ—¶é—´
            utc_time = datetime.fromtimestamp(date, timezone.utc)
        else:
            # å¦‚æœæ˜¯datetimeå¯¹è±¡ï¼Œç¡®ä¿å®ƒæœ‰UTCæ—¶åŒºä¿¡æ¯
            utc_time = timezone.utc.localize(date) if not date.tzinfo else date
            
        # è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´ (UTC+8)
        beijing_tz = timezone(timedelta(hours=8))
        beijing_time = utc_time.astimezone(beijing_tz)
        
        # è¾“å‡ºåŒ—äº¬æ—¶é—´ï¼Œæ˜ç¡®æ ‡æ³¨æ—¶åŒº
        output.append(f"æ—¶é—´: {beijing_time.strftime('%Y-%m-%d %H:%M:%S')} (UTC+8)")
        if promo:
            if promo.market_cap is not None:
                output.append(f"å¸‚å€¼: ${promo.market_cap:,.2f}")
            if promo.promotion_count is not None:
                output.append(f"æ¨å¹¿æ¬¡æ•°: {promo.promotion_count}")
        output.append(f"æ¶ˆæ¯ID: {message['message_id']}")
        output.append("æ¶ˆæ¯å†…å®¹:")
        output.append(message['text'])
        output.append("-" * 50 + "\n")
    
    return "\n".join(output)

def get_db_performance_stats():
    """è·å–æ•°æ®åº“æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        æ€§èƒ½ç»Ÿè®¡æ•°æ®å­—å…¸
    """
    stats = db_performance_stats.copy()
    
    # è®¡ç®—æ¯ä¸ªæ“ä½œçš„å¹³å‡æ‰§è¡Œæ—¶é—´
    avg_times = {}
    for op_name, total_time in stats['operation_times'].items():
        count = stats['operation_counts'].get(op_name, 0)
        if count > 0:
            avg_times[op_name] = total_time / count
        else:
            avg_times[op_name] = 0
    
    stats['average_times'] = avg_times
    
    # æ·»åŠ SupabaseçŠ¶æ€ä¿¡æ¯
    try:
        from src.database.db_factory import get_db_adapter
        db_adapter = get_db_adapter()
        stats['adapter_type'] = 'supabase'
        stats['database_uri'] = 'supabase://*****' # éšè—æ•æ„Ÿä¿¡æ¯
    except Exception as e:
        logger.error(f"è·å–æ•°æ®åº“çŠ¶æ€ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        stats['adapter_error'] = str(e)
    
    return stats

def reset_db_performance_stats():
    """é‡ç½®æ•°æ®åº“æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
    global db_performance_stats
    db_performance_stats = {
        'operation_counts': {},
        'operation_times': {},
        'lock_errors': 0,
        'total_retries': 0
    }

# æ·»åŠ ç¼ºå¤±çš„æ¸…ç†æ‰¹å¤„ç†ä»»åŠ¡å‡½æ•°
async def cleanup_batch_tasks():
    """æ¸…ç†æ‰€æœ‰æ‰¹å¤„ç†ä»»åŠ¡é˜Ÿåˆ—ï¼Œç¡®ä¿æ‰€æœ‰å¾…å¤„ç†æ•°æ®éƒ½è¢«ä¿å­˜
    
    åœ¨ç¨‹åºå…³é—­å‰è°ƒç”¨æ­¤å‡½æ•°ï¼Œä»¥é˜²æ­¢æ•°æ®ä¸¢å¤±
    """
    global message_batch, token_batch
    
    try:
        # å¤„ç†æ¶ˆæ¯æ‰¹å¤„ç†é˜Ÿåˆ—
        if message_batch:
            logger.info(f"æ¸…ç† {len(message_batch)} æ¡æœªå¤„ç†çš„æ¶ˆæ¯...")
            await process_message_batch()
        
        # å¤„ç†ä»£å¸æ‰¹å¤„ç†é˜Ÿåˆ—
        if token_batch:
            logger.info(f"æ¸…ç† {len(token_batch)} æ¡æœªå¤„ç†çš„ä»£å¸ä¿¡æ¯...")
            try:
                # å¤åˆ¶å½“å‰é˜Ÿåˆ—å¹¶æ¸…ç©ºå…¨å±€é˜Ÿåˆ—
                local_batch = token_batch.copy()
                token_batch = []
                
                # å®‰å…¨åœ°å¤„ç†æ¯ä¸ªä»£å¸æ•°æ®
                processed_count = 0
                for token_data in local_batch:
                    try:
                        if isinstance(token_data, dict) and 'contract' in token_data and token_data['contract']:
                            # ä½¿ç”¨å¢å¼ºçš„save_token_infoå‡½æ•°å¤„ç†å•ä¸ªä»£å¸ä¿¡æ¯
                            if save_token_info(token_data):
                                processed_count += 1
                        else:
                            logger.warning(f"è·³è¿‡æ— æ•ˆçš„ä»£å¸æ•°æ®: {token_data}")
                    except Exception as e:
                        logger.error(f"å¤„ç†å•ä¸ªä»£å¸æ•°æ®æ—¶å‡ºé”™: {str(e)}")
                
                logger.info(f"æˆåŠŸæ¸…ç† {processed_count}/{len(local_batch)} æ¡ä»£å¸ä¿¡æ¯")
            except Exception as e:
                logger.error(f"æ¸…ç†ä»£å¸ä¿¡æ¯é˜Ÿåˆ—æ—¶å‡ºé”™: {str(e)}")
                import traceback
                logger.debug(traceback.format_exc())
        
        logger.info("æ‰¹å¤„ç†ä»»åŠ¡é˜Ÿåˆ—æ¸…ç†å®Œæˆ")
        return True
    except Exception as e:
        logger.error(f"æ¸…ç†æ‰¹å¤„ç†ä»»åŠ¡é˜Ÿåˆ—æ—¶å‡ºé”™: {str(e)}")
        import traceback
        logger.debug(traceback.format_exc())
        return False

def calculate_community_reach(token_symbol: str, session=None):
    """è®¡ç®—ä»£å¸çš„ç¤¾ç¾¤è¦†ç›–äººæ•°
    
    è®¡ç®—æ–¹å¼ï¼š
    1. æ ¹æ®token_symbolæŸ¥è¯¢tokens_markè¡¨ä¸­çš„æ‰€æœ‰æ¡ç›®
    2. ç»Ÿè®¡æŸ¥è¯¢ç»“æœä¸­çš„å”¯ä¸€channel_id
    3. æ ¹æ®channel_idæŸ¥è¯¢telegram_channelsè¡¨ä¸­çš„member_count
    4. å°†member_countç›¸åŠ å¾—åˆ°community_reach
    
    Args:
        token_symbol: ä»£å¸ç¬¦å·
        session: åºŸå¼ƒå‚æ•°ï¼Œä¸ºäº†å…¼å®¹æ€§ä¿ç•™
        
    Returns:
        int: è®¡ç®—å¾—åˆ°çš„ç¤¾ç¾¤è¦†ç›–äººæ•°
    """
    # ä½¿ç”¨ç¼“å­˜é¿å…é¢‘ç¹è®¡ç®—ç›¸åŒçš„token
    cache_key = f"community_reach_{token_symbol}"
    if hasattr(calculate_community_reach, 'cache') and cache_key in calculate_community_reach.cache:
        # ç¼“å­˜æœ‰æ•ˆæœŸä¸º5åˆ†é’Ÿ
        cache_time, value = calculate_community_reach.cache[cache_key]
        if (datetime.now() - cache_time).total_seconds() < 300:  # 5åˆ†é’Ÿå†…
            return value

    # åˆå§‹åŒ–ç¼“å­˜å­—å…¸(å¦‚æœä¸å­˜åœ¨)
    if not hasattr(calculate_community_reach, 'cache'):
        calculate_community_reach.cache = {}
    
    try:
        # ä½¿ç”¨Supabaseé€‚é…å™¨
        from src.database.db_factory import get_db_adapter
        db_adapter = get_db_adapter()
        
        # ä½¿ç”¨å¼‚æ­¥è°ƒç”¨ä½†åœ¨åŒæ­¥ç¯å¢ƒä¸­æ‰§è¡Œ
        async def get_community_reach():
            # 1. è·å–æ‰€æœ‰æåŠè¯¥ä»£å¸çš„token_markè®°å½•
            token_marks = await db_adapter.execute_query(
                'tokens_mark',
                'select',
                filters={'token_symbol': token_symbol}
            )
            
            # å¦‚æœæ²¡æœ‰è®°å½•ï¼Œè¿”å›0
            if not token_marks:
                return 0
                
            # 2. æå–å”¯ä¸€çš„channel_id
            channel_ids = []
            for mark in token_marks:
                if isinstance(mark, dict) and mark.get('channel_id') and mark['channel_id'] not in channel_ids:
                    channel_ids.append(mark['channel_id'])
            
            # å¦‚æœæ²¡æœ‰channel_idï¼Œè¿”å›0
            if not channel_ids:
                return 0
                
            # 3. è·å–è¿™äº›é¢‘é“çš„æˆå‘˜æ•°
            total_reach = 0
            for channel_id in channel_ids:
                channel_info = await db_adapter.get_channel_by_id(channel_id)
                if channel_info and channel_info.get('member_count'):
                    total_reach += channel_info['member_count']
            
            return total_reach
        
        # æ‰§è¡Œå¼‚æ­¥å‡½æ•°ï¼Œä½¿ç”¨äº‹ä»¶å¾ªç¯
        loop = asyncio.new_event_loop()
        try:
            total_reach = loop.run_until_complete(get_community_reach())
        finally:
            loop.close()
                
        # å­˜å…¥ç¼“å­˜
        calculate_community_reach.cache[cache_key] = (datetime.now(), total_reach)
        
        # é™åˆ¶ç¼“å­˜å¤§å°ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
        if len(calculate_community_reach.cache) > 1000:  # æœ€å¤šç¼“å­˜1000ä¸ªtoken
            # åˆ é™¤æœ€æ—©çš„20%ç¼“å­˜
            sorted_keys = sorted(
                calculate_community_reach.cache.keys(),
                key=lambda k: calculate_community_reach.cache[k][0]
            )
            for key in sorted_keys[:200]:  # åˆ é™¤æœ€æ—©çš„200ä¸ª
                del calculate_community_reach.cache[key]
        
        # è¿”å›æ€»è¦†ç›–äººæ•°
        return total_reach
        
    except Exception as e:
        logger.error(f"è®¡ç®—ä»£å¸ {token_symbol} çš„ç¤¾ç¾¤è¦†ç›–äººæ•°æ—¶å‡ºé”™: {str(e)}")
        logger.debug(traceback.format_exc())
        return 0

# æ·»åŠ ç®€å•çš„ä»£å¸æ•°æ®éªŒè¯å‡½æ•°
def validate_token_data(token_data: Dict[str, Any]) -> Tuple[bool, str]:
    """
    éªŒè¯ä»£å¸æ•°æ®çš„å®Œæ•´æ€§
    
    Args:
        token_data: ä»£å¸æ•°æ®
        
    Returns:
        (bool, str): æ˜¯å¦æœ‰æ•ˆï¼Œé”™è¯¯ä¿¡æ¯
    """
    required_fields = ['chain', 'token_symbol', 'contract']
    
    # æ£€æŸ¥å¿…è¦å­—æ®µ
    for field in required_fields:
        if field not in token_data or not token_data[field]:
            return False, f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}"
    
    return True, ""
