import re
import os
import json
import logging
import jieba
import numpy as np
from collections import Counter
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any

# å»ºç«‹æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åŠ è½½æƒ…æ„Ÿåˆ†æè¯å…¸
def load_sentiment_dict():
    """åŠ è½½æƒ…æ„Ÿåˆ†æè¯å…¸"""
    try:
        # å®šä¹‰å¯èƒ½çš„è¯å…¸è·¯å¾„
        dict_paths = [
            # é»˜è®¤è·¯å¾„ï¼ˆç›¸å¯¹äºåˆ†æå™¨ï¼‰
            os.path.join(os.path.dirname(__file__), 'dicts'),
            # é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„data/sentiment
            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), 'data', 'sentiment')
        ]
        
        # ç§¯æè¯æ±‡
        positive_words = []
        for base_path in dict_paths:
            positive_path = os.path.join(base_path, 'positive_words.txt')
            if os.path.exists(positive_path):
                logger.info(f"ä» {positive_path} åŠ è½½ç§¯æè¯æ±‡")
                with open(positive_path, 'r', encoding='utf-8') as f:
                    positive_words = [line.strip() for line in f if line.strip()]
                break
                
        if not positive_words:
            # é»˜è®¤è¯æ±‡
            logger.warning("æœªæ‰¾åˆ°ç§¯æè¯æ±‡æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤è¯æ±‡")
            positive_words = [
                'çœ‹æ¶¨', 'ä¸Šæ¶¨', 'åˆ©å¥½', 'çªç ´', 'å¼ºçƒˆæ¨è', 'ç‰›å¸‚', 'åˆ©æ¶¦', 'å¢é•¿', 'ä¸Šå‡', 'å‘å±•',
                'æœºä¼š', 'ä¼˜è´¨', 'æˆåŠŸ', 'çªç ´', 'é¢†å…ˆ', 'å¤§æ¶¨', 'æ½œåŠ›', 'å€¼å¾—', 'èµš', 'ä¼˜åŠ¿',
                'bullish', 'moon', 'gem', 'pump', 'profit', 'gain', 'up', 'grow', 'increase',
                'opportunity', 'success', 'breakthrough', 'potential', 'recommend', 'promising',
                'ğŸš€', 'ğŸ”¥', 'ğŸ’ª', 'ğŸ’°', 'ğŸ“ˆ', 'âœ…'
            ]
            
        # æ¶ˆæè¯æ±‡
        negative_words = []
        for base_path in dict_paths:
            negative_path = os.path.join(base_path, 'negative_words.txt')
            if os.path.exists(negative_path):
                logger.info(f"ä» {negative_path} åŠ è½½æ¶ˆæè¯æ±‡")
                with open(negative_path, 'r', encoding='utf-8') as f:
                    negative_words = [line.strip() for line in f if line.strip()]
                break
                
        if not negative_words:
            # é»˜è®¤è¯æ±‡
            logger.warning("æœªæ‰¾åˆ°æ¶ˆæè¯æ±‡æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤è¯æ±‡")
            negative_words = [
                'çœ‹è·Œ', 'ä¸‹è·Œ', 'åˆ©ç©º', 'è·Œç ´', 'é£é™©', 'ç†Šå¸‚', 'æŸå¤±', 'ä¸‹é™', 'äºæŸ', 'å´©ç›˜',
                'æ³¡æ²«', 'èµ„é‡‘ç›˜', 'éª—å±€', 'è·‘è·¯', 'å¤±è´¥', 'å±æœº', 'è­¦æƒ•', 'è°¨æ…', 'è¯ˆéª—', 'é»‘å¹•',
                'bearish', 'dump', 'scam', 'rug', 'loss', 'crash', 'down', 'decrease', 'fail',
                'ponzi', 'scheme', 'suspect', 'risk', 'warning', 'careful', 'suspect',
                'ğŸ“‰', 'âš ï¸', 'âŒ'
            ]
        
        # ç‚’ä½œè¯æ±‡
        hype_words = []
        for base_path in dict_paths:
            hype_path = os.path.join(base_path, 'hype_words.txt')
            if os.path.exists(hype_path):
                logger.info(f"ä» {hype_path} åŠ è½½ç‚’ä½œè¯æ±‡")
                with open(hype_path, 'r', encoding='utf-8') as f:
                    hype_words = [line.strip() for line in f if line.strip()]
                break
                
        if not hype_words:
            # é»˜è®¤è¯æ±‡
            logger.warning("æœªæ‰¾åˆ°ç‚’ä½œè¯æ±‡æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤è¯æ±‡")
            hype_words = [
                'æš´æ¶¨', 'ç™¾å€', 'åƒå€', 'ä¸‡å€', 'ç§’æ€', 'èµ·é£', 'ä¸€å¤œæš´å¯Œ', 'è´¢å¯Œè‡ªç”±', 'ä¸å®¹é”™è¿‡', 
                'æœºä¸å¯å¤±', 'ç¨€ç¼º', 'æŠ„åº•', 'æœ€åæœºä¼š', 'å¿«ä¸Šè½¦', 'é”™è¿‡å¿…åæ‚”', 'çˆ†å‘', 'ç¥ç§˜', 
                '100x', '1000x', 'moonshot', 'to the moon', 'huge', 'massive', 'incredible',
                'don\'t miss', 'last chance', 'hidden gem', 'next big thing', 'explode'
            ]
            
        # è®°å½•è¯æ±‡æ•°é‡
        logger.info(f"åŠ è½½äº† {len(positive_words)} ä¸ªç§¯æè¯æ±‡, {len(negative_words)} ä¸ªæ¶ˆæè¯æ±‡, {len(hype_words)} ä¸ªç‚’ä½œè¯æ±‡")
        
        return {
            'positive': positive_words,
            'negative': negative_words,
            'hype': hype_words
        }
    except Exception as e:
        logger.error(f"åŠ è½½æƒ…æ„Ÿè¯å…¸å‡ºé”™: {str(e)}")
        return {'positive': [], 'negative': [], 'hype': []}

# å…¨å±€æƒ…æ„Ÿè¯å…¸
SENTIMENT_DICT = load_sentiment_dict()

class TokenAnalyzer:
    """ä»£å¸ä¿¡æ¯åˆ†æå™¨ï¼Œæä¾›æƒ…æ„Ÿåˆ†æå’Œä»·æ ¼è¶‹åŠ¿åˆ†æåŠŸèƒ½"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.sentiment_dict = SENTIMENT_DICT
        
    def analyze_text(self, text: str) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬ï¼Œæå–æƒ…æ„Ÿä¿¡æ¯å’Œå…³é”®è¯"""
        if not text:
            return {
                'sentiment_score': 0,
                'positive_words': [],
                'negative_words': [],
                'hype_score': 0,
                'risk_level': 'unknown'
            }
            
        # åˆ†è¯ï¼ˆæ”¯æŒä¸­è‹±æ–‡æ··åˆï¼‰
        words = self._tokenize(text)
        
        # åŒ¹é…æƒ…æ„Ÿè¯
        positive_matches = [word for word in words if word.lower() in [w.lower() for w in self.sentiment_dict['positive']]]
        negative_matches = [word for word in words if word.lower() in [w.lower() for w in self.sentiment_dict['negative']]]
        hype_matches = [word for word in words if word.lower() in [w.lower() for w in self.sentiment_dict['hype']]]
        
        # åŒ¹é…è¡¨æƒ…ç¬¦å·
        emoji_score = self._analyze_emojis(text)
        
        # è®¡ç®—æƒ…æ„Ÿå¾—åˆ†
        positive_count = len(positive_matches) + emoji_score['positive']
        negative_count = len(negative_matches) + emoji_score['negative']
        total_count = positive_count + negative_count
        
        # å½’ä¸€åŒ–æƒ…æ„Ÿå¾—åˆ†åˆ°[-1, 1]åŒºé—´
        sentiment_score = 0
        if total_count > 0:
            sentiment_score = (positive_count - negative_count) / total_count
            
        # è®¡ç®—ç‚’ä½œå¾—åˆ†
        hype_score = (len(hype_matches) / max(len(words), 1)) * 5  # å½’ä¸€åŒ–åˆ°0-5èŒƒå›´
        
        # ç¡®å®šé£é™©ç­‰çº§
        risk_level = self._determine_risk_level(sentiment_score, hype_score, text)
        
        return {
            'sentiment_score': sentiment_score,
            'positive_words': positive_matches,
            'negative_words': negative_matches,
            'hype_score': hype_score,
            'risk_level': risk_level
        }
        
    def _tokenize(self, text: str) -> List[str]:
        """åˆ†è¯å¤„ç†ï¼Œæ”¯æŒä¸­è‹±æ–‡æ··åˆ"""
        # å¯¹ä¸­æ–‡è¿›è¡Œåˆ†è¯
        words = []
        if any('\u4e00' <= ch <= '\u9fff' for ch in text):  # åŒ…å«ä¸­æ–‡å­—ç¬¦
            words = list(jieba.cut(text))
        else:
            # è‹±æ–‡åˆ†è¯
            words = re.findall(r'\b\w+\b', text.lower())
            
        # è¿‡æ»¤ç©ºç™½è¯
        words = [w for w in words if w.strip()]
        return words
        
    def _analyze_emojis(self, text: str) -> Dict[str, int]:
        """åˆ†ææ–‡æœ¬ä¸­çš„è¡¨æƒ…ç¬¦å·"""
        positive_emojis = ['ğŸš€', 'ğŸ”¥', 'ğŸ’ª', 'ğŸ’°', 'ğŸ“ˆ', 'âœ…', 'ğŸ‘', 'ğŸ˜Š', 'ğŸ™Œ', 'ğŸ’']
        negative_emojis = ['ğŸ“‰', 'âš ï¸', 'âŒ', 'ğŸ‘', 'ğŸ˜±', 'ğŸ˜¢', 'ğŸ˜­', 'ğŸ™„', 'ğŸ¤”', 'ğŸ˜¡']
        
        positive_count = sum(text.count(emoji) for emoji in positive_emojis)
        negative_count = sum(text.count(emoji) for emoji in negative_emojis)
        
        return {
            'positive': positive_count,
            'negative': negative_count
        }
        
    def _determine_risk_level(self, sentiment_score: float, hype_score: float, text: str) -> str:
        """åŸºäºæƒ…æ„Ÿåˆ†æå’Œç‚’ä½œå¾—åˆ†ç¡®å®šé£é™©ç­‰çº§"""
        # æ£€æµ‹é£é™©å…³é”®è¯
        risk_keywords = ['scam', 'éª—å±€', 'rug', 'è·‘è·¯', 'ponzi', 'èµ„é‡‘ç›˜', 'warning', 'é£é™©', 'honeypot']
        has_risk_keyword = any(keyword in text.lower() for keyword in risk_keywords)
        
        # æ£€æµ‹å¼‚å¸¸é«˜çš„ç‚’ä½œ
        excessive_hype = hype_score > 3.5
        
        # ç»¼åˆè¯„ä¼°é£é™©
        if has_risk_keyword:
            return 'high'  # ç»Ÿä¸€ä½¿ç”¨è‹±æ–‡
        elif excessive_hype and sentiment_score > 0.7:
            return 'medium-high'  # ç»Ÿä¸€ä½¿ç”¨è‹±æ–‡
        elif excessive_hype:
            return 'medium'  # ç»Ÿä¸€ä½¿ç”¨è‹±æ–‡
        elif sentiment_score < -0.5:
            return 'medium'  # ç»Ÿä¸€ä½¿ç”¨è‹±æ–‡
        elif sentiment_score > 0.7:
            return 'low-medium'  # ç»Ÿä¸€ä½¿ç”¨è‹±æ–‡
        else:
            return 'low'  # ç»Ÿä¸€ä½¿ç”¨è‹±æ–‡
            
    def analyze_price_trend(self, current_price: float, price_history: List[Dict]) -> Dict[str, Any]:
        """åˆ†æä»·æ ¼è¶‹åŠ¿"""
        if not price_history or current_price is None:
            return {
                'price_change_24h': 0,
                'price_change_7d': 0,
                'volatility': 0,
                'trend': 'neutral'
            }
            
        # æŒ‰æ—¶é—´æ’åº
        sorted_history = sorted(price_history, key=lambda x: x.get('timestamp', 0))
        
        # æå–24å°æ—¶å‰å’Œ7å¤©å‰çš„ä»·æ ¼
        now = datetime.now()
        day_ago = now - timedelta(days=1)
        week_ago = now - timedelta(days=7)
        
        price_24h_ago = None
        price_7d_ago = None
        
        for entry in sorted_history:
            timestamp = entry.get('timestamp')
            if not timestamp:
                continue
                
            entry_time = datetime.fromtimestamp(timestamp)
            if price_24h_ago is None and entry_time <= day_ago:
                price_24h_ago = entry.get('price')
            if price_7d_ago is None and entry_time <= week_ago:
                price_7d_ago = entry.get('price')
                
        # è®¡ç®—ä»·æ ¼å˜åŒ–
        price_change_24h = 0
        if price_24h_ago and price_24h_ago > 0:
            price_change_24h = (current_price - price_24h_ago) / price_24h_ago * 100
            
        price_change_7d = 0
        if price_7d_ago and price_7d_ago > 0:
            price_change_7d = (current_price - price_7d_ago) / price_7d_ago * 100
            
        # è®¡ç®—æ³¢åŠ¨ç‡
        prices = [entry.get('price', 0) for entry in sorted_history if entry.get('price')]
        volatility = 0
        if len(prices) > 1:
            volatility = np.std(prices) / np.mean(prices) * 100 if np.mean(prices) > 0 else 0
            
        # ç¡®å®šè¶‹åŠ¿
        trend = 'neutral'
        if price_change_24h > 10:
            trend = 'strong_bullish'
        elif price_change_24h > 5:
            trend = 'bullish'
        elif price_change_24h < -10:
            trend = 'strong_bearish'
        elif price_change_24h < -5:
            trend = 'bearish'
            
        return {
            'price_change_24h': price_change_24h,
            'price_change_7d': price_change_7d,
            'volatility': volatility,
            'trend': trend
        }
        
    def analyze_token(self, text: str, price: Optional[float] = None, price_history: List[Dict] = None) -> Dict[str, Any]:
        """ç»¼åˆåˆ†æä»£å¸ä¿¡æ¯"""
        # æƒ…æ„Ÿåˆ†æ
        sentiment_result = self.analyze_text(text)
        
        # ä»·æ ¼è¶‹åŠ¿åˆ†æ
        price_trend = {}
        if price is not None and price_history:
            price_trend = self.analyze_price_trend(price, price_history)
            
        # ç»„åˆç»“æœ
        result = {
            # æƒ…æ„Ÿåˆ†æç»“æœ
            'sentiment_score': sentiment_result.get('sentiment_score', 0),
            'positive_words': sentiment_result.get('positive_words', []),
            'negative_words': sentiment_result.get('negative_words', []),
            'hype_score': sentiment_result.get('hype_score', 0),
            'risk_level': sentiment_result.get('risk_level', 'unknown'),
            
            # ä»·æ ¼è¶‹åŠ¿ç»“æœ
            'price': price,
            'price_change_24h': price_trend.get('price_change_24h', 0),
            'price_change_7d': price_trend.get('price_change_7d', 0),
            'volatility': price_trend.get('volatility', 0),
            'trend': price_trend.get('trend', 'neutral'),
            
            # ç”Ÿæˆæ‘˜è¦
            'summary': self._generate_summary({**sentiment_result, **price_trend, 'price': price})
        }
        
        return result
        
    def _generate_summary(self, analysis_result: Dict) -> str:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        
        # æƒ…æ„Ÿåˆ†æéƒ¨åˆ†
        sentiment = analysis_result.get('sentiment_score', 0)
        sentiment_desc = "ä¸­æ€§"
        if sentiment > 0.5:
            sentiment_desc = "éå¸¸ç§¯æ"
        elif sentiment > 0.1:
            sentiment_desc = "ç§¯æ"
        elif sentiment < -0.5:
            sentiment_desc = "éå¸¸æ¶ˆæ"
        elif sentiment < -0.1:
            sentiment_desc = "æ¶ˆæ"
            
        # ç‚’ä½œè¯„åˆ†æè¿°
        hype_score = analysis_result.get('hype_score', 0)
        hype_desc = "æ— æ˜æ˜¾ç‚’ä½œ"
        if hype_score > 4:
            hype_desc = "éå¸¸é«˜çš„ç‚’ä½œ"
        elif hype_score > 3:
            hype_desc = "é«˜ç‚’ä½œ"
        elif hype_score > 2:
            hype_desc = "ä¸­ç­‰ç‚’ä½œ"
            
        # é£é™©ç­‰çº§æè¿°
        risk_map = {
            'high': "é«˜é£é™©",
            'medium-high': "ä¸­é«˜é£é™©",
            'medium': "ä¸­é£é™©", 
            'low-medium': "ä½ä¸­é£é™©",
            'low': "ä½é£é™©",
            'unknown': "æœªçŸ¥é£é™©"
        }
        risk_desc = risk_map.get(analysis_result.get('risk_level', 'unknown'), "æœªçŸ¥é£é™©")
        
        # ä»·æ ¼è¶‹åŠ¿éƒ¨åˆ†
        trend_part = ""
        if 'price_change_24h' in analysis_result:
            change_24h = analysis_result.get('price_change_24h', 0)
            trend = analysis_result.get('trend', 'neutral')
            
            trend_map = {
                'strong_bullish': "å¼ºçƒˆä¸Šæ¶¨",
                'bullish': "ä¸Šæ¶¨",
                'neutral': "ç¨³å®š",
                'bearish': "ä¸‹è·Œ",
                'strong_bearish': "å¼ºçƒˆä¸‹è·Œ"
            }
            
            trend_desc = trend_map.get(trend, "ç¨³å®š")
            trend_part = f"ä»·æ ¼24å°æ—¶å˜åŒ–: {change_24h:.2f}%, è¶‹åŠ¿: {trend_desc}. "
            
        # ç»„åˆæ‘˜è¦
        summary = f"æƒ…æ„Ÿåˆ†æ: {sentiment_desc}, ç‚’ä½œè¯„åˆ†: {hype_desc}, é£é™©è¯„çº§: {risk_desc}. {trend_part}"
        
        # æ·»åŠ æç¤ºè¯­
        if analysis_result.get('risk_level') in ['high', 'medium-high']:
            summary += "è¯·æ³¨æ„é£é™©ï¼Œè°¨æ…æŠ•èµ„!"
        
        return summary
        
        
# å•ä¾‹æ¨¡å¼
_analyzer = None

def get_analyzer() -> TokenAnalyzer:
    """è·å–TokenAnalyzerå•ä¾‹"""
    global _analyzer
    if _analyzer is None:
        _analyzer = TokenAnalyzer()
    return _analyzer


# æµ‹è¯•å‡½æ•°
if __name__ == "__main__":
    # æµ‹è¯•æ–‡æœ¬
    test_text = """
    ğŸš€ æœ€æ–°ä»£å¸: MEMEX
    
    è¿™ä¸ªä»£å¸å³å°†æš´æ¶¨100å€ï¼ğŸ”¥
    å¸‚å€¼åªæœ‰: 500K
    
    Telegram: https://t.me/memex_coin
    ç½‘ç«™: https://memex.finance
    
    ä¸å®¹é”™è¿‡çš„åƒå€æœºä¼šï¼ğŸ’°
    """
    
    analyzer = TokenAnalyzer()
    result = analyzer.analyze_token(test_text)
    
    print("åˆ†æç»“æœ:")
    print(f"æƒ…æ„Ÿå¾—åˆ†: {result['sentiment_score']}")
    print(f"ç§¯æè¯æ±‡: {result['positive_words']}")
    print(f"æ¶ˆæè¯æ±‡: {result['negative_words']}")
    print(f"ç‚’ä½œå¾—åˆ†: {result['hype_score']}")
    print(f"é£é™©ç­‰çº§: {result['risk_level']}")
    print(f"æ‘˜è¦: {result['summary']}") 